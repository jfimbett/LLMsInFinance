# Word Embeddings in Finance

## What Are Word Embeddings?

### Technical Definition
- **Word embeddings**: Dense vector representations of words in a continuous vector space
- Words are mapped to real-valued vectors in an n-dimensional space (typically 100-300 dimensions)
- Semantically similar words are positioned closer together in this vector space
- The relative positions and distances between word vectors encode meaningful relationships

### Intuition
- Think of embeddings as "translating" words into a language that computers understand (vectors)
- Each dimension represents a latent feature of the word's meaning
- Instead of treating words as isolated symbols, embeddings capture their context and relationships
- Example: In a 2D simplification, "profit" and "earnings" would be close together, while "loss" would be farther away

## Why Embeddings Matter in Finance

- Transform unstructured textual data (news, reports, filings) into structured numerical data
- Enable quantitative analysis of qualitative information
- Allow algorithms to understand semantic relationships between financial concepts
- Support tasks like sentiment analysis, document classification, and information retrieval
- Bridge the gap between natural language and mathematical models
- You can do this with words, as well as with entire sentences, paragraphs, or documents.

## Traditional Methods

### One-Hot Encoding
- Represents each word as a unique vector with a single 1 and all other elements 0.
- High-dimensional and sparse representation
- Example: "dog" = [0, 0, ..., 1, 0, ..., 0] (1 at index for "dog")
- Pros: Simple and Fast to compute
- Cons: Inefficient, high-dimensional, and does not capture relationships between words

### TF-IDF (Term Frequency-Inverse Document Frequency)
- Weighs words based on their frequency in a document relative to their frequency across a corpus. 
- Helps identify important terms in documents
Formula

$$
\begin{align*}
TF-IDF(w, d) = TF(w, d) * IDF(w) \\
TF(w, d) = \frac{f(w, d)}{|d|} \\
IDF(w) = \log\frac{N}{n(w)}
\end{align*}
$$

Where:
- \( f(w, d) \): Frequency of word \( w \) in document \( d \)
- \( |d| \): Total number of words in document \( d \)
- \( N \): Total number of documents in the corpus
- \( n(w) \): Number of documents containing word \( w \)
- Example:
```
Doc 1: "The quick brown fox jumps over the lazy dog."
Doc 2: "The lazy dog sleeps all day."
```

- TF-IDF for "lazy"
```math
TF("lazy", Doc 1) = 1/9
IDF("lazy") = log(2/2) = 0
TF-IDF("lazy", Doc 1) = (1/9) * 0
TF-IDF("lazy", Doc 2) = (1/7) * log(2/2) = 0  
```

- TF-IDF for "quick"
```math
TF("quick", Doc 1) = 1/9
IDF("quick") = log(2/1) = log(2)
TF-IDF("quick", Doc 1) = (1/9) * log(2) 
TF-IDF("quick", Doc 2) = 0
```

### Word2Vec
- Meaning of a word based from its surrounding words. 
- Trained with large corpora to learn word relationships
- Words which appear in similar contexts are mapped to vectors which are nearby as measured by cosine similarity. 

### GloVe (Global Vectors)
- It was developed by Stanford in 2014.
- It is trained on aggregated global word-word co-occurrence statistics from a corpus. (e.g. how often words appear together)

#### Bag of Words (BoW)
- Represents text as occurrence counts of words
- Simple but loses word order and context

#### FastText
- Extension of Word2Vec that uses character n-grams
- Better handles rare words and morphologically rich languages
- Can represent out-of-vocabulary words

## Properties of Word Vector Spaces

### Semantic Clustering
- Words with similar meanings cluster together
- Example: "equity," "stock," "share" form a cluster

### Vector Arithmetic
- Word vectors can be added and subtracted with meaningful results
- Classical example: "king" - "man" + "woman" ≈ "queen"
- Financial examples:
  - "Bull" - "Market" + "Housing" ≈ "Bubble"
  - "Fed" + "Increase" ≈ "Rates"
  - "Bond" - "Price" + "Increase" ≈ "Yield"

### Analogical Reasoning
- Vector relationships encode semantic relationships
- Enables solving analogies: (A is to B as C is to D)
- Financial example: "Stock:Equity :: Bond:Debt"

## Finance-Specific Word Embeddings

### Domain Adaptation
- Generic embeddings often miss nuances in financial language
- Domain-specific embeddings are trained on financial corpora:
  - Earnings call transcripts
  - Financial news
  - SEC filings
  - Analyst reports

### Benefits in Financial Applications
- More accurate representation of financial terminology
- Better capture of relationships between market concepts
- Improved performance in financial NLP tasks:
  - Sentiment analysis of market news
  - Classification of financial documents
  - Information extraction from reports

## Limitations of Traditional Embeddings

### Context Insensitivity
- Each word has only one vector regardless of context
- Polysemy problem: "bank" (financial institution vs. river edge)
- "Interest" (monetary vs. attentional) has different meanings in finance

### Static Nature
- Cannot adapt to evolving language and new terminology
- Market-specific terms change meaning during different economic cycles

### Rare Terms Challenge
- Financial jargon and specialized terminology often lack quality embeddings
- Numerical values and symbols are not well represented

### Phrase Handling
- Important financial phrases ("interest rate," "balance sheet") need special treatment
- Individual word embeddings may not capture phrase meanings