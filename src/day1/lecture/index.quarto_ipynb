{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Day 1: Introduction to Large Language Models\"\n",
        "author: \"Juan F. Imbet\"\n",
        "date: \"2025-05-07\"\n",
        "format:\n",
        "  revealjs:\n",
        "    theme: default\n",
        "    slide-number: true\n",
        "    preview-links: auto\n",
        "    css: ../../styles.css\n",
        "    logo: ../../images/logo_header.svg\n",
        "jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "# Introduction to Large Language Models in Finance\n",
        "\n",
        "## Overview of Today's Lecture\n",
        "\n",
        "- Evolution of NLP applications in Finance\n",
        "- Word Embeddings\n",
        "- Tokenizers\n",
        "- The Transformers Architecture\n",
        "- Softmax Probabilities and Token Generation\n",
        "- Classification and Scalability of LLMs\n",
        "\n",
        "\n",
        "# Evolution of NLP Applications in Finance\n",
        "\n",
        "## Historical Development\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"100%\"}\n",
        "- **Traditional rule-based approaches (1960s – 1990s)** keyword spotting and hand-written grammars for parsing bank statements and wire stories. Access to computing power limited to large institutions.\n",
        "- **Statistical methods (1990s – 2010s)** – TF-IDF, n-grams, naïve Bayes sentiment. \n",
        "- **Machine-learning era (2010s)** – supervised classifiers & finance-specific dictionaries.  \n",
        "- **Deep-learning revolution (≈2015 +)** – word-embeddings, CNN/RNN sentiment on earnings calls, topic models.\n",
        "- **Large Language Models (2018 +)** – GPT-style chatbots & summarizers now embedded in research and advisory workflows (see Morgan Stanley GPT-4 Assistant 2023).\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "## Selected Academic Milestones (2004 – 2024)\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Antweiler & Frank 2004, *JF*** – Internet-forum sentiment & volatility ([link](https://doi.org/10.1111/j.1540-6261.2004.00662.x))  \n",
        "- **Tetlock 2007, *JF*** – WSJ pessimism predicts next-day returns ([link](https://doi.org/10.1111/j.1540-6261.2007.01232.x))  \n",
        "- **Dougal *et al.* 2012, *RFS*** – Columnist-specific tone moves the market ([link](https://academic.oup.com/rfs/article-abstract/25/3/639/1617372))  \n",
        "- **Loughran & McDonald 2011, *JF*** – Finance-specific tone dictionaries for 10-Ks ([link](https://doi.org/10.1111/j.1540-6261.2010.01625.x))  \n",
        "- **Jegadeesh & Wu 2013, *JFE*** – Market-reaction-weighted tone metric ([link](https://www.sciencedirect.com/science/article/pii/S0304405X13002328))  \n",
        "- **Chen *et al.* 2014, *RFS*** – Seeking Alpha opinions predict returns & EPS surprises ([link](https://academic.oup.com/rfs/article/27/5/1367/1581938))  \n",
        "- **Manela & Moreira 2017, *JFE*** – NVIX text-based disaster-risk index ([link](https://doi.org/10.1016/j.jfineco.2016.08.013))  \n",
        "- **Buehlmaier & Whited 2018, *RFS*** – Text-identified financial-constraints premium ([link](https://academic.oup.com/rfs/article-abstract/31/7/2693/4824924))  \n",
        "- **Hassan *et al.* 2019, *QJE*** – Firm-level political-risk from earnings-call text ([link](https://academic.oup.com/qje/article/134/4/2135/5531768))  \n",
        "- **Bybee *et al.* 2024, *JF*** – Topic-model news-attention indices improve macro forecasts ([link](https://doi.org/10.1111/jofi.13377))  \n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "**Why it matters**\n",
        "\n",
        "- Predict **returns**, **volatility**, and **risk premia**  \n",
        "- Reveal intangible firm traits (constraints, political risk)  \n",
        "- Enhance macro forecasting with text-derived factors  \n",
        "- Methodology shifted: word-counts → ML classification → embeddings & topic models\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "## Real-World Industry Applications of NLP\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Thomson Reuters / Bloomberg News Analytics** – Real-time machine-readable sentiment feeds power quant desks and HFT strategies ([Forbes](https://www.forbes.com/sites/tomgroenfeldt/2011/11/28/trading-on-sentiment-analysis-a-public-relations-tool-goes-to-wall-street/))  \n",
        "- **RavenPack** – >70 % of top quant funds ingest its news-sentiment data for alpha & risk ([RavenPack](https://www.ravenpack.com/products/edge/data/news-analytics))  \n",
        "- **MarketPsych Capital (2008-10)** – Hedge fund trading on media sentiment, +28 % during the 2008 crisis ([MarketPsych](https://www.marketpsych.com/))  \n",
        "- **Derwent Capital \"Twitter Fund\" 2011** – $40 m portfolio guided by Twitter mood ([Atlantic](https://www.theatlantic.com/business/archive/2011/05/the-worlds-first-twitter-based-hedge-fund-is-finally-open-for-business/239097/))  \n",
        "- **J.P. Morgan COIN 2017** – NLP reviews loan contracts in seconds, saving 360 k lawyer-hours ([ABA Journal](https://www.abajournal.com/news/article/jpmorgan_chase_uses_tech_to_save_360000_hours_of_annual_work_by_lawyers_and))  \n",
        "- **Morgan Stanley GPT-4 Assistant 2023-24** – Chatbot for 16 k advisors, instant Q&A on 100 k research docs ([Press release](https://www.morganstanley.com/press-releases/morgan-stanley-research-announces-askresearchgpt))  \n",
        "- **Kensho (acq. S&P Global 2018)** – NLP Q&A platform \"Warren\" enhances S&P analytics ([S&P Global](https://investor.spglobal.com/news-releases/news-details/2018/SP-Global-to-Acquire-Kensho-Bolsters-Core-Capabilities-in-Artificial-Intelligence-Natural-Language-Processing-and-Data-Analytics-2018-3-6/default.aspx))  \n",
        "- **SEC \"RoboCop\" 2013** – Accounting Quality Model flags anomalous filings for enforcement ([Harvard Law Blog](https://corpgov.law.harvard.edu/2014/01/27/the-secs-refocus-on-accounting-irregularities/))  \n",
        "- **Lloyd's / FRISS Fraud Detection** – Text-mining claims boosts fraud-catch rate ≈30 % ([Lloyd's Lab report](https://assets.lloyds.com/media/dc22cd29-1c4e-441c-a872-e1bf5ce9142a/Lloyds%20Lab_impact%20report_FINAL.pdf))  \n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "**Key take-aways**\n",
        "\n",
        "- **Alpha & Risk** – sentiment + event extraction  \n",
        "- **Efficiency** – contract analysis, research Q&A  \n",
        "- **Governance** – regulators spot fraud & anomalies  \n",
        "- **Generative AI** – LLM-powered advisory tools\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "\n",
        "# Word Embeddings in Finance\n",
        "\n",
        "## What Are Word Embeddings?\n",
        "\n",
        "### Technical Definition\n",
        "- **Word embeddings**: Dense vector representations of words in a continuous vector space\n",
        "- Words are mapped to real-valued vectors in an n-dimensional space (typically 100-300 dimensions)\n",
        "- Semantically similar words are positioned closer together in this vector space\n",
        "- The relative positions and distances between word vectors encode meaningful relationships\n",
        "\n",
        "### Intuition\n",
        "- Think of embeddings as \"translating\" words into a language that computers understand (vectors)\n",
        "- Each dimension represents a latent feature of the word's meaning\n",
        "- Instead of treating words as isolated symbols, embeddings capture their context and relationships\n",
        "- Example: In a 2D simplification, \"profit\" and \"earnings\" would be close together, while \"loss\" would be farther away\n",
        "\n",
        "## Why Embeddings Matter in Finance\n",
        "\n",
        "- Transform unstructured textual data (news, reports, filings) into structured numerical data\n",
        "- Enable quantitative analysis of qualitative information\n",
        "- Allow algorithms to understand semantic relationships between financial concepts\n",
        "- Support tasks like sentiment analysis, document classification, and information retrieval\n",
        "- Bridge the gap between natural language and mathematical models\n",
        "- You can do this with words, as well as with entire sentences, paragraphs, or documents.\n",
        "\n",
        "## Traditional Methods\n",
        "\n",
        "### One-Hot Encoding\n",
        "- Represents each word as a unique vector with a single 1 and all other elements 0.\n",
        "- High-dimensional and sparse representation\n",
        "- Example: \"dog\" = [0, 0, ..., 1, 0, ..., 0] (1 at index for \"dog\")\n",
        "- Pros: Simple and Fast to compute\n",
        "- Cons: Inefficient, high-dimensional, and does not capture relationships between words\n",
        "\n",
        "### TF-IDF (Term Frequency-Inverse Document Frequency)\n",
        "- Weighs words based on their frequency in a document relative to their frequency across a corpus. \n",
        "- Helps identify important terms in documents\n",
        "Formula\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "TF-IDF(w, d) = TF(w, d) * IDF(w) \\\\\n",
        "TF(w, d) = \\frac{f(w, d)}{|d|} \\\\\n",
        "IDF(w) = \\log\\frac{N}{n(w)}\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- \\( f(w, d) \\): Frequency of word \\( w \\) in document \\( d \\)\n",
        "- \\( |d| \\): Total number of words in document \\( d \\)\n",
        "- \\( N \\): Total number of documents in the corpus\n",
        "- \\( n(w) \\): Number of documents containing word \\( w \\)\n",
        "- Example:\n",
        "```\n",
        "Doc 1: \"The quick brown fox jumps over the lazy dog.\"\n",
        "Doc 2: \"The lazy dog sleeps all day.\"\n",
        "```\n",
        "\n",
        "- TF-IDF for \"lazy\"\n",
        "```math\n",
        "TF(\"lazy\", Doc 1) = 1/9\n",
        "IDF(\"lazy\") = log(2/2) = 0\n",
        "TF-IDF(\"lazy\", Doc 1) = (1/9) * 0\n",
        "TF-IDF(\"lazy\", Doc 2) = (1/7) * log(2/2) = 0  \n",
        "```\n",
        "\n",
        "- TF-IDF for \"quick\"\n",
        "```math\n",
        "TF(\"quick\", Doc 1) = 1/9\n",
        "IDF(\"quick\") = log(2/1) = log(2)\n",
        "TF-IDF(\"quick\", Doc 1) = (1/9) * log(2) \n",
        "TF-IDF(\"quick\", Doc 2) = 0\n",
        "```\n",
        "\n",
        "### Word2Vec\n",
        "- Meaning of a word based from its surrounding words. \n",
        "- Trained with large corpora to learn word relationships\n",
        "- Words which appear in similar contexts are mapped to vectors which are nearby as measured by cosine similarity. \n",
        "\n",
        "### GloVe (Global Vectors)\n",
        "- It was developed by Stanford in 2014.\n",
        "- It is trained on aggregated global word-word co-occurrence statistics from a corpus. (e.g. how often words appear together)\n",
        "\n",
        "#### Bag of Words (BoW)\n",
        "- Represents text as occurrence counts of words\n",
        "- Simple but loses word order and context\n",
        "\n",
        "#### FastText\n",
        "- Extension of Word2Vec that uses character n-grams\n",
        "- Better handles rare words and morphologically rich languages\n",
        "- Can represent out-of-vocabulary words\n",
        "\n",
        "## Properties of Word Vector Spaces\n",
        "\n",
        "### Semantic Clustering\n",
        "- Words with similar meanings cluster together\n",
        "- Example: \"equity,\" \"stock,\" \"share\" form a cluster\n",
        "\n",
        "### Vector Arithmetic\n",
        "- Word vectors can be added and subtracted with meaningful results\n",
        "- Classical example: \"king\" - \"man\" + \"woman\" ≈ \"queen\"\n",
        "- Financial examples:\n",
        "  - \"Bull\" - \"Market\" + \"Housing\" ≈ \"Bubble\"\n",
        "  - \"Fed\" + \"Increase\" ≈ \"Rates\"\n",
        "  - \"Bond\" - \"Price\" + \"Increase\" ≈ \"Yield\"\n",
        "\n",
        "### Analogical Reasoning\n",
        "- Vector relationships encode semantic relationships\n",
        "- Enables solving analogies: (A is to B as C is to D)\n",
        "- Financial example: \"Stock:Equity :: Bond:Debt\"\n",
        "\n",
        "## Finance-Specific Word Embeddings\n",
        "\n",
        "### Domain Adaptation\n",
        "- Generic embeddings often miss nuances in financial language\n",
        "- Domain-specific embeddings are trained on financial corpora:\n",
        "  - Earnings call transcripts\n",
        "  - Financial news\n",
        "  - SEC filings\n",
        "  - Analyst reports\n",
        "\n",
        "### Benefits in Financial Applications\n",
        "- More accurate representation of financial terminology\n",
        "- Better capture of relationships between market concepts\n",
        "- Improved performance in financial NLP tasks:\n",
        "  - Sentiment analysis of market news\n",
        "  - Classification of financial documents\n",
        "  - Information extraction from reports\n",
        "\n",
        "## Limitations of Traditional Embeddings\n",
        "\n",
        "### Context Insensitivity\n",
        "- Each word has only one vector regardless of context\n",
        "- Polysemy problem: \"bank\" (financial institution vs. river edge)\n",
        "- \"Interest\" (monetary vs. attentional) has different meanings in finance\n",
        "\n",
        "### Static Nature\n",
        "- Cannot adapt to evolving language and new terminology\n",
        "- Market-specific terms change meaning during different economic cycles\n",
        "\n",
        "### Rare Terms Challenge\n",
        "- Financial jargon and specialized terminology often lack quality embeddings\n",
        "- Numerical values and symbols are not well represented\n",
        "\n",
        "### Phrase Handling\n",
        "- Important financial phrases (\"interest rate,\" \"balance sheet\") need special treatment\n",
        "- Individual word embeddings may not capture phrase meanings\n",
        "\n",
        "\n",
        "## Limitations of traditional embeddings\n",
        "\n",
        "\n",
        "- Do you recall how the word ***bank*** can refer to a financial institution or the side of a river? Traditional word embeddings struggle with such polysemy, as they assign a single vector representation to each word, regardless of context.\n",
        "- The only way to address this is to use context-sensitive embeddings, which means that words need to talk to each other. The word embedding for a word depends on the words around it.\n",
        "- Letting words **talk to each other** was first explored in the context of machine translation using RNN (Recurrent Neural Networks). \n",
        "- However. it was not until the introduction of the **Transformer architecture** that we could effectively let words talk to each other in a scalable way. We will explore this in the next section.\n",
        "\n",
        "## Some Definitions\n",
        "- **Recurrent Neural Networks (RNNs)**: A type of neural network designed for sequential data, where the output from previous steps is fed as input to the current step.\n",
        "- **LSTM (Long Short-Term Memory)**: A type of RNN that can learn long-term dependencies, making it suitable for tasks like language modeling and translation.\n",
        "- Prior to transformers, RNN architectures were the state of the art. They contained a **feedback*** loop in the network connections that allows information to propagate, making it ideal for sequential data like text.\n",
        "- A crutial feature of these networks is that the **input** and **output** do not have to be the same length.\n",
        "\n",
        "## Basic RNNs\n",
        "\n",
        "- Consider a sequence of observations of arbitrary length and a prediction of the next observation in the sequence. (E.g. bond quotes in TRACE)\n",
        "- A basic RNN would take the previous observations as input, process it through a hidden layer, and output a prediction for the next return.\n",
        "- A RNN Cell is a simple unit that takes an input and the previous hidden state, processes them, and outputs a new hidden state and an output.\n",
        "\n",
        "![Basic RNN Cell](../../images/rnn1.png)\n",
        "\n",
        "## Unfolding the RNN\n",
        "\n",
        "- The RNN can be unfolded over time, where each time step corresponds to a new observation in the sequence.\n",
        "\n",
        "![Basic RNN Cell](../../images/rnn2.png)\n",
        "\n",
        "\n",
        "## More on RNNs\n",
        "\n",
        "- Recurrent Neural Networks (RNNs) extend traditional neural networks by allowing them to process **sequences of variable length**, unlike vanilla or convolutional networks which operate on fixed-size inputs and outputs.\n",
        "\n",
        "- RNNs can handle diverse tasks such as **sequence-to-sequence** (e.g., machine translation), **sequence-to-one** (e.g., sentiment analysis), **one-to-sequence** (e.g., image captioning), and **synced input/output sequences** (e.g., video frame labeling).\n",
        "\n",
        "- The **core mechanism** of RNNs is the state vector, which evolves through a **fixed, learned transformation** that combines past information (state) with new input at each time step.\n",
        "\n",
        "- RNNs are more **computationally expressive** than feedforward networks: they can be seen as running a learned program, and are theoretically **Turing-complete**.\n",
        "\n",
        "- Even when inputs and outputs are fixed-size vectors, RNNs can still be used to process them **sequentially** — for example, by learning to attend over parts of an image or generating images step by step.\n",
        "\n",
        "## Different types of RNNs\n",
        "\n",
        "![Different RNNs](https://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "If you are interested in all that RNNs can do, I recommend reading [Andrej Karpathy's blog post](https://karpathy.github.io/2015/05/21/rnn-effectiveness/) on the effectiveness of RNNs.\n",
        "\n",
        "## The Encoder-Decoder Framework\n",
        "\n",
        "- For most applications we will focus on mapping a sequence of inputs to a sequence of outputs. In a RNN the *encoder* encodes the information from the input sequence into a numerical representation, usually **encoded** in the last hidden state. \n",
        "- In the ***The capital of France is** example, the encoded representation is the last hidden state. \n",
        "- The *decoder* then takes this representation and generates the output sequence, one token at a time.\n",
        "\n",
        "## Encoder-Decoder blocks for machine translation, e.g. english to catalan\n",
        "\n",
        "![RNN for machine translation](../../images/rnn3.png)\n",
        "\n",
        "## Limitations of the traditional Encoder-Decoder framework\n",
        "\n",
        "- Although elegant in its simplicity, one weakness is that the final hidden state of the encoder creates an **information bottleneck**. A single state needs to be able to represent the meaning of the whole input sequence. \n",
        "- This is particularly challenging for long sequences. \n",
        "- What if we give access to the decoder to all the hidden states of the encoder?\n",
        "- This is the idea behind the **attention mechanism**, let RNN cells in the decoder **pay attention** to all the hidden states of the encoder, not just the last one.\n",
        "\n",
        "## Attention Mechanisms \n",
        "\n",
        "![Attention Mechanism](../../images/encoder-decoder-attention.png)\n",
        "\n",
        "- The idea behind attention is to give the decoder access to the hidden states of the encoder. \n",
        "- However, using all the states at the same time would create a huge computational burden, so we need to **weight** the hidden states of the encoder.\n",
        "- These weights are learned during training and allow the decoder to focus on the most relevant parts of the input sequence.\n",
        "- This general attention mechanism is also referred as **cross-attention**.\n",
        "- A big limitation is that the attention mechanism is still sequential, meaning that the decoder needs to process the input sequence one word at a time. \n",
        "\n",
        "\n",
        "# Understanding Transformers: Step by Step\n",
        "\n",
        "## What is a Transformer?\n",
        "\n",
        "- Revolutionary neural network architecture introduced in \"Attention Is All You Need\" (2017)\n",
        "- **Key innovation**: Replaces recurrence and convolutions entirely with attention mechanisms\n",
        "- Enables **parallel processing** of sequences (unlike RNNs)\n",
        "- Foundation for modern LLMs including GPT-2, GPT-3/4, BERT\n",
        "\n",
        "![Attention Paper](../../images/attention_paper.png)\n",
        "\n",
        "## Transformer Evolution\n",
        "\n",
        "![Attention Mechanism](../../images/attention_abstract.png)\n",
        "\n",
        "## A Visual Representation\n",
        "\n",
        "![Transformer Baby](../../images/transformer_baby.png)\n",
        "\n",
        "\n",
        "## The Big Picture: Transformer Architecture\n",
        "\n",
        "![Transformer Architecture](../../images/transformers_model.png)\n",
        "\n",
        "- **Encoder-Decoder Structure**: Input → Encoder → Decoder → Output\n",
        "- **Self-Attention**: Each position can attend to all positions in previous layer\n",
        "- **Parallelizable**: No sequential dependencies like RNNs\n",
        "\n",
        "\n",
        "\n",
        "## The recipe: Part 1\n",
        "\n",
        "\n",
        "- **Tokenise** the *source* sentence and add start/end markers  \n",
        "- **Embed** each token **+** add positional encodings\n",
        "\n",
        "- **Encoder (× N layers)**  \n",
        "  - Multi-head **self-attention**  \n",
        "  - Position-wise **feed-forward network**  \n",
        "  - **Residual connection + LayerNorm** after each sub-layer  \n",
        "\n",
        "- Cache the resulting **encoder hidden states** (the \"memory\")\n",
        "\n",
        "\n",
        "\n",
        "## The recipe: Part 2\n",
        "\n",
        "\n",
        "- **Decoder (run autoregressively)**  \n",
        "  1. Embed the generated prefix tokens **+** positional encodings.  \n",
        "  2. **Masked self-attention** (each token sees only ≤ current position).  \n",
        "  3. **Cross-attention** over the encoder memory (lets the decoder “look back” at the source).  \n",
        "  4. Feed-forward → Residual → LayerNorm.  \n",
        "  5. **Linear projection** (tied to embeddings) → **softmax** → probability distribution.  \n",
        "  6. **Select** the next token (greedy, top-k, nucleus, beam, …), append it, and repeat until ⟨end of sentence EOS⟩ or a maximum length.\n",
        "\n",
        "\n",
        "## New vocabulary?\n",
        "\n",
        "- **Self-Attention**: Mechanism allowing each token to attend to all other tokens in the sequence, capturing dependencies regardless of distance.\n",
        "- **Multi-Head Attention**: Multiple self-attention mechanisms running in parallel, allowing the model to capture different types of relationships.\n",
        "- **Feed-Forward Network (FFN)**: A fully connected neural network applied to each position independently, typically with a ReLU (Rectified Linear Unit) activation.\n",
        "- **Positional Encoding**: Adds information about the position of each token in the sequence, since transformers do not have a built-in notion of order.\n",
        "- **Residual Connection**: A shortcut connection that adds the input of a layer to its output, helping to prevent vanishing gradients in deep networks.\n",
        "- **Layer Normalization**: A technique to stabilize and accelerate training by normalizing the inputs to each layer, applied after residual connections.\n",
        "\n",
        "\n",
        "## Step 1: Input Embeddings\n",
        "\n",
        "### Token Embeddings\n",
        "- When you feed a sequence of tokens into a transformer‐based LLM, each discrete token (an integer index) is turned into a dense vector of lower dimensionality than the vocabulary size\n",
        "\n",
        "- Let $V$ be the vocabulary size, and $N$ the sequence length \n",
        "- A token $j$ is an integer in the set $\\{0, 1, \\ldots, V-1\\}$\n",
        "- A brute force one-hot vector encoding $x \\in \\{0,1\\}^V$\n",
        "$$\n",
        "x_j = \\begin{cases}\n",
        "1 & \\text{if } j = i \\\\\n",
        "0 & \\text{otherwise}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "- This is inefficient, especially for large vocabularies, as it results in high-dimensional sparse vectors\n",
        "- Instead, we use a **learnable embedding matrix** $\\mathbf{E} \\in \\mathbb{R}^{V \\times d_{\\text{model}}}$, where $d_{\\text{model}}$ is typically much smaller than $V$\n",
        "\n",
        "## Step 1: Positional Encoding\n",
        "\n",
        "- If token $t$ is represented by index $j$, its embedding is given by the $j$-th row of \n",
        "$$\n",
        "\\mathbf{e}_j = \\mathbf{x}_j^T \\mathbf{E} = \\mathbf{E}_j\n",
        "$$\n",
        "\n",
        "### Positional Encoding\n",
        "Since transformers have no inherent notion of position, we add positional information:\n",
        "\n",
        "$$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
        "\n",
        "$$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)$$\n",
        "\n",
        "**Final input**: $\\text{input}_i = \\text{embedding}_i + PE_i$\n",
        "\n",
        "\n",
        "\n",
        "## Step 2: Self-Attention Mechanism - The Core\n",
        "\n",
        "### Queries, Keys, and Values\n",
        "For each token embedding $\\mathbf{x}_i$, we create three vectors:\n",
        "\n",
        "$$\\mathbf{q}_i = \\mathbf{x}_i \\mathbf{W}^Q \\quad \\text{(Query)}$$\n",
        "$$\\mathbf{k}_i = \\mathbf{x}_i \\mathbf{W}^K \\quad \\text{(Key)}$$  \n",
        "$$\\mathbf{v}_i = \\mathbf{x}_i \\mathbf{W}^V \\quad \\text{(Value)}$$\n",
        "\n",
        "Where $\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in \\mathbb{R}^{d_{\\text{model}} \\times d_k}$ are learned parameter matrices.\n",
        "\n",
        "\n",
        "\n",
        "## Step 3: Computing Attention - The Intuition\n",
        "\n",
        "### Attention Formula\n",
        "$$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n",
        "\n",
        "### Step-by-step breakdown:\n",
        "\n",
        "1. **Dot product**: $\\mathbf{Q}\\mathbf{K}^T$ gives similarity scores between all pairs\n",
        "2. **Scale**: Divide by $\\sqrt{d_k}$ to prevent softmax saturation\n",
        "3. **Normalize**: Apply softmax to get attention weights\n",
        "4. **Weighted sum**: Multiply by values $\\mathbf{V}$\n",
        "\n",
        "## Step 3: Attention Visualization\n",
        "\n",
        "![Attention Visualization](../../images/transformers1.png)\n",
        "\n",
        "- Each token attends to all other tokens\n",
        "- Attention weights determine how much information flows\n",
        "- Self-attention is the key to capturing dependencies regardless of distance\n",
        "- This replaces the need for recurrence in traditional RNNs\n",
        "\n",
        "## Step 4: Multi-Head Attention\n",
        "\n",
        "- Single attention mechanism provides limited representational power\n",
        "- **Multi-head attention** runs multiple attention computations in parallel\n",
        "- Each \"head\" learns different relationship patterns:\n",
        "  - Some heads focus on nearby words\n",
        "  - Others capture long-range dependencies\n",
        "  - Some track syntactic relationships\n",
        "\n",
        "$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\text{head}_2, ..., \\text{head}_h)\\mathbf{W}^O$$\n",
        "\n",
        "Where each head is computed as:\n",
        "$$\\text{head}_i = \\text{Attention}(\\mathbf{Q}\\mathbf{W}_i^Q, \\mathbf{K}\\mathbf{W}_i^K, \\mathbf{V}\\mathbf{W}_i^V)$$\n",
        "\n",
        "## Step 5: Feed-Forward Networks\n",
        "\n",
        "- After attention, each position goes through identical feed-forward networks\n",
        "- Applied to each position separately and identically\n",
        "- Consists of two linear transformations with a ReLU activation in between:\n",
        "\n",
        "$$\\text{FFN}(x) = \\max(0, x\\mathbf{W}_1 + \\mathbf{b}_1)\\mathbf{W}_2 + \\mathbf{b}_2$$\n",
        "\n",
        "- This introduces non-linearity and allows the model to transform the representation\n",
        "\n",
        "## Step 6: Residual Connections & Layer Normalization\n",
        "\n",
        "- **Residual connections** help with training deep networks:\n",
        "  - Add the input of each sub-layer to its output: $x + \\text{Sublayer}(x)$\n",
        "  - Allows gradients to flow through the network more easily\n",
        "\n",
        "- **Layer normalization** stabilizes the learning process:\n",
        "  - Normalizes the inputs across the features\n",
        "  - Applied after each residual connection\n",
        "\n",
        "$$\\text{LayerNorm}(x + \\text{Sublayer}(x))$$\n",
        "\n",
        "## Transformer Model Applications\n",
        "\n",
        "- **Machine Translation**: Original use case in \"Attention Is All You Need\"\n",
        "- **Text Generation**: Foundation for GPT models\n",
        "- **Document Understanding**: BERT and its variants\n",
        "- **Multimodal Applications**: Vision transformers, audio transformers\n",
        "- **Financial Applications**: Market prediction, sentiment analysis, report generation\n",
        "\n",
        "## GPT-2: A Landmark Decoder-Only Model\n",
        "\n",
        "### GPT-2 Architecture Basics\n",
        "\n",
        "- Released by OpenAI in 2019\n",
        "- Decoder-only transformer architecture\n",
        "- Trained on 40GB of internet text\n",
        "- Available in different sizes:\n",
        "  - Small: 117M parameters\n",
        "  - Medium: 345M parameters\n",
        "  - Large: 762M parameters\n",
        "  - XL: 1.5B parameters\n",
        "\n",
        "## GPT-2: Model Architecture Details\n",
        "\n",
        "- Uses masked self-attention (can only attend to previous tokens)\n",
        "- No encoder-decoder structure, just the decoder component\n",
        "- Trained on a simple next-token prediction objective\n",
        "- Layer configuration (for 1.5B model):\n",
        "  - 48 layers\n",
        "  - 1600 dimensional embeddings\n",
        "  - 25 attention heads\n",
        "\n",
        "## GPT-2: Key Innovations\n",
        "\n",
        "- Demonstrated impressive zero-shot capabilities\n",
        "- Introduced **unsupervised pre-training** at scale\n",
        "- Showed that scaling model size and data substantially improves performance\n",
        "- Established the foundation for subsequent models like GPT-3 and GPT-4\n",
        "- Pioneered better sampling methods for text generation\n",
        "\n",
        "## Sampling Strategies: Introduction\n",
        "\n",
        "- After the model computes the probability distribution for the next token, how do we select it?\n",
        "- Different sampling methods produce different text qualities and characteristics\n",
        "- Trade-off between:\n",
        "  - **Determinism**: Consistent, predictable outputs\n",
        "  - **Creativity**: Novel, diverse text generation\n",
        "  - **Coherence**: Staying on topic without degrading\n",
        "\n",
        "## Sampling Strategy: Greedy Decoding\n",
        "\n",
        "- **Approach**: Always select the most probable next token\n",
        "- **Formula**: $y_t = \\arg\\max_w P(w|y_{<t})$\n",
        "\n",
        "### Advantages:\n",
        "- Simple to implement\n",
        "- Often produces coherent text for short sequences\n",
        "- Deterministic results\n",
        "\n",
        "### Disadvantages:\n",
        "- Lacks diversity\n",
        "- Can get stuck in repetition loops\n",
        "- May produce suboptimal overall sequences\n",
        "\n",
        "## Sampling Strategy: Temperature Sampling\n",
        "\n",
        "- **Approach**: Sample from softmax distribution with temperature adjustment\n",
        "- **Formula**: $P(w|y_{<t}) = \\frac{\\exp(z_w/T)}{\\sum_{w'} \\exp(z_{w'}/T)}$\n",
        "\n",
        "### Temperature effects:\n",
        "- $T < 1$: Makes distribution more peaked (less random)\n",
        "- $T > 1$: Makes distribution more uniform (more random)\n",
        "- $T = 1$: Standard softmax, no adjustment\n",
        "- $T \\to 0$: Approaches greedy decoding\n",
        "\n",
        "## Sampling Strategy: Top-K Sampling\n",
        "\n",
        "- **Approach**: Limit sampling to the K most likely next tokens\n",
        "- **Procedure**:\n",
        "  1. Sort the vocabulary by probability\n",
        "  2. Keep only the top K tokens\n",
        "  3. Renormalize probabilities\n",
        "  4. Sample from this smaller distribution\n",
        "\n",
        "### Advantages:\n",
        "- Reduces chance of selecting low-probability (potentially nonsensical) tokens\n",
        "- Maintains some randomness\n",
        "- Often produces more coherent text than pure sampling\n",
        "\n",
        "### Disadvantages:\n",
        "- K is a fixed hyperparameter regardless of confidence distribution\n",
        "- May be too restrictive for some contexts, too permissive for others\n",
        "\n",
        "## Sampling Strategy: Nucleus (Top-p) Sampling\n",
        "\n",
        "- **Approach**: Sample from the smallest set of tokens whose cumulative probability exceeds threshold p\n",
        "- **Procedure**:\n",
        "  1. Sort tokens by probability\n",
        "  2. Keep adding tokens until cumulative probability ≥ p\n",
        "  3. Renormalize and sample from this dynamic set\n",
        "\n",
        "### Advantages:\n",
        "- Adapts to the confidence of the model\n",
        "- More flexible than Top-K\n",
        "- Current standard for high-quality text generation\n",
        "\n",
        "### Disadvantages:\n",
        "- Slightly more complex to implement\n",
        "- Still requires tuning the p parameter (typically 0.9-0.95)\n",
        "\n",
        "## Sampling in Financial Applications\n",
        "\n",
        "### Conservative Approach (Low Temperature/High Precision):\n",
        "- Regulatory reporting\n",
        "- Earnings statement generation\n",
        "- Financial advice\n",
        "\n",
        "### Creative Approach (Higher Temperature/More Exploration):\n",
        "- Market scenario generation\n",
        "- Stress testing\n",
        "- Alternative investment thesis formulation\n",
        "\n",
        "## Putting It All Together: The Transformer Revolution\n",
        "\n",
        "- Transformers dramatically improved NLP capabilities through:\n",
        "  - **Parallelization**: Training efficiency\n",
        "  - **Attention Mechanism**: Better at capturing relationships\n",
        "  - **Scalability**: Performance continues to improve with size\n",
        "\n",
        "- Led to a new paradigm of foundation models\n",
        "- Enabled financial applications previously considered impossible\n",
        "- Continues to evolve with each new model generation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Softmax Probabilities in Token Generation\n",
        "\n",
        "## From Encoder-Decoder to Next Token Prediction\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **The transformer pipeline overview**:\n",
        "  - Input tokens → Embeddings → Encoder layers\n",
        "  - Decoder layers → Linear projection → Softmax\n",
        "  - Probability distribution over vocabulary\n",
        "  - Sampling/selection of next token\n",
        "  \n",
        "- **Key transformation stages**:\n",
        "  - Multi-head attention creates contextual representations\n",
        "  - Feed-forward networks refine token representations  \n",
        "  - Layer normalization ensures stable training\n",
        "  - Final linear layer maps to vocabulary size\n",
        "  - Softmax converts logits to probabilities\n",
        ":::\n",
        "\n",
        "\n",
        "- **Mathematical flow**:\n",
        "  - Hidden states: $h_i \\in \\mathbb{R}^{d_{model}}$\n",
        "  - Linear projection: $W_{out} \\in \\mathbb{R}^{d_{model} \\times |V|}$\n",
        "  - Logits: $z_i = h_i W_{out}$\n",
        "  - Probabilities: $p_i = \\text{softmax}(z_i)$\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## The Softmax Function in Detail\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Mathematical definition**:\n",
        "  $$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{|V|} e^{z_j}}$$\n",
        "  \n",
        "- **Key properties**:\n",
        "  - Outputs sum to 1 (valid probability distribution)\n",
        "  - Emphasizes largest logit values (temperature effect)\n",
        "  - Differentiable for backpropagation\n",
        "  - Maps any real number to (0,1) range\n",
        "  \n",
        "- **Temperature parameter**:\n",
        "  $$\\text{softmax}(z_i/T) = \\frac{e^{z_i/T}}{\\sum_{j=1}^{|V|} e^{z_j/T}}$$\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "- **Temperature effects**:\n",
        "  - $T = 1$: Standard softmax\n",
        "  - $T > 1$: More uniform distribution (creative)\n",
        "  - $T < 1$: More peaked distribution (conservative)\n",
        "  - $T \\to 0$: Approaches argmax (deterministic)\n",
        "  - $T \\to \\infty$: Approaches uniform distribution\n",
        "\n",
        "- **Financial text implications**:\n",
        "  - Low temperature: Precise financial terminology\n",
        "  - High temperature: Creative financial analysis\n",
        "  - Balanced approach for professional content\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Vocabulary Mapping and Logits\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Vocabulary construction**:\n",
        "  - Tokenizer creates mapping: token ↔ integer ID\n",
        "  - Vocabulary size typically 30K-100K+ tokens\n",
        "  - Special tokens: [PAD], [UNK], [CLS], [SEP]\n",
        "  - Subword tokenization (BPE, WordPiece, SentencePiece)\n",
        "  \n",
        "- **Logit computation**:\n",
        "  - Final hidden state: $h_{final} \\in \\mathbb{R}^{d_{model}}$\n",
        "  - Output projection: $W_{out} \\in \\mathbb{R}^{d_{model} \\times |V|}$\n",
        "  - Bias term: $b_{out} \\in \\mathbb{R}^{|V|}$\n",
        "  - Logits: $z = h_{final} W_{out} + b_{out}$\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "- **Financial vocabulary considerations**:\n",
        "  - Specialized financial terms (EBITDA, derivatives)\n",
        "  - Numerical representations ($, %, basis points)\n",
        "  - Company names and ticker symbols\n",
        "  - Regulatory terminology (SEC, GAAP, IFRS)\n",
        "  - Domain-specific abbreviations (P/E, ROE, NPV)\n",
        "  \n",
        "- **Example logit interpretation**:\n",
        "  - High logit for \"earnings\" after \"quarterly\"\n",
        "  - High logit for \"%\" after numerical values\n",
        "  - Context-dependent probabilities\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## From Probabilities to Token Selection\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Deterministic selection (Greedy)**:\n",
        "  - Choose token with highest probability\n",
        "  - $\\text{token} = \\arg\\max_i p_i$\n",
        "  - Consistent but potentially repetitive\n",
        "  - Risk of getting stuck in loops\n",
        "  \n",
        "- **Probabilistic sampling**:\n",
        "  - Sample from probability distribution\n",
        "  - $\\text{token} \\sim \\text{Multinomial}(p_1, p_2, ..., p_{|V|})$\n",
        "  - Introduces randomness and creativity\n",
        "  - Multiple runs produce different outputs\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "- **Advanced sampling strategies**:\n",
        "  \n",
        "  **Top-k sampling**:\n",
        "  - Consider only k most probable tokens\n",
        "  - Reduces probability mass on unlikely tokens\n",
        "  - Balances quality and diversity\n",
        "  \n",
        "  **Top-p (nucleus) sampling**:\n",
        "  - Select smallest set with cumulative probability ≥ p\n",
        "  - Adaptive vocabulary size based on confidence\n",
        "  - More dynamic than fixed top-k\n",
        "  \n",
        "  **Beam search** (for deterministic quality):\n",
        "  - Maintain multiple candidate sequences\n",
        "  - Exponential search space pruning\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Temperature and Sampling in Financial Context\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Conservative financial writing** (T = 0.3-0.7):\n",
        "  - Precise terminology and standard phrases\n",
        "  - Regulatory compliance language\n",
        "  - Technical analysis descriptions\n",
        "  - Risk disclosures and disclaimers\n",
        "  \n",
        "- **Creative financial analysis** (T = 0.8-1.2):\n",
        "  - Novel insights and interpretations\n",
        "  - Alternative scenario descriptions\n",
        "  - Innovative investment strategies\n",
        "  - Market commentary and opinions\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "- **Practical temperature settings**:\n",
        "\n",
        "  ```\n",
        "  Financial reports: T = 0.2\n",
        "  \"The company reported strong \n",
        "   quarterly earnings...\"\n",
        "  \n",
        "  Market analysis: T = 0.7  \n",
        "  \"Given current market dynamics,\n",
        "   we anticipate...\"\n",
        "  \n",
        "  Creative insights: T = 1.0\n",
        "  \"An unconventional perspective \n",
        "   suggests...\"\n",
        "  ```\n",
        "  \n",
        "- **Risk considerations**:\n",
        "  - High temperature may generate inaccurate numbers\n",
        "  - Low temperature may lack analytical depth\n",
        "  - Context-dependent optimization needed\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Mathematical Properties of Financial Token Generation\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Probability distribution constraints**:\n",
        "  - $\\sum_{i=1}^{|V|} p_i = 1$ (normalization)\n",
        "  - $p_i \\geq 0$ for all $i$ (non-negativity)\n",
        "  - $\\max_i p_i$ indicates model confidence\n",
        "  - Entropy $H = -\\sum_i p_i \\log p_i$ measures uncertainty\n",
        "  \n",
        "- **Information theory perspective**:\n",
        "  - Low entropy: Model is confident (peaked distribution)\n",
        "  - High entropy: Model is uncertain (flat distribution)\n",
        "  - Cross-entropy loss drives training optimization\n",
        "  - Perplexity measures model surprise: $2^H$\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "- **Financial implications**:\n",
        "  \n",
        "  **High confidence predictions**:\n",
        "  - Standard financial formulas\n",
        "  - Well-established market terminology\n",
        "  - Common financial ratios and metrics\n",
        "  \n",
        "  **High uncertainty predictions**:\n",
        "  - Novel market conditions\n",
        "  - Emerging financial instruments\n",
        "  - Ambiguous regulatory interpretations\n",
        "  \n",
        "- **Quality metrics**:\n",
        "  - Perplexity on financial test sets\n",
        "  - Domain-specific evaluation benchmarks\n",
        "  - Human expert evaluation scores\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Attention Influence on Token Probabilities\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Attention-weighted context**:\n",
        "  - Each token attends to relevant previous tokens\n",
        "  - Attention weights influence final representations\n",
        "  - Financial entities receive higher attention weights\n",
        "  - Temporal relationships in financial time series\n",
        "  \n",
        "- **Multi-head attention aggregation**:\n",
        "  - Different heads capture different relationships\n",
        "  - Some heads focus on syntax, others on semantics\n",
        "  - Financial domain heads for numerical relationships\n",
        "  - Entity-relation heads for company connections\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "- **Financial attention patterns**:\n",
        "  \n",
        "  **Entity-focused attention**:\n",
        "  - Company names → Financial metrics\n",
        "  - Dates → Performance periods\n",
        "  - Currency symbols → Numerical values\n",
        "  \n",
        "  **Causal attention**:\n",
        "  - Market events → Price movements\n",
        "  - Economic indicators → Sector performance\n",
        "  - Regulatory changes → Compliance requirements\n",
        "  \n",
        "  **Temporal attention**:\n",
        "  - Historical performance → Future projections\n",
        "  - Seasonal patterns in financial data\n",
        "  - Business cycle phase relationships\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Practical Implementation Considerations\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Computational efficiency**:\n",
        "  - Softmax computation over large vocabularies\n",
        "  - Memory requirements for probability distributions\n",
        "  - Hierarchical softmax for very large vocabularies\n",
        "  - Approximate methods for real-time applications\n",
        "  \n",
        "- **Numerical stability**:\n",
        "  - LogSumExp trick: $\\log(\\sum e^{x_i}) = \\max(x) + \\log(\\sum e^{x_i - \\max(x)})$\n",
        "  - Prevents overflow for large logits\n",
        "  - Critical for stable training and inference\n",
        "  - Especially important for financial applications\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "- **Implementation details**:\n",
        "\n",
        "  **Efficient softmax computation**:\n",
        "  ```\n",
        "  # Numerical stability approach\n",
        "  logits_max = max(logits)\n",
        "  logits_shifted = logits - logits_max\n",
        "  exp_logits = exp(logits_shifted)\n",
        "  probabilities = exp_logits / sum(exp_logits)\n",
        "  ```\n",
        "  \n",
        "  **Memory optimization**:\n",
        "  - Sparse attention for long sequences\n",
        "  - Gradient checkpointing\n",
        "  - Mixed precision training\n",
        "  - Model parallelism for large vocabularies\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Advanced Token Generation Strategies\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Repetition penalties**:\n",
        "  - Reduce probability of recently generated tokens\n",
        "  - Prevent repetitive financial phrases\n",
        "  - Encourage diverse vocabulary usage\n",
        "  - Balance between coherence and variety\n",
        "  \n",
        "- **Length penalties**:\n",
        "  - Bias toward longer or shorter sequences\n",
        "  - Important for financial document structure\n",
        "  - Executive summary vs. detailed analysis\n",
        "  - Regulatory compliance requirements\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "- **Constrained generation**:\n",
        "  \n",
        "  **Financial format constraints**:\n",
        "  - Currency formatting ($1,234.56)\n",
        "  - Percentage notation (12.34%)\n",
        "  - Date standardization (Q1 2024)\n",
        "  - Ticker symbol validation (AAPL, MSFT)\n",
        "  \n",
        "  **Content constraints**:\n",
        "  - Regulatory compliance checking\n",
        "  - Factual accuracy verification\n",
        "  - Risk disclosure requirements\n",
        "  - Professional tone maintenance\n",
        "  \n",
        "- **Quality control mechanisms**:\n",
        "  - Post-processing validation\n",
        "  - Rule-based filtering\n",
        "  - Confidence thresholding\n",
        "  - Human-in-the-loop verification\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Evaluation Metrics for Financial Token Generation\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Intrinsic metrics**:\n",
        "  - Perplexity on financial corpora\n",
        "  - BLEU scores for reference comparisons\n",
        "  - ROUGE scores for summarization tasks\n",
        "  - BERTScore for semantic similarity\n",
        "  \n",
        "- **Extrinsic metrics**:\n",
        "  - Financial accuracy of generated numbers\n",
        "  - Compliance with regulatory requirements\n",
        "  - Professional tone and style consistency\n",
        "  - Domain expert evaluation scores\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "- **Financial-specific evaluation**:\n",
        "  \n",
        "  **Numerical accuracy**:\n",
        "  - Correct calculation of financial ratios\n",
        "  - Consistent units and formatting\n",
        "  - Reasonable value ranges\n",
        "  - Mathematical relationship preservation\n",
        "  \n",
        "  **Domain coherence**:\n",
        "  - Appropriate financial terminology\n",
        "  - Logical sequence of financial concepts\n",
        "  - Compliance with industry standards\n",
        "  - Factual consistency with known data\n",
        "  \n",
        "- **Human evaluation criteria**:\n",
        "  - Professional appropriateness\n",
        "  - Analytical insight quality\n",
        "  - Recommendation soundness\n",
        "  - Risk assessment accuracy\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Debugging and Interpretability\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Probability analysis techniques**:\n",
        "  - Ranking top-k predictions at each step\n",
        "  - Analyzing probability mass distribution\n",
        "  - Identifying confident vs. uncertain predictions\n",
        "  - Tracking probability changes across layers\n",
        "  \n",
        "- **Attention visualization**:\n",
        "  - Which input tokens influenced final prediction\n",
        "  - Head-specific attention patterns\n",
        "  - Layer-wise attention evolution\n",
        "  - Financial entity relationship mapping\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "- **Diagnostic tools**:\n",
        "  \n",
        "  **Probability debugging**:\n",
        "  ```\n",
        "  Top 5 predictions:\n",
        "  1. \"earnings\" (p=0.34)\n",
        "  2. \"revenue\" (p=0.22) \n",
        "  3. \"profit\" (p=0.18)\n",
        "  4. \"income\" (p=0.12)\n",
        "  5. \"performance\" (p=0.08)\n",
        "  ```\n",
        "  \n",
        "  **Attention analysis**:\n",
        "  - Input: \"Q3 financial results show\"\n",
        "  - High attention: \"Q3\" → time context\n",
        "  - High attention: \"financial\" → domain context\n",
        "  - Output prediction: \"strong\" (financial qualifier)\n",
        "  \n",
        "- **Model introspection**:\n",
        "  - Layer-wise representation analysis\n",
        "  - Neuron activation patterns\n",
        "  - Concept emergence tracking\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Future Directions and Research\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "- **Improved sampling methods**:\n",
        "  - Contrastive search for coherent generation\n",
        "  - Typical sampling for natural distributions\n",
        "  - Mirostat for consistent text quality\n",
        "  - Adaptive temperature scheduling\n",
        "  \n",
        "- **Financial domain adaptations**:\n",
        "  - Specialized vocabularies for financial subdomains\n",
        "  - Multi-modal token generation (text + numbers)\n",
        "  - Structured output generation (tables, reports)\n",
        "  - Fact-grounded generation techniques\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "- **Emerging research areas**:\n",
        "  \n",
        "  **Controllable generation**:\n",
        "  - Style and tone control\n",
        "  - Risk level adjustment\n",
        "  - Audience-specific adaptation\n",
        "  - Compliance-aware generation\n",
        "  \n",
        "  **Multimodal integration**:\n",
        "  - Chart and graph generation\n",
        "  - Table structure prediction\n",
        "  - Visual financial data interpretation\n",
        "  - Cross-modal attention mechanisms\n",
        "  \n",
        "- **Evaluation advances**:\n",
        "  - Automated fact-checking\n",
        "  - Real-time accuracy assessment\n",
        "  - Bias detection and mitigation\n",
        "  - Professional quality metrics\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "\n",
        "# Tokenizers: Theory, Algorithms, and Practical Considerations\n",
        "\n",
        "## 1. Motivation\n",
        "Natural-language text is a sequence of Unicode code points that is **too sparse and high-entropy** for efficient statistical modeling.  A *tokenizer* transforms this raw character stream into a shorter sequence of discrete symbols drawn from a bounded *vocabulary* $V$, enabling language models to learn meaningful patterns.\n",
        "\n",
        "- Unicode code points are just a way to represent characters in a computer. Each character is assigned a unique number, which allows computers to handle text in different languages and scripts. E.g. `A` is represented by the code point `U+0041`, and `€` by `U+20AC`.\n",
        "\n",
        "- The complete set of Unicode code points is denoted $\\Sigma$, and the set of all finite-length strings over $\\Sigma$ is $\\Sigma^{*}$.  A tokenizer maps these strings to a sequence of tokens from a finite vocabulary $V$.\n",
        "\n",
        "**Definition.**  A tokenizer is a deterministic (or stochastic) mapping  \n",
        "$\\mathcal T : \\Sigma^{*} \\;\\longrightarrow\\; V^{*},$  \n",
        "where $\\Sigma$ is the character alphabet and $V$ is a finite set of tokens.\n",
        "\n",
        "- $V^*$ is the set of all finite-length sequences of tokens, including the empty sequence.\n",
        "\n",
        "## Example \n",
        "\n",
        "- Imagine you are an alien from a civilization with only 3 symbols (letters): `A`, `B`, and `C`. \n",
        "\n",
        "- $\\Sigma = \\{A, B, C\\}$ and $\\Sigma^{*} = \\{\\epsilon, A, B, C, AA, AB, AC, BA, BB, BC, CA, CB, CC, AAA, AAB, \\ldots\\}$. where $\\epsilon$ is the empty string.\n",
        "- You are creating your own LLM and you define a vocabulary $V = \\{A, B, C, AB, AC, BA, BB, BC\\}$. Recall that $|\\Sigma^*| = \\infty$.\n",
        "- Your tokenizer is a function $\\mathcal T : \\Sigma^{*} \\longrightarrow V^{*}$ that maps strings from $\\Sigma^{*}$ to sequences of tokens in $V^{*}$. For example\n",
        "$$\n",
        "\\mathcal T(AA) = (A, A), \\quad \\mathcal T(AB) = (AB), \\quad \\mathcal T(ACB) = (AC, B), \\quad \\mathcal T(ABC) = (AB, C).\n",
        "$$\n",
        "\n",
        "\n",
        "## 2. What would you like from a tokenizer?\n",
        "1. **Coverage** — every input string should be tokenisable without `UNK` (unknown) tokens. This means that the tokenizer should be able to handle any input string without producing tokens that are not in the vocabulary.\n",
        "\n",
        "2. **Compression** — minimise the expected token sequence length $\\mathbb E[|\\mathcal T(x)|] \\forall x\\in\\Sigma^{*}$. \n",
        "3. **Consistency** — identical substrings map to identical token sequences.  \n",
        "4. **Latency** — $\\mathcal T$ should run in $O(|x|)$ time (linear time).\n",
        "5. **Reversibility** — decoding $\\mathcal T^{-1}$ must recover the original text (modulo normalisation). E.g. $T^{-1}(AB,A,C)=ABAC$\n",
        "\n",
        "Balancing these criteria leads to different tokenization families.\n",
        "\n",
        "## 3. Taxonomy of Tokenizers\n",
        "| Family | Vocabulary Size $|V|$ | Sequence Length | OOV Risk | Typical Use |\n",
        "|--------|----------------------|-----------------|----------|-------------|\n",
        "| **Character** | $|\\Sigma|\\approx 10^{3}$ | High | None | OCR, robust systems |\n",
        "| **Word**      | $\\sim 10^{5}$          | Low  | High | Early NLP, controlled domains |\n",
        "| **Sub-word**  | $2\\times10^{4}$–$8\\times10^{4}$ | Medium | Very low | Modern LLMs |\n",
        "\n",
        "## 4. Training a Tokenizer\n",
        "- Tokenizers can be trained on a corpus of text to learn the most effective way to split the text into tokens.\n",
        "- The training process involves analyzing the frequency of character sequences in the corpus and selecting the most common sequences as tokens.\n",
        "- The goal is to create a vocabulary that balances coverage, compression, and consistency.\n",
        "- The most common algorithms for training tokenizers are:\n",
        "  - Byte-Pair Encoding (BPE): \n",
        "    - Iteratively merges the most frequent pairs of characters or tokens until a desired vocabulary size is reached.\n",
        "  - Unigram Language Model: \n",
        "    - Treats the tokenization problem as a probabilistic model, selecting tokens based on their likelihood of occurrence in the corpus.\n",
        "  - WordPiece:\n",
        "    - Similar to BPE, but uses a probabilistic approach to select the most likely tokens based on their frequency and context.\n",
        "- Most LLM providers do not train their own tokenizers, but rather use pre-trained tokenizers.\n",
        "\n",
        "\n",
        "# Classification and Scalability of LLMs\n",
        "\n",
        "## LLM Classification Framework\n",
        "\n",
        ":::: {.columns}\n",
        "\n",
        "::: {.column width=\"60%\"}\n",
        "**By Architecture:**\n",
        "- **Autoregressive (Decoder-only)**: GPT family, Claude, Llama\n",
        "- **Autoencoding (Encoder-only)**: BERT, RoBERTa, DistilBERT\n",
        "- **Encoder-Decoder**: T5, BART, FLAN-T5\n",
        "\n",
        "**By Training Approach:**\n",
        "- **Pre-training**: Self-supervised learning on massive corpora\n",
        "- **Fine-tuning**: Task-specific supervised learning\n",
        "- **Instruction-following**: Trained to follow human instructions\n",
        "- **Reinforcement Learning from Human Feedback (RLHF)**\n",
        ":::\n",
        "\n",
        "::: {.column width=\"40%\"}\n",
        "![LLM Classification](../../images/llm_classification_placeholder.png)\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "---\n",
        "\n",
        "## Model Architecture Types\n",
        "\n",
        "### Autoregressive Models (Decoder-only)\n",
        "- **Examples**: GPT-3/4, Claude, Llama, Mistral\n",
        "- **Training**: Next-token prediction\n",
        "- **Use cases**: Text generation, completion, conversation\n",
        "- **Advantages**: Excellent for generative tasks\n",
        "- **Disadvantages**: Less efficient for understanding tasks\n",
        "\n",
        "### Autoencoding Models (Encoder-only)\n",
        "- **Examples**: BERT, RoBERTa, DistilBERT\n",
        "- **Training**: Masked language modeling\n",
        "- **Use cases**: Classification, sentiment analysis, NER\n",
        "- **Advantages**: Bidirectional context, efficient for understanding\n",
        "- **Disadvantages**: Cannot generate text naturally\n",
        "\n",
        "### Encoder-Decoder Models\n",
        "- **Examples**: T5, BART, FLAN-T5\n",
        "- **Training**: Various objectives (span corruption, denoising)\n",
        "- **Use cases**: Translation, summarization, question answering\n",
        "- **Advantages**: Flexible for both understanding and generation\n",
        "- **Disadvantages**: More complex architecture\n",
        "\n",
        "---\n",
        "\n",
        "## Training Paradigms\n",
        "\n",
        "### Pre-training Objectives\n",
        "\n",
        "**Autoregressive (AR):**\n",
        "$$P(\\text{sequence}) = \\prod_{i=1}^{n} P(x_i | x_1, \\ldots, x_{i-1})$$\n",
        "\n",
        "**Masked Language Modeling (MLM):**\n",
        "- Randomly mask tokens and predict them\n",
        "- Bidirectional context understanding\n",
        "\n",
        "**Span Corruption (T5-style):**\n",
        "- Mask contiguous spans of text\n",
        "- Predict the masked spans\n",
        "\n",
        "---\n",
        "\n",
        "## Model Scale Categories\n",
        "\n",
        "| Category | Parameters | Examples | Characteristics |\n",
        "|----------|-----------|----------|----------------|\n",
        "| **Small** | <1B | DistilBERT, ALBERT | Fast inference, limited capabilities |\n",
        "| **Base** | 1B-10B | BERT-Large, GPT-2 | Good balance of performance/efficiency |\n",
        "| **Large** | 10B-100B | GPT-3, T5-XXL | Strong performance, higher compute |\n",
        "| **Extra Large** | 100B+ | GPT-4, PaLM, Claude | State-of-the-art, very high compute |\n",
        "\n",
        "---\n",
        "\n",
        "## Specialized LLM Categories\n",
        "\n",
        "### Code Models\n",
        "- **GitHub Copilot** (based on Codex)\n",
        "- **CodeT5**, **InCoder**, **CodeGen**\n",
        "- **StarCoder**, **WizardCoder**\n",
        "\n",
        "### Financial Domain Models\n",
        "- **BloombergGPT**: 50B parameters, trained on financial data\n",
        "- **FinBERT**: BERT fine-tuned for financial sentiment\n",
        "- **PaLM-Finance**: Specialized for financial reasoning\n",
        "\n",
        "### Multimodal Models\n",
        "- **GPT-4V**: Vision capabilities\n",
        "- **Claude 3**: Image understanding\n",
        "- **DALL-E 3**: Text-to-image generation\n",
        "\n",
        "---\n",
        "\n",
        "# LLM Providers\n",
        "\n",
        "## Frontier Labs (Global Leaders)\n",
        "\n",
        "| Company | Flagship or Latest LLM(s) (2024 – 25) | Brief description |\n",
        "|---------|---------------------------------------|-------------------|\n",
        "| **OpenAI** | GPT-4o, GPT-4.1 mini | Original ChatGPT maker; continues to set benchmark accuracy and multimodality |\n",
        "| **Anthropic** | Claude 4 (Opus & Sonnet) | Safety-first research lab spun out of OpenAI; emphasises \"constitutional AI\" |\n",
        "| **Google DeepMind** | Gemini 2.5 Pro | Multimodal model powering Google Search, Workspace & the Gemini app |\n",
        "| **Microsoft (Azure AI)** | Phi-3 / Phi-4 SLM family | Compact open-source \"small language models\"; also resells OpenAI models via Azure |\n",
        "| **Meta** | Llama 4 (Scout & Maverick) | Open-weight, natively-multimodal successor to Llama 2/3 |\n",
        "\n",
        "---\n",
        "\n",
        "## Big Tech & Hardware Companies\n",
        "\n",
        "| Company | Flagship or Latest LLM(s) (2024 – 25) | Brief description |\n",
        "|---------|---------------------------------------|-------------------|\n",
        "| **Amazon AWS** | Titan Text G1 (Premier / Express) | Proprietary Bedrock-hosted models for enterprise workloads |\n",
        "| **Apple** | 3 B \"Apple Intelligence\" model | First fully on-device LLM for iPhone, iPad & Mac |\n",
        "| **xAI** | Grok 3 (Think / Fast) | Elon Musk-backed lab focused on real-time reasoning and openness |\n",
        "| **NVIDIA** | Nemotron-4 340B | Open models optimised for synthetic-data generation and self-training |\n",
        "\n",
        "---\n",
        "\n",
        "## Enterprise & Specialized Players (Part 1)\n",
        "\n",
        "| Company | Flagship or Latest LLM(s) (2024 – 25) | Brief description |\n",
        "|---------|---------------------------------------|-------------------|\n",
        "| **Mistral AI** | Mistral Medium 3 | High-performance, permissively licensed models at low latency & cost |\n",
        "| **Cohere** | Command R & Command A | Retrieval-augmented, long-context LLMs built for private data |\n",
        "| **AI21 Labs** | Jurassic-3, Jamba | Early entrant offering controllable text generation APIs |\n",
        "| **Databricks** | DBRX | Open-weight Mixture-of-Experts tuned for data-engineering use cases |\n",
        "| **Snowflake** | Arctic | 128 k-context Apache-licensed model for cost-efficient enterprise AI |\n",
        "| **IBM** | Granite 4.0 family | Trustworthy, business-oriented models aligned with EU AI Act |\n",
        "\n",
        "---\n",
        "\n",
        "## Enterprise & Specialized Players (Part 2)\n",
        "\n",
        "| Company | Flagship or Latest LLM(s) (2024 – 25) | Brief description |\n",
        "|---------|---------------------------------------|-------------------|\n",
        "| **Salesforce AI** | xGen (small / code) | Long-context, domain-tuned models powering Einstein Copilot |\n",
        "| **Stability AI** | Stable LM 2 (1.6 B → 12 B) | Lightweight multilingual open-source models for consumer GPUs |\n",
        "| **Adept AI** | Fuyu-Heavy & Fuyu-8B | Multimodal transformers designed for agentic tasks and UI control |\n",
        "| **Reka AI** | Reka Flash 21 B | Efficient multilingual reasoning model for real-time & edge |\n",
        "| **Aleph Alpha** | Luminous / Pharia | European sovereign stack with explainability APIs |\n",
        "| **Together AI** | RedPajama-v2 & training-as-a-service | Open datasets + cloud for fine-tuning and hosting OSS models |\n",
        "\n",
        "---\n",
        "\n",
        "## China Ecosystem \n",
        "\n",
        "What makes China's LLM ecosystem unique is its rapid development, large-scale models, and focus on domestic applications. The Chinese government has also been supportive of AI initiatives, leading to a vibrant ecosystem.\n",
        "\n",
        "The US has banned the export of advanced chips to China, which has led to a focus on developing indigenous AI capabilities. Chinese companies are also focusing on building large-scale models that can handle the Chinese language and cultural context effectively.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "| Company | Flagship or Latest LLM(s) (2024 – 25) | Brief description |\n",
        "|---------|---------------------------------------|-------------------|\n",
        "| **Baidu** | Ernie 4.0 Turbo | Search-integrated LLM; 300 M+ users |\n",
        "| **Alibaba Cloud** | Qwen 3 family | Hybrid-reasoning models matching frontier benchmarks |\n",
        "| **Tencent** | Hunyuan Turbo S | Fast Transformer-Mamba MoE model for Chinese & maths tasks |\n",
        "| **Huawei Cloud** | PanGu 5 / Ultra MoE | Large-scale models optimised for Ascend NPUs & on-prem deployment |\n",
        "| **SenseTime** | SenseNova 5.5 | China's first real-time multimodal model series |\n",
        "| **Zhipu AI** | GLM-4 (Air / Flash) | Open-source bilingual models with free API tier |\n",
        "| **DeepSeek** | DeepSeek-V3 (671 B MoE) | Open-weight MoE excelling at maths & code |\n",
        "| **01.AI** | Yi-1.5 (6 B → 34 B) | Apache-licensed zh-en models for community fine-tuning |\n",
        "\n",
        "---\n",
        "\n",
        "## Open-Source & Community\n",
        "\n",
        "| Company | Flagship or Latest LLM(s) (2024 – 25) | Brief description |\n",
        "|---------|---------------------------------------|-------------------|\n",
        "| **BigScience + Hugging Face** | BLOOM 176 B | First 100 B+ multilingual model released with full weights |\n",
        "| **Eleuther AI** | GPT-NeoX-20B | Volunteer collective behind \"The Pile\"; open 20 B model |\n",
        "| **Cerebras Systems** | Cerebras-GPT family | 111 M → 13 B models trained on wafer-scale CS-2 hardware |\n",
        "| **Ollama** | Ollama LLMs | Open-source models with a focus on simplicity and ease of use |\n",
        "\n",
        "- [A Survey of LLM Surveys](https://github.com/NiuTrans/ABigSurveyOfLLMs)\n"
      ],
      "id": "9c8ab0a9"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}