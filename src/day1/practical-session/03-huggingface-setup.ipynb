{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5d7f91c",
   "metadata": {},
   "source": [
    "# HuggingFace Setup and Local Model Usage\n",
    "\n",
    "In this notebook, we'll set up HuggingFace and learn how to work with language models locally. HuggingFace is the most popular platform for sharing and using pre-trained models.\n",
    "\n",
    "## Learning Objectives\n",
    "- Set up HuggingFace transformers library\n",
    "- Understand the HuggingFace ecosystem\n",
    "- Load and use models locally\n",
    "- Practice with different model types\n",
    "- Understand model sizes and hardware requirements\n",
    "- Learn caching and optimization strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87e2aec",
   "metadata": {},
   "source": [
    "## 1. HuggingFace Ecosystem Overview\n",
    "\n",
    "### Key Components:\n",
    "- **ü§ó Hub**: Repository for models, datasets, and demos\n",
    "- **Transformers**: Library for using pre-trained models\n",
    "- **Datasets**: Library for accessing and processing datasets\n",
    "- **Tokenizers**: Fast tokenization library\n",
    "- **Accelerate**: Distributed training and inference\n",
    "- **Gradio**: Quick UI creation for ML models\n",
    "\n",
    "### Popular Models for Finance:\n",
    "- **FinBERT**: Financial sentiment analysis\n",
    "- **BloombergGPT**: Financial domain language model\n",
    "- **RoBERTa**: Robust text understanding\n",
    "- **GPT-2/GPT-Neo**: Text generation\n",
    "- **T5**: Text-to-text transfer transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97a91a8",
   "metadata": {},
   "source": [
    "## 2. Environment Setup and Authentication\n",
    "\n",
    "Let's start by setting up our HuggingFace environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1dc9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import essential libraries\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModel, AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM, pipeline, set_seed\n",
    ")\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(42)\n",
    "\n",
    "print(\"ü§ó HuggingFace Transformers Setup\")\n",
    "print(f\"Transformers version: {__import__('transformers').__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "# Check available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"üöÄ Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"üçé Using Apple MPS\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"üíª Using CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85656684",
   "metadata": {},
   "source": [
    "## 3. HuggingFace Authentication (Optional)\n",
    "\n",
    "Some models require authentication. Let's set up the HuggingFace token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a788903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables for HuggingFace token\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load .env file if it exists\n",
    "if os.path.exists('.env'):\n",
    "    load_dotenv()\n",
    "    print(\"‚úÖ Loaded .env file\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No .env file found\")\n",
    "    print(\"   Create one from .env.example if you need API access\")\n",
    "\n",
    "# Check for HuggingFace token\n",
    "hf_token = os.getenv('HUGGINGFACE_API_KEY')\n",
    "\n",
    "if hf_token:\n",
    "    print(\"üîë HuggingFace token found\")\n",
    "    # Login to HuggingFace Hub (optional but recommended)\n",
    "    try:\n",
    "        from huggingface_hub import login\n",
    "        login(token=hf_token)\n",
    "        print(\"‚úÖ Successfully logged in to HuggingFace Hub\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Could not login to HuggingFace Hub: {e}\")\n",
    "        print(\"   This is fine for public models\")\n",
    "else:\n",
    "    print(\"üîì No HuggingFace token - using public models only\")\n",
    "    print(\"   To access private models, add HUGGINGFACE_API_KEY to .env\")\n",
    "\n",
    "print(\"\\nüìù HuggingFace token info:\")\n",
    "print(\"   ‚Ä¢ Get token: https://huggingface.co/settings/tokens\")\n",
    "print(\"   ‚Ä¢ Token types: Read, Write, or Fine-grained\")\n",
    "print(\"   ‚Ä¢ Required for: Private models, uploading models, some datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40dc7e0",
   "metadata": {},
   "source": [
    "## 4. Understanding Model Sizes and Requirements\n",
    "\n",
    "Let's explore different model sizes and their requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6953b733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model size reference for financial applications\n",
    "model_info = {\n",
    "    \"Small Models (< 500M params)\": {\n",
    "        \"examples\": [\"distilbert-base-uncased\", \"albert-base-v2\"],\n",
    "        \"memory\": \"~2GB RAM\",\n",
    "        \"speed\": \"Fast\",\n",
    "        \"use_case\": \"Classification, simple NLP tasks\"\n",
    "    },\n",
    "    \"Medium Models (500M - 2B params)\": {\n",
    "        \"examples\": [\"bert-base-uncased\", \"roberta-base\"],\n",
    "        \"memory\": \"~4-8GB RAM\",\n",
    "        \"speed\": \"Medium\",\n",
    "        \"use_case\": \"Most financial NLP tasks\"\n",
    "    },\n",
    "    \"Large Models (2B - 7B params)\": {\n",
    "        \"examples\": [\"gpt2-xl\", \"microsoft/DialoGPT-large\"],\n",
    "        \"memory\": \"~16-32GB RAM\",\n",
    "        \"speed\": \"Slow\",\n",
    "        \"use_case\": \"Text generation, complex reasoning\"\n",
    "    },\n",
    "    \"Extra Large Models (7B+ params)\": {\n",
    "        \"examples\": [\"microsoft/DialoGPT-large\", \"EleutherAI/gpt-neo-2.7B\"],\n",
    "        \"memory\": \"~32GB+ RAM or GPU\",\n",
    "        \"speed\": \"Very slow on CPU\",\n",
    "        \"use_case\": \"Advanced generation, research\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üìä Model Size Guide for Financial Applications\\n\")\n",
    "for category, info in model_info.items():\n",
    "    print(f\"üîπ {category}\")\n",
    "    print(f\"   Examples: {', '.join(info['examples'])}\")\n",
    "    print(f\"   Memory: {info['memory']}\")\n",
    "    print(f\"   Speed: {info['speed']}\")\n",
    "    print(f\"   Best for: {info['use_case']}\\n\")\n",
    "\n",
    "# Check available memory\n",
    "import psutil\n",
    "available_memory = psutil.virtual_memory().available / (1024**3)  # GB\n",
    "total_memory = psutil.virtual_memory().total / (1024**3)  # GB\n",
    "\n",
    "print(f\"üíæ Your System:\")\n",
    "print(f\"   Total RAM: {total_memory:.1f} GB\")\n",
    "print(f\"   Available RAM: {available_memory:.1f} GB\")\n",
    "\n",
    "# Recommendations based on available memory\n",
    "if available_memory < 4:\n",
    "    print(\"\\nüí° Recommendation: Use small models (DistilBERT, ALBERT)\")\n",
    "elif available_memory < 16:\n",
    "    print(\"\\nüí° Recommendation: Medium models work well (BERT, RoBERTa)\")\n",
    "else:\n",
    "    print(\"\\nüí° Recommendation: You can run large models (GPT-2, T5-large)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6546e4",
   "metadata": {},
   "source": [
    "## 5. Loading Your First Model: Financial Sentiment Analysis\n",
    "\n",
    "Let's start with a practical example - financial sentiment analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc6279e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a financial sentiment analysis model\n",
    "print(\"üìà Loading Financial Sentiment Analysis Model...\\n\")\n",
    "\n",
    "# Using a financial domain-specific model\n",
    "model_name = \"ProsusAI/finbert\"\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model\n",
    "    print(f\"üîÑ Loading {model_name}...\")\n",
    "    \n",
    "    # Using pipeline for easy inference\n",
    "    finbert = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=model_name,\n",
    "        tokenizer=model_name,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ FinBERT loaded successfully!\")\n",
    "    \n",
    "    # Test with financial texts\n",
    "    financial_texts = [\n",
    "        \"The company reported strong quarterly earnings with revenue up 15%\",\n",
    "        \"Stock prices fell dramatically after the disappointing earnings report\",\n",
    "        \"The market remained stable with mixed trading volumes\",\n",
    "        \"Apple announced a new product launch that exceeded expectations\",\n",
    "        \"The Federal Reserve raised interest rates by 0.5%\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nüîç Analyzing Financial Sentiment:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i, text in enumerate(financial_texts, 1):\n",
    "        result = finbert(text)\n",
    "        sentiment = result[0]['label']\n",
    "        confidence = result[0]['score']\n",
    "        \n",
    "        print(f\"\\n{i}. Text: {text}\")\n",
    "        print(f\"   Sentiment: {sentiment} (confidence: {confidence:.3f})\")\n",
    "        \n",
    "        # Add emoji for visualization\n",
    "        if sentiment == 'positive':\n",
    "            emoji = \"üìà üü¢\"\n",
    "        elif sentiment == 'negative':\n",
    "            emoji = \"üìâ üî¥\"\n",
    "        else:\n",
    "            emoji = \"‚û°Ô∏è üü°\"\n",
    "        \n",
    "        print(f\"   {emoji}\")\n",
    "    \n",
    "    print(\"\\n‚ú® FinBERT Analysis Complete!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading FinBERT: {e}\")\n",
    "    print(\"\\nüîÑ Falling back to general sentiment model...\")\n",
    "    \n",
    "    # Fallback to a smaller, general sentiment model\n",
    "    try:\n",
    "        general_sentiment = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ General sentiment model loaded!\")\n",
    "        \n",
    "        # Test with one example\n",
    "        test_text = \"The company reported strong quarterly earnings\"\n",
    "        result = general_sentiment(test_text)\n",
    "        print(f\"\\nTest: {test_text}\")\n",
    "        print(f\"Result: {result}\")\n",
    "        \n",
    "    except Exception as e2:\n",
    "        print(f\"‚ùå Fallback also failed: {e2}\")\n",
    "        print(\"   This might be due to network issues or insufficient memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2644983e",
   "metadata": {},
   "source": [
    "## 6. Working with Different Model Types\n",
    "\n",
    "Let's explore different types of models commonly used in finance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abfed1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different model types\n",
    "print(\"üéØ Exploring Different Model Types for Finance\\n\")\n",
    "\n",
    "# 1. Classification Model (Already done above with FinBERT)\n",
    "print(\"1Ô∏è‚É£ Classification Models:\")\n",
    "print(\"   ‚úÖ FinBERT (financial sentiment) - demonstrated above\")\n",
    "print(\"   ‚Ä¢ Use case: Sentiment analysis, document classification\")\n",
    "print(\"   ‚Ä¢ Output: Probability scores for predefined classes\\n\")\n",
    "\n",
    "# 2. Feature Extraction Model\n",
    "print(\"2Ô∏è‚É£ Feature Extraction Models:\")\n",
    "try:\n",
    "    # Load a lightweight model for feature extraction\n",
    "    feature_model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    feature_extractor = pipeline(\n",
    "        \"feature-extraction\",\n",
    "        model=feature_model_name,\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    # Extract features from financial text\n",
    "    financial_text = \"The Federal Reserve announced an interest rate cut\"\n",
    "    features = feature_extractor(financial_text)\n",
    "    \n",
    "    print(f\"   ‚úÖ Loaded: {feature_model_name}\")\n",
    "    print(f\"   ‚Ä¢ Text: {financial_text}\")\n",
    "    print(f\"   ‚Ä¢ Feature vector shape: {len(features[0])} dimensions\")\n",
    "    print(f\"   ‚Ä¢ Use case: Document similarity, semantic search, clustering\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Could not load feature extraction model: {e}\\n\")\n",
    "\n",
    "# 3. Text Generation Model (Lightweight)\n",
    "print(\"3Ô∏è‚É£ Text Generation Models:\")\n",
    "try:\n",
    "    # Use a small GPT-2 model for demonstration\n",
    "    generator = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=\"gpt2\",\n",
    "        device=0 if torch.cuda.is_available() else -1,\n",
    "        pad_token_id=50256  # GPT-2 doesn't have a pad token\n",
    "    )\n",
    "    \n",
    "    # Generate financial text\n",
    "    prompt = \"The stock market today\"\n",
    "    generated = generator(\n",
    "        prompt,\n",
    "        max_length=50,\n",
    "        num_return_sequences=1,\n",
    "        temperature=0.7,\n",
    "        do_sample=True,\n",
    "        pad_token_id=50256\n",
    "    )\n",
    "    \n",
    "    print(f\"   ‚úÖ Loaded: GPT-2\")\n",
    "    print(f\"   ‚Ä¢ Prompt: {prompt}\")\n",
    "    print(f\"   ‚Ä¢ Generated: {generated[0]['generated_text']}\")\n",
    "    print(f\"   ‚Ä¢ Use case: Report generation, content creation\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Could not load text generation model: {e}\\n\")\n",
    "\n",
    "# 4. Question Answering Model\n",
    "print(\"4Ô∏è‚É£ Question Answering Models:\")\n",
    "try:\n",
    "    qa_pipeline = pipeline(\n",
    "        \"question-answering\",\n",
    "        model=\"distilbert-base-cased-distilled-squad\",\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    \n",
    "    # Financial Q&A example\n",
    "    context = \"\"\"\n",
    "    Apple Inc. reported quarterly revenue of $81.4 billion, up 8% year over year. \n",
    "    The company's iPhone sales were particularly strong, contributing $51.3 billion \n",
    "    to total revenue. Services revenue reached $19.2 billion, marking a 12% increase.\n",
    "    \"\"\"\n",
    "    \n",
    "    question = \"What was Apple's total quarterly revenue?\"\n",
    "    \n",
    "    answer = qa_pipeline(question=question, context=context)\n",
    "    \n",
    "    print(f\"   ‚úÖ Loaded: DistilBERT QA\")\n",
    "    print(f\"   ‚Ä¢ Question: {question}\")\n",
    "    print(f\"   ‚Ä¢ Answer: {answer['answer']} (confidence: {answer['score']:.3f})\")\n",
    "    print(f\"   ‚Ä¢ Use case: Document Q&A, information extraction\\n\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚ùå Could not load QA model: {e}\\n\")\n",
    "\n",
    "print(\"üéâ Model exploration complete!\")\n",
    "print(\"\\nüí° Key takeaways:\")\n",
    "print(\"   ‚Ä¢ Different models for different tasks\")\n",
    "print(\"   ‚Ä¢ Start with smaller models for testing\")\n",
    "print(\"   ‚Ä¢ Use pipelines for quick prototyping\")\n",
    "print(\"   ‚Ä¢ Consider domain-specific models for finance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66913a5",
   "metadata": {},
   "source": [
    "## 7. Model Caching and Optimization\n",
    "\n",
    "Understanding how models are cached and optimized is important for efficient development:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6596d154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding model caching and optimization\n",
    "import transformers\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üíæ Model Caching and Optimization\\n\")\n",
    "\n",
    "# 1. Check cache directory\n",
    "cache_dir = transformers.utils.hub.TRANSFORMERS_CACHE\n",
    "print(f\"üóÇÔ∏è HuggingFace Cache Directory:\")\n",
    "print(f\"   Location: {cache_dir}\")\n",
    "\n",
    "# Check cache size\n",
    "try:\n",
    "    cache_path = Path(cache_dir)\n",
    "    if cache_path.exists():\n",
    "        cache_size = sum(f.stat().st_size for f in cache_path.rglob('*') if f.is_file())\n",
    "        cache_size_gb = cache_size / (1024**3)\n",
    "        print(f\"   Size: {cache_size_gb:.2f} GB\")\n",
    "        \n",
    "        # List some cached models\n",
    "        cached_models = [d.name for d in cache_path.iterdir() if d.is_dir()]\n",
    "        if cached_models:\n",
    "            print(f\"   Cached models: {len(cached_models)} items\")\n",
    "            print(f\"   Recent: {cached_models[:3]}...\" if len(cached_models) > 3 else f\"   All: {cached_models}\")\n",
    "    else:\n",
    "        print(\"   Cache directory not found (no models cached yet)\")\n",
    "except Exception as e:\n",
    "    print(f\"   Could not analyze cache: {e}\")\n",
    "\n",
    "print(\"\\n‚ö° Optimization Strategies:\")\n",
    "\n",
    "# 2. Model quantization example\n",
    "print(\"\\n1Ô∏è‚É£ Model Quantization (Reducing Memory):\")\n",
    "print(\"   ‚Ä¢ 8-bit quantization: ~50% memory reduction\")\n",
    "print(\"   ‚Ä¢ 4-bit quantization: ~75% memory reduction\")\n",
    "print(\"   ‚Ä¢ Trade-off: Slight accuracy loss for much lower memory\")\n",
    "\n",
    "# 3. Efficient model loading\n",
    "print(\"\\n2Ô∏è‚É£ Efficient Loading Strategies:\")\n",
    "strategies = {\n",
    "    \"device_map='auto'\": \"Automatically distribute model across available devices\",\n",
    "    \"torch_dtype=torch.float16\": \"Use half precision (50% memory reduction)\",\n",
    "    \"low_cpu_mem_usage=True\": \"Reduce CPU memory during loading\",\n",
    "    \"load_in_8bit=True\": \"Load model in 8-bit precision\",\n",
    "    \"offload_folder='./offload'\": \"Offload weights to disk when needed\"\n",
    "}\n",
    "\n",
    "for strategy, description in strategies.items():\n",
    "    print(f\"   ‚Ä¢ {strategy}: {description}\")\n",
    "\n",
    "# 4. Practical example of optimized loading\n",
    "print(\"\\n3Ô∏è‚É£ Optimized Loading Example:\")\n",
    "print(\"```python\")\n",
    "print(\"# Memory-efficient model loading\")\n",
    "print(\"model = AutoModelForCausalLM.from_pretrained(\")\n",
    "print(\"    'microsoft/DialoGPT-large',\")\n",
    "print(\"    torch_dtype=torch.float16,  # Half precision\")\n",
    "print(\"    device_map='auto',          # Auto device placement\")\n",
    "print(\"    low_cpu_mem_usage=True,     # Reduce CPU usage\")\n",
    "print(\")\")\n",
    "print(\"```\")\n",
    "\n",
    "print(\"\\nüîß Cache Management:\")\n",
    "print(\"   ‚Ä¢ Models are automatically cached on first download\")\n",
    "print(\"   ‚Ä¢ Subsequent loads are much faster\")\n",
    "print(\"   ‚Ä¢ Clear cache if disk space is limited\")\n",
    "print(\"   ‚Ä¢ Use offline mode when internet is unavailable\")\n",
    "\n",
    "print(\"\\nüìä Memory Usage Tips:\")\n",
    "memory_tips = [\n",
    "    \"Monitor GPU/RAM usage during inference\",\n",
    "    \"Use smaller batch sizes if memory is limited\",\n",
    "    \"Clear model from memory when switching models\",\n",
    "    \"Consider model distillation for production\",\n",
    "    \"Use gradient checkpointing for training\"\n",
    "]\n",
    "\n",
    "for i, tip in enumerate(memory_tips, 1):\n",
    "    print(f\"   {i}. {tip}\")\n",
    "\n",
    "# 5. Check current memory usage\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
    "    gpu_allocated = torch.cuda.memory_allocated() / (1024**3)\n",
    "    gpu_reserved = torch.cuda.memory_reserved() / (1024**3)\n",
    "    \n",
    "    print(f\"\\nüñ•Ô∏è GPU Memory Status:\")\n",
    "    print(f\"   Total: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   Allocated: {gpu_allocated:.2f} GB\")\n",
    "    print(f\"   Reserved: {gpu_reserved:.2f} GB\")\n",
    "    print(f\"   Available: {gpu_memory - gpu_reserved:.1f} GB\")\n",
    "\n",
    "print(\"\\nüí° Next steps:\")\n",
    "print(\"   ‚Ä¢ Experiment with different model sizes\")\n",
    "print(\"   ‚Ä¢ Test optimization strategies\")\n",
    "print(\"   ‚Ä¢ Monitor performance vs. accuracy trade-offs\")\n",
    "print(\"   ‚Ä¢ Set up API connections for larger models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d904ee6d",
   "metadata": {},
   "source": [
    "## 8. Practical Exercise: Financial Text Analysis\n",
    "\n",
    "Let's put it all together with a comprehensive financial text analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1f0b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive financial text analysis pipeline\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(\"üìä Financial Text Analysis Pipeline\\n\")\n",
    "\n",
    "# Sample financial news/reports\n",
    "financial_texts = [\n",
    "    \"Apple Inc. reported record quarterly revenue of $123.9 billion, exceeding analyst expectations.\",\n",
    "    \"Tesla stock plummeted 15% following disappointing delivery numbers for Q3.\",\n",
    "    \"The Federal Reserve maintained interest rates at current levels, signaling a cautious approach.\",\n",
    "    \"Microsoft Azure cloud services showed strong growth with 50% year-over-year increase.\",\n",
    "    \"Oil prices surged to $85 per barrel amid supply chain disruptions in the Middle East.\",\n",
    "    \"The unemployment rate fell to 3.5%, indicating a robust labor market recovery.\",\n",
    "    \"Cryptocurrency markets experienced volatility with Bitcoin dropping below $30,000.\",\n",
    "    \"Inflation concerns continue to weigh on consumer sentiment and spending patterns.\",\n",
    "    \"Goldman Sachs upgraded its outlook for emerging markets in the upcoming quarter.\",\n",
    "    \"Tech IPOs showed mixed results with some companies struggling to meet initial valuations.\"\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'text': financial_texts,\n",
    "    'id': range(1, len(financial_texts) + 1)\n",
    "})\n",
    "\n",
    "print(f\"üìà Analyzing {len(financial_texts)} financial texts...\\n\")\n",
    "\n",
    "# Try to load sentiment model (with fallback)\n",
    "sentiment_model = None\n",
    "try:\n",
    "    # Try financial-specific model first\n",
    "    sentiment_model = pipeline(\n",
    "        \"sentiment-analysis\",\n",
    "        model=\"ProsusAI/finbert\",\n",
    "        device=0 if torch.cuda.is_available() else -1\n",
    "    )\n",
    "    model_name = \"FinBERT\"\n",
    "    print(\"‚úÖ Using FinBERT for sentiment analysis\")\n",
    "except:\n",
    "    try:\n",
    "        # Fallback to general model\n",
    "        sentiment_model = pipeline(\n",
    "            \"sentiment-analysis\",\n",
    "            model=\"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        model_name = \"RoBERTa\"\n",
    "        print(\"‚úÖ Using RoBERTa for sentiment analysis\")\n",
    "    except:\n",
    "        print(\"‚ùå Could not load sentiment model\")\n",
    "\n",
    "# Perform sentiment analysis\n",
    "if sentiment_model:\n",
    "    print(\"\\nüîç Performing sentiment analysis...\")\n",
    "    \n",
    "    sentiments = []\n",
    "    confidences = []\n",
    "    \n",
    "    for text in df['text']:\n",
    "        try:\n",
    "            result = sentiment_model(text)\n",
    "            sentiment = result[0]['label'].lower()\n",
    "            confidence = result[0]['score']\n",
    "            \n",
    "            # Normalize sentiment labels\n",
    "            if 'pos' in sentiment or sentiment == 'positive':\n",
    "                sentiment = 'positive'\n",
    "            elif 'neg' in sentiment or sentiment == 'negative':\n",
    "                sentiment = 'negative'\n",
    "            else:\n",
    "                sentiment = 'neutral'\n",
    "            \n",
    "            sentiments.append(sentiment)\n",
    "            confidences.append(confidence)\n",
    "        except Exception as e:\n",
    "            print(f\"   Error processing text: {e}\")\n",
    "            sentiments.append('neutral')\n",
    "            confidences.append(0.5)\n",
    "    \n",
    "    # Add results to DataFrame\n",
    "    df['sentiment'] = sentiments\n",
    "    df['confidence'] = confidences\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nüìã Analysis Results:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        # Add emoji for sentiment\n",
    "        if row['sentiment'] == 'positive':\n",
    "            emoji = \"üìà\"\n",
    "        elif row['sentiment'] == 'negative':\n",
    "            emoji = \"üìâ\"\n",
    "        else:\n",
    "            emoji = \"‚û°Ô∏è\"\n",
    "        \n",
    "        print(f\"\\n{row['id']:2d}. {row['text'][:60]}...\")\n",
    "        print(f\"    {emoji} {row['sentiment'].title()} (confidence: {row['confidence']:.3f})\")\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Subplot 1: Sentiment distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sentiment_counts = df['sentiment'].value_counts()\n",
    "    colors = {'positive': 'green', 'negative': 'red', 'neutral': 'orange'}\n",
    "    bars = plt.bar(sentiment_counts.index, sentiment_counts.values, \n",
    "                   color=[colors.get(x, 'blue') for x in sentiment_counts.index])\n",
    "    plt.title(f'Sentiment Distribution\\n({model_name} Analysis)', fontsize=12)\n",
    "    plt.ylabel('Number of Texts')\n",
    "    plt.xlabel('Sentiment')\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + 0.1,\n",
    "                f'{int(height)}', ha='center', va='bottom')\n",
    "    \n",
    "    # Subplot 2: Confidence distribution\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(df['confidence'], bins=10, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.title('Confidence Score Distribution', fontsize=12)\n",
    "    plt.xlabel('Confidence Score')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.axvline(df['confidence'].mean(), color='red', linestyle='--', \n",
    "                label=f'Mean: {df[\"confidence\"].mean():.3f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('financial_sentiment_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\nüìä Summary Statistics:\")\n",
    "    print(f\"   Total texts analyzed: {len(df)}\")\n",
    "    print(f\"   Positive sentiment: {sentiment_counts.get('positive', 0)} ({sentiment_counts.get('positive', 0)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Negative sentiment: {sentiment_counts.get('negative', 0)} ({sentiment_counts.get('negative', 0)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Neutral sentiment: {sentiment_counts.get('neutral', 0)} ({sentiment_counts.get('neutral', 0)/len(df)*100:.1f}%)\")\n",
    "    print(f\"   Average confidence: {df['confidence'].mean():.3f}\")\n",
    "    print(f\"   Model used: {model_name}\")\n",
    "    \n",
    "    # Save results\n",
    "    df.to_csv('financial_sentiment_results.csv', index=False)\n",
    "    print(\"\\nüíæ Results saved to 'financial_sentiment_results.csv'\")\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Sentiment analysis skipped due to model loading issues\")\n",
    "\n",
    "print(\"\\nüéâ Financial text analysis complete!\")\n",
    "print(\"\\nüí° This demonstrates:\")\n",
    "print(\"   ‚Ä¢ Loading and using pre-trained models\")\n",
    "print(\"   ‚Ä¢ Processing multiple texts efficiently\")\n",
    "print(\"   ‚Ä¢ Handling errors and fallbacks gracefully\")\n",
    "print(\"   ‚Ä¢ Visualizing and saving results\")\n",
    "print(\"   ‚Ä¢ Real-world application to financial data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08110ded",
   "metadata": {},
   "source": [
    "## 9. Summary and Next Steps\n",
    "\n",
    "### ‚úÖ What We've Accomplished:\n",
    "\n",
    "1. **HuggingFace Setup**: Installed and configured the transformers library\n",
    "2. **Authentication**: Set up HuggingFace Hub access (optional)\n",
    "3. **Model Understanding**: Learned about different model sizes and requirements\n",
    "4. **Practical Application**: Used financial sentiment analysis models\n",
    "5. **Model Types**: Explored classification, generation, and Q&A models\n",
    "6. **Optimization**: Understood caching and memory management\n",
    "7. **End-to-End Pipeline**: Built a complete financial text analysis workflow\n",
    "\n",
    "### üîß Key Technical Skills:\n",
    "\n",
    "- Using HuggingFace `pipeline()` for quick prototyping\n",
    "- Loading models with memory optimization\n",
    "- Understanding model caching and storage\n",
    "- Error handling and fallback strategies\n",
    "- Processing multiple texts efficiently\n",
    "- Visualizing and saving results\n",
    "\n",
    "### üìà Financial Applications:\n",
    "\n",
    "- **Sentiment Analysis**: News, earnings calls, social media\n",
    "- **Document Classification**: Research reports, regulatory filings\n",
    "- **Information Extraction**: Key metrics from financial documents\n",
    "- **Text Generation**: Report summaries, investment insights\n",
    "- **Question Answering**: Automated analysis of financial documents\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "\n",
    "1. **API Integration**: Connect to cloud-based LLMs (OpenAI, DeepSeek)\n",
    "2. **Advanced Techniques**: Fine-tuning, custom models\n",
    "3. **Production Deployment**: Scaling and optimization\n",
    "4. **Domain Adaptation**: Financial-specific model training\n",
    "5. **Integration**: Combining local models with API-based models\n",
    "\n",
    "### üõ†Ô∏è Troubleshooting Tips:\n",
    "\n",
    "- **Memory Issues**: Use smaller models, enable quantization\n",
    "- **Network Problems**: Work offline with cached models\n",
    "- **Performance**: Use GPU if available, optimize batch sizes\n",
    "- **Accuracy**: Try domain-specific models (FinBERT vs. general BERT)\n",
    "\n",
    "### üìö Additional Resources:\n",
    "\n",
    "- [HuggingFace Course](https://huggingface.co/course/)\n",
    "- [Transformers Documentation](https://huggingface.co/docs/transformers/)\n",
    "- [Financial NLP Papers](https://paperswithcode.com/task/financial-sentiment-analysis)\n",
    "- [Model Hub](https://huggingface.co/models?pipeline_tag=text-classification&sort=downloads)\n",
    "\n",
    "**You're now ready to work with local LLMs and integrate them with API-based solutions!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
