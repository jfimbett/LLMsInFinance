# Mathematical Foundations of LLM Training

## Loss Functions and Cross-Entropy

- **Fundamental training objective**: Learn probability distribution over tokens
  $$P(x_{t+1} | x_1, x_2, ..., x_t; \theta)$$
  
  Where:
  - $x_{t+1}$ = next token to predict
  - $x_1, x_2, ..., x_t$ = context tokens
  - $\theta$ = model parameters

- **Cross-entropy loss**: Measures difference between predicted and true distributions
  $$L_{CE} = -\sum_{i=1}^{|V|} y_i \log(\hat{y}_i)$$
  
  Where:
  - $y_i$ = true probability (one-hot encoded)
  - $\hat{y}_i$ = predicted probability from softmax
  - $|V|$ = vocabulary size

- **Why cross-entropy?**
  
  **Information theory foundation**:
  - Measures "surprise" of predictions
  - Minimizes Kullback-Leibler divergence: $D_{KL}(P_{true} || P_{model})$
  - Connects to maximum likelihood estimation
  
  **Mathematical properties**:
  - Convex function (locally)
  - Differentiable everywhere
  - Penalizes incorrect predictions heavily

## Entropy in Information Theory

:::: {.columns}

::: {.column width="60%"}
- **Shannon entropy**: Measures information content
  $$H(X) = -\sum_{i} P(x_i) \log P(x_i)$$
  
  Where:
  - $X$ = random variable (e.g., next token)
  - $x_i$ = specific outcome/token
  - $P(x_i)$ = probability of outcome $x_i$

- **In language modeling context**:
  $$H = -\frac{1}{N} \sum_{t=1}^{N} \log P(x_t | x_{1:t-1})$$
  
  Where:
  - $N$ = total number of tokens in the sequence
  - $t$ = time step/position in sequence
  - $x_t$ = token at position $t$
  - $x_{1:t-1}$ = all previous tokens (context)
  
- **Perplexity**: Exponential of entropy
  $$\text{Perplexity} = 2^{H}$$
:::

::: {.column width="40%"}
- **Intuitive understanding**:
  
  **Low entropy/perplexity (predictable)**:
  - Model is confident about next token
  - Language follows clear patterns
  - Better model performance
  
  **High entropy/perplexity (uncertain)**:
  - Model uncertain about next token
  - Language is more random/creative
  - Worse model performance
  
- **Training goal**: Minimize cross-entropy = Minimize perplexity
:::

::::

## Why "Cross-Entropy"?

:::: {.columns}

::: {.column width="60%"}
- **Self-entropy**: $H(P) = -\sum_i P(x_i) \log P(x_i)$ (entropy of true distribution)
- **Cross-entropy**: $H(P, Q) = -\sum_i P(x_i) \log Q(x_i)$ (entropy between distributions)
  
  Where:
  - $P$ = true/target distribution (one-hot encoded in training)
  - $Q$ = predicted distribution (from model's softmax output)
  - $P(x_i)$ = true probability of token $x_i$ (1 for correct token, 0 for others)
  - $Q(x_i)$ = predicted probability of token $x_i$ (from model)
:::

::: {.column width="40%"}
- **Key insight**: We measure entropy "across" true distribution $P$ and model distribution $Q$
- **Mathematical relationship**: $H(P, Q) = H(P) + D_{KL}(P || Q)$ where $D_{KL}$ is KL divergence
- **Training objective**: Minimizing cross-entropy = minimizing KL divergence (since $H(P)$ is constant)
:::

::::

## From Logits to Probabilities: The Softmax Function

:::: {.columns}

::: {.column width="60%"}
- **Raw model outputs**: Logits $z_i$ for each vocabulary token
  $$z = W_o h + b_o$$
  
  Where:
  - $z$ = logits vector
  - $h$ = final hidden state
  - $W_o$ = output projection matrix
  - $b_o$ = output bias
  
- **Softmax transformation**: Convert to probabilities
  $$P(x_i) = \frac{e^{z_i}}{\sum_{j=1}^{|V|} e^{z_j}}$$
  
  Where:
  - $P(x_i)$ = probability of token $x_i$
  - $z_i$ = logit for token $x_i$
  - $|V|$ = vocabulary size
  
- **Cross-entropy with softmax**:
  $$L = -\log P(x_{target}) = -z_{target} + \log \sum_{j=1}^{|V|} e^{z_j}$$
:::

::: {.column width="40%"}
- **Why softmax properties matter**:
  
  **Probability constraints**:
  - $\sum_i P(x_i) = 1$ (proper distribution)
  - $P(x_i) > 0$ for all $i$ (non-zero probabilities)
  
  **Gradient properties**:
  - $\frac{\partial L}{\partial z_i} = P(x_i) - y_i$
  - Simple, interpretable gradients
  - Difference between prediction and truth
  
  **Numerical stability**:
  - LogSumExp trick prevents overflow
  - Subtracting max before exponential
:::

::::

## Gradient Descent and Backpropagation

:::: {.columns}

::: {.column width="60%"}
- **Gradient descent update rule**:
  $$\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)$$
  
  Where:
  - $\theta_t$ = parameters at time step $t$
  - $\eta$ = learning rate
  - $\nabla_\theta L$ = gradient of loss with respect to parameters
  
- **Chain rule in transformers**:
  $$\frac{\partial L}{\partial W^{(l)}} = \frac{\partial L}{\partial h^{(L)}} \frac{\partial h^{(L)}}{\partial h^{(l)}} \frac{\partial h^{(l)}}{\partial W^{(l)}}$$
  
  Where:
  - $W^{(l)}$ = weights at layer $l$
  - $h^{(l)}$ = hidden state at layer $l$
  - $L$ = total number of layers
  
  Backpropagates through $L$ layers of self-attention and feedforward blocks
:::

::: {.column width="40%"}
- **Challenges in deep networks**:
  
  **Vanishing gradients**:
  - $\frac{\partial h^{(L)}}{\partial h^{(l)}} \to 0$ as $L - l$ increases
  - Information doesn't flow to early layers
  - Solved by residual connections
  
  **Exploding gradients**:
  - Gradients grow exponentially
  - Training becomes unstable
  - Solved by gradient clipping
  
  **Memory requirements**:
  - Store all intermediate activations
  - $O(L \times N \times d)$ where $N$ = sequence length, $d$ = hidden size
:::

::::

## Optimization Algorithms Beyond SGD

:::: {.columns}

::: {.column width="60%"}
- **Stochastic Gradient Descent (SGD)**:
  $$\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)$$
  
- **SGD with Momentum**:
  $$v_{t+1} = \beta v_t + \eta \nabla_\theta L(\theta_t)$$
  $$\theta_{t+1} = \theta_t - v_{t+1}$$
  
  Where:
  - $v_t$ = momentum term (velocity)
  - $\beta$ = momentum coefficient (typically 0.9)
  
- **Adam Optimizer** (most common for LLMs):
  $$m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla_\theta L$$
  $$v_t = \beta_2 v_{t-1} + (1-\beta_2) (\nabla_\theta L)^2$$
  $$\theta_{t+1} = \theta_t - \frac{\eta \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$$
  
  Where:
  - $m_t$ = first moment estimate (momentum)
  - $v_t$ = second moment estimate (variance)
  - $\hat{m}_t = \frac{m_t}{1-\beta_1^t}$, $\hat{v}_t = \frac{v_t}{1-\beta_2^t}$ = bias correction
:::

::: {.column width="40%"}
- **Why Adam for LLM training?**
  
  **Adaptive learning rates**:
  - Different learning rate per parameter
  - Automatically adjusts based on gradient history
  - Works well for sparse gradients
  
  **Bias correction**:
  - Corrects initialization bias
  - Stable early training
  
  **Robust performance**:
  - Less sensitive to hyperparameter choices
  - Good default for most LLM training
  
  **Typical hyperparameters**:
  - $\beta_1 = 0.9$ (momentum)
  - $\beta_2 = 0.999$ (variance)
  - $\epsilon = 10^{-8}$ (numerical stability)
:::

::::

## Learning Rate Scheduling

:::: {.columns}

::: {.column width="60%"}
- **Constant learning rate**: $\eta_t = \eta_0$
  - Simple but often suboptimal
  - May not converge to best solution
  
- **Linear decay**: $\eta_t = \eta_0 (1 - \frac{t}{T})$
  - Decreases linearly to zero
  - Common for fine-tuning
  
  Where:
  - $\eta_0$ = initial learning rate
  - $t$ = current step
  - $T$ = total training steps
  
- **Cosine annealing**: 
  $$\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t\pi}{T}))$$
  
- **Warmup + decay**: Increase then decrease
  $$\eta_t = \begin{cases}
  \eta_{max} \frac{t}{t_w} & \text{if } t < t_w \\
  \eta_{max} \left(\frac{T - t}{T - t_w}\right)^p & \text{otherwise}
  \end{cases}$$
  
  Where:
  - $t_w$ = warmup steps
  - $p$ = decay power (typically 1)
:::

::: {.column width="40%"}
- **Why learning rate matters**:
  
  **Too high**:
  - Training unstable, loss oscillates
  - May not converge at all
  - Overshoots optimal parameters
  
  **Too low**:
  - Training very slow
  - May get stuck in local minima
  - Inefficient use of compute
  
  **Warmup benefits**:
  - Gradual increase prevents early instability
  - Allows model to "settle" into good region
  - Particularly important for large models
  
  **Decay benefits**:
  - Fine-tunes solution in later stages
  - Helps convergence to better minima
:::

::::

## Gradient Clipping and Numerical Stability

:::: {.columns}

::: {.column width="60%"}
- **Gradient clipping**: Prevent exploding gradients
  
  **Global norm clipping**:
  $$g = \sqrt{\sum_i ||\nabla W_i||^2}$$
  $$c = \min\left(1, \frac{\tau}{g}\right)$$
  $$\nabla W_i \leftarrow c \cdot \nabla W_i$$
  
  Where:
  - $g$ = global gradient norm
  - $\tau$ = maximum allowed norm (threshold)
  - $c$ = clipping factor

- **Alternative: Value clipping**:
  $$\nabla W_i = \text{clip}(\nabla W_i, -\tau, +\tau)$$
  
  Where $\tau$ is the clipping threshold
:::

::: {.column width="40%"}
- **Numerical stability techniques**:
  
  **Mixed precision training**:
  - FP16 for forward pass (faster, less memory)
  - FP32 for gradients (numerical stability)
  - Loss scaling to prevent underflow
  
  **Layer normalization**:
  - Stabilizes activations across layers
  - Reduces internal covariate shift
  - Enables higher learning rates
  
  **Gradient accumulation**:
  - Simulate larger batch sizes
  - Average gradients over multiple mini-batches
  - Reduces memory requirements
:::

::::

## Loss Landscapes and Optimization Challenges

:::: {.columns}

::: {.column width="60%"}
- **Loss landscape visualization**:
  - High-dimensional, non-convex surface
  - Multiple local minima and saddle points
  - Path-dependent optimization
  
- **Challenges in LLM training**:
  
  **Saddle points**: $\nabla L = 0$ but not minimum
  - More common in high dimensions
  - Adam helps escape through momentum
  
  **Plateaus**: Flat regions with small gradients
  - Slow convergence
  - Learning rate scheduling helps
  
  **Sharp vs. flat minima**:
  - Flat minima generalize better
  - SGD noise helps find flat minima
:::

::: {.column width="40%"}
- **Optimization strategies**:
  
  **Batch size effects**:
  - Large batches: More accurate gradients, sharp minima
  - Small batches: Noisy gradients, flat minima
  - Sweet spot: 32-512 for most LLMs
  
  **Initialization importance**:
  - Xavier/He initialization for weights
  - Zero initialization for biases
  - Proper scaling prevents gradient explosion
  
  **Second-order methods**:
  - Newton's method: $\theta_{t+1} = \theta_t - H^{-1} \nabla L$
  - Too expensive for LLMs (Hessian $O(n^2)$)
  - Approximations like L-BFGS occasionally used
  
  Where:
  - $H$ = Hessian matrix (second derivatives)
  - $n$ = number of parameters
:::

::::

## The Connection: Entropy → Gradients → Updates

:::: {.columns}

::: {.column width="60%"}
- **Complete training pipeline**:
  
  1. **Forward pass**: Compute logits $z = f(x; \theta)$
  2. **Softmax**: Convert to probabilities $p = \text{softmax}(z)$
  3. **Cross-entropy**: Compute loss $L = -\log p_{target}$
  4. **Backward pass**: Compute $\nabla_\theta L$ via backpropagation
  5. **Optimization step**: Update $\theta$ using optimizer
  6. **Repeat**: Next batch/epoch
  
  Where:
  - $f(x; \theta)$ = model function with parameters $\theta$
  - $p_{target}$ = probability of the correct token
  
- **Information flow**:
  $$\text{Data} \xrightarrow{\text{model}} \text{Logits} \xrightarrow{\text{softmax}} \text{Probabilities} \xrightarrow{\text{entropy}} \text{Loss}$$
  $$\text{Loss} \xrightarrow{\text{backprop}} \text{Gradients} \xrightarrow{\text{optimizer}} \text{Parameter Updates}$$
:::

::: {.column width="40%"}
- **Key mathematical insights**:
  
  **Entropy drives learning**:
  - Model learns to reduce uncertainty
  - Cross-entropy measures this directly
  - Optimization minimizes prediction entropy
  
  **Gradients provide direction**:
  - Point toward steepest descent
  - Magnitude indicates update size
  - Chain rule connects output to all parameters
  
  **Optimizers determine path**:
  - How to use gradient information
  - Balance exploration vs. exploitation
  - Adapt to landscape properties
  
- **This connects to reasoning models**: Higher entropy allows exploration of reasoning paths
:::

::::

## Mathematical Foundations Summary

:::: {.columns}

::: {.column width="60%"}
- **Core mathematical concepts**:
  - **Entropy**: Measures uncertainty/information content
  - **Cross-entropy**: Training objective connecting predictions to truth
  - **Softmax**: Converts raw outputs to probability distributions
  - **Gradients**: Direction of steepest change in loss landscape
  - **Optimizers**: Algorithms for following gradients efficiently
  
- **Why understanding matters**:
  - Debugging training issues (vanishing/exploding gradients)
  - Choosing appropriate hyperparameters
  - Understanding model behavior and limitations
  - Designing better training procedures
:::

::: {.column width="40%"}
- **Practical implications**:
  
  **For model training**:
  - Learning rate needs careful tuning
  - Gradient clipping prevents instability
  - Adam usually works well for LLMs
  
  **For inference**:
  - Temperature controls entropy/creativity
  - Softmax probabilities enable reasoning
  - Understanding helps with prompt engineering
  
  **For research**:
  - Foundation for advanced techniques
  - Understanding enables innovation
  - Mathematical rigor guides experimentation
:::

::::
