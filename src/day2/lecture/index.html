<!DOCTYPE html>
<html lang="en"><head>
<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/tabby.min.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/light-border.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.5.57">

  <meta name="author" content="Juan F. Imbet">
  <meta name="dcterms.date" content="2025-07-08">
  <title>Day 2: Advanced LLM Concepts &amp; Techniques</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="index_files/libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="../../styles.css">
  <link href="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="index_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    padding-bottom: 0.5rem;
    margin-bottom: 0;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Day 2: Advanced LLM Concepts &amp; Techniques</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Juan F. Imbet 
</div>
</div>
</div>

  <p class="date">2025-07-08</p>
</section>
<section>
<section id="advanced-llm-concepts-techniques" class="title-slide slide level1 center">
<h1>Advanced LLM Concepts &amp; Techniques</h1>

</section>
<section id="overview-of-todays-lecture" class="slide level2">
<h2>Overview of Today’s Lecture</h2>
<ul>
<li>Mathematical foundations of LLM training (entropy, gradients, optimization)</li>
<li>Reasoning models and multi-step inference</li>
<li>How LLMs are trained and optimized</li>
<li>Effective prompt engineering strategies</li>
<li>Understanding and tracing thought processes</li>
<li>Emergent behaviors and interpretability challenges</li>
</ul>
</section></section>
<section>
<section id="mathematical-foundations-of-llm-training" class="title-slide slide level1 center">
<h1>Mathematical Foundations of LLM Training</h1>

</section>
<section id="loss-functions-and-cross-entropy" class="slide level2">
<h2>Loss Functions and Cross-Entropy</h2>
<ul>
<li><p><strong>Fundamental training objective</strong>: Learn probability distribution over tokens <span class="math display">\[P(x_{t+1} | x_1, x_2, ..., x_t; \theta)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(x_{t+1}\)</span> = next token to predict</li>
<li><span class="math inline">\(x_1, x_2, ..., x_t\)</span> = context tokens</li>
<li><span class="math inline">\(\theta\)</span> = model parameters</li>
</ul></li>
<li><p><strong>Cross-entropy loss</strong>: Measures difference between predicted and true distributions <span class="math display">\[L_{CE} = -\sum_{i=1}^{|V|} y_i \log(\hat{y}_i)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(y_i\)</span> = true probability (one-hot encoded)</li>
<li><span class="math inline">\(\hat{y}_i\)</span> = predicted probability from softmax</li>
<li><span class="math inline">\(|V|\)</span> = vocabulary size</li>
</ul></li>
<li><p><strong>Why cross-entropy?</strong></p>
<p><strong>Information theory foundation</strong>:</p>
<ul>
<li>Measures “surprise” of predictions</li>
<li>Minimizes Kullback-Leibler divergence: <span class="math inline">\(D_{KL}(P_{true} || P_{model})\)</span></li>
<li>Connects to maximum likelihood estimation</li>
</ul>
<p><strong>Mathematical properties</strong>:</p>
<ul>
<li>Convex function (locally)</li>
<li>Differentiable everywhere</li>
<li>Penalizes incorrect predictions heavily</li>
</ul></li>
</ul>
</section>
<section id="entropy-in-information-theory" class="slide level2">
<h2>Entropy in Information Theory</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Shannon entropy</strong>: Measures information content <span class="math display">\[H(X) = -\sum_{i} P(x_i) \log P(x_i)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(X\)</span> = random variable (e.g., next token)</li>
<li><span class="math inline">\(x_i\)</span> = specific outcome/token</li>
<li><span class="math inline">\(P(x_i)\)</span> = probability of outcome <span class="math inline">\(x_i\)</span></li>
</ul></li>
<li><p><strong>In language modeling context</strong>: <span class="math display">\[H = -\frac{1}{N} \sum_{t=1}^{N} \log P(x_t | x_{1:t-1})\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(N\)</span> = total number of tokens in the sequence</li>
<li><span class="math inline">\(t\)</span> = time step/position in sequence</li>
<li><span class="math inline">\(x_t\)</span> = token at position <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(x_{1:t-1}\)</span> = all previous tokens (context)</li>
</ul></li>
<li><p><strong>Perplexity</strong>: Exponential of entropy <span class="math display">\[\text{Perplexity} = 2^{H}\]</span></p></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Intuitive understanding</strong>:</p>
<p><strong>Low entropy/perplexity (predictable)</strong>:</p>
<ul>
<li>Model is confident about next token</li>
<li>Language follows clear patterns</li>
<li>Better model performance</li>
</ul>
<p><strong>High entropy/perplexity (uncertain)</strong>:</p>
<ul>
<li>Model uncertain about next token</li>
<li>Language is more random/creative</li>
<li>Worse model performance</li>
</ul></li>
<li><p><strong>Training goal</strong>: Minimize cross-entropy = Minimize perplexity</p></li>
</ul>
</div></div>
</section>
<section id="why-cross-entropy" class="slide level2">
<h2>Why “Cross-Entropy”?</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Self-entropy</strong>: <span class="math inline">\(H(P) = -\sum_i P(x_i) \log P(x_i)\)</span> (entropy of true distribution)</p></li>
<li><p><strong>Cross-entropy</strong>: <span class="math inline">\(H(P, Q) = -\sum_i P(x_i) \log Q(x_i)\)</span> (entropy between distributions)</p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(P\)</span> = true/target distribution (one-hot encoded in training)</li>
<li><span class="math inline">\(Q\)</span> = predicted distribution (from model’s softmax output)</li>
<li><span class="math inline">\(P(x_i)\)</span> = true probability of token <span class="math inline">\(x_i\)</span> (1 for correct token, 0 for others)</li>
<li><span class="math inline">\(Q(x_i)\)</span> = predicted probability of token <span class="math inline">\(x_i\)</span> (from model)</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Key insight</strong>: We measure entropy “across” true distribution <span class="math inline">\(P\)</span> and model distribution <span class="math inline">\(Q\)</span></li>
<li><strong>Mathematical relationship</strong>: <span class="math inline">\(H(P, Q) = H(P) + D_{KL}(P || Q)\)</span> where <span class="math inline">\(D_{KL}\)</span> is KL divergence</li>
<li><strong>Training objective</strong>: Minimizing cross-entropy = minimizing KL divergence (since <span class="math inline">\(H(P)\)</span> is constant)</li>
</ul>
</div></div>
</section>
<section id="from-logits-to-probabilities-the-softmax-function" class="slide level2">
<h2>From Logits to Probabilities: The Softmax Function</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Raw model outputs</strong>: Logits <span class="math inline">\(z_i\)</span> for each vocabulary token <span class="math display">\[z = W_o h + b_o\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(z\)</span> = logits vector</li>
<li><span class="math inline">\(h\)</span> = final hidden state</li>
<li><span class="math inline">\(W_o\)</span> = output projection matrix</li>
<li><span class="math inline">\(b_o\)</span> = output bias</li>
</ul></li>
<li><p><strong>Softmax transformation</strong>: Convert to probabilities <span class="math display">\[P(x_i) = \frac{e^{z_i}}{\sum_{j=1}^{|V|} e^{z_j}}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(P(x_i)\)</span> = probability of token <span class="math inline">\(x_i\)</span></li>
<li><span class="math inline">\(z_i\)</span> = logit for token <span class="math inline">\(x_i\)</span></li>
<li><span class="math inline">\(|V|\)</span> = vocabulary size</li>
</ul></li>
<li><p><strong>Cross-entropy with softmax</strong>: <span class="math display">\[L = -\log P(x_{target}) = -z_{target} + \log \sum_{j=1}^{|V|} e^{z_j}\]</span></p></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Why softmax properties matter</strong>:</p>
<p><strong>Probability constraints</strong>:</p>
<ul>
<li><span class="math inline">\(\sum_i P(x_i) = 1\)</span> (proper distribution)</li>
<li><span class="math inline">\(P(x_i) &gt; 0\)</span> for all <span class="math inline">\(i\)</span> (non-zero probabilities)</li>
</ul>
<p><strong>Gradient properties</strong>:</p>
<ul>
<li><span class="math inline">\(\frac{\partial L}{\partial z_i} = P(x_i) - y_i\)</span></li>
<li>Simple, interpretable gradients</li>
<li>Difference between prediction and truth</li>
</ul>
<p><strong>Numerical stability</strong>:</p>
<ul>
<li>LogSumExp trick prevents overflow</li>
<li>Subtracting max before exponential</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="gradient-descent-and-backpropagation" class="slide level2">
<h2>Gradient Descent and Backpropagation</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Gradient descent update rule</strong>: <span class="math display">\[\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\theta_t\)</span> = parameters at time step <span class="math inline">\(t\)</span></li>
<li><span class="math inline">\(\eta\)</span> = learning rate</li>
<li><span class="math inline">\(\nabla_\theta L\)</span> = gradient of loss with respect to parameters</li>
</ul></li>
<li><p><strong>Chain rule in transformers</strong>: <span class="math display">\[\frac{\partial L}{\partial W^{(l)}} = \frac{\partial L}{\partial h^{(L)}} \frac{\partial h^{(L)}}{\partial h^{(l)}} \frac{\partial h^{(l)}}{\partial W^{(l)}}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(W^{(l)}\)</span> = weights at layer <span class="math inline">\(l\)</span></li>
<li><span class="math inline">\(h^{(l)}\)</span> = hidden state at layer <span class="math inline">\(l\)</span></li>
<li><span class="math inline">\(L\)</span> = total number of layers</li>
</ul>
<p>Backpropagates through <span class="math inline">\(L\)</span> layers of self-attention and feedforward blocks</p></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Challenges in deep networks</strong>:</p>
<p><strong>Vanishing gradients</strong>:</p>
<ul>
<li><span class="math inline">\(\frac{\partial h^{(L)}}{\partial h^{(l)}} \to 0\)</span> as <span class="math inline">\(L - l\)</span> increases</li>
<li>Information doesn’t flow to early layers</li>
<li>Solved by residual connections</li>
</ul>
<p><strong>Exploding gradients</strong>:</p>
<ul>
<li>Gradients grow exponentially</li>
<li>Training becomes unstable</li>
<li>Solved by gradient clipping</li>
</ul>
<p><strong>Memory requirements</strong>:</p>
<ul>
<li>Store all intermediate activations</li>
<li><span class="math inline">\(O(L \times N \times d)\)</span> where <span class="math inline">\(N\)</span> = sequence length, <span class="math inline">\(d\)</span> = hidden size</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="optimization-algorithms-beyond-sgd" class="slide level2">
<h2>Optimization Algorithms Beyond SGD</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Stochastic Gradient Descent (SGD)</strong>: <span class="math display">\[\theta_{t+1} = \theta_t - \eta \nabla_\theta L(\theta_t)\]</span></p></li>
<li><p><strong>SGD with Momentum</strong>: <span class="math display">\[v_{t+1} = \beta v_t + \eta \nabla_\theta L(\theta_t)\]</span> <span class="math display">\[\theta_{t+1} = \theta_t - v_{t+1}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(v_t\)</span> = momentum term (velocity)</li>
<li><span class="math inline">\(\beta\)</span> = momentum coefficient (typically 0.9)</li>
</ul></li>
<li><p><strong>Adam Optimizer</strong> (most common for LLMs): <span class="math display">\[m_t = \beta_1 m_{t-1} + (1-\beta_1) \nabla_\theta L\]</span> <span class="math display">\[v_t = \beta_2 v_{t-1} + (1-\beta_2) (\nabla_\theta L)^2\]</span> <span class="math display">\[\theta_{t+1} = \theta_t - \frac{\eta \hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(m_t\)</span> = first moment estimate (momentum)</li>
<li><span class="math inline">\(v_t\)</span> = second moment estimate (variance)</li>
<li><span class="math inline">\(\hat{m}_t = \frac{m_t}{1-\beta_1^t}\)</span>, <span class="math inline">\(\hat{v}_t = \frac{v_t}{1-\beta_2^t}\)</span> = bias correction</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Why Adam for LLM training?</strong></p>
<p><strong>Adaptive learning rates</strong>:</p>
<ul>
<li>Different learning rate per parameter</li>
<li>Automatically adjusts based on gradient history</li>
<li>Works well for sparse gradients</li>
</ul>
<p><strong>Bias correction</strong>:</p>
<ul>
<li>Corrects initialization bias</li>
<li>Stable early training</li>
</ul>
<p><strong>Robust performance</strong>:</p>
<ul>
<li>Less sensitive to hyperparameter choices</li>
<li>Good default for most LLM training</li>
</ul>
<p><strong>Typical hyperparameters</strong>:</p>
<ul>
<li><span class="math inline">\(\beta_1 = 0.9\)</span> (momentum)</li>
<li><span class="math inline">\(\beta_2 = 0.999\)</span> (variance)</li>
<li><span class="math inline">\(\epsilon = 10^{-8}\)</span> (numerical stability)</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="learning-rate-scheduling" class="slide level2">
<h2>Learning Rate Scheduling</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Constant learning rate</strong>: <span class="math inline">\(\eta_t = \eta_0\)</span></p>
<ul>
<li>Simple but often suboptimal</li>
<li>May not converge to best solution</li>
</ul></li>
<li><p><strong>Linear decay</strong>: <span class="math inline">\(\eta_t = \eta_0 (1 - \frac{t}{T})\)</span></p>
<ul>
<li>Decreases linearly to zero</li>
<li>Common for fine-tuning</li>
</ul>
<p>Where:</p>
<ul>
<li><span class="math inline">\(\eta_0\)</span> = initial learning rate</li>
<li><span class="math inline">\(t\)</span> = current step</li>
<li><span class="math inline">\(T\)</span> = total training steps</li>
</ul></li>
<li><p><strong>Cosine annealing</strong>: <span class="math display">\[\eta_t = \eta_{min} + \frac{1}{2}(\eta_{max} - \eta_{min})(1 + \cos(\frac{t\pi}{T}))\]</span></p></li>
<li><p><strong>Warmup + decay</strong>: Increase then decrease <span class="math display">\[\eta_t = \begin{cases}
\eta_{max} \frac{t}{t_w} &amp; \text{if } t &lt; t_w \\
\eta_{max} \left(\frac{T - t}{T - t_w}\right)^p &amp; \text{otherwise}
\end{cases}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(t_w\)</span> = warmup steps</li>
<li><span class="math inline">\(p\)</span> = decay power (typically 1)</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Why learning rate matters</strong>:</p>
<p><strong>Too high</strong>:</p>
<ul>
<li>Training unstable, loss oscillates</li>
<li>May not converge at all</li>
<li>Overshoots optimal parameters</li>
</ul>
<p><strong>Too low</strong>:</p>
<ul>
<li>Training very slow</li>
<li>May get stuck in local minima</li>
<li>Inefficient use of compute</li>
</ul>
<p><strong>Warmup benefits</strong>:</p>
<ul>
<li>Gradual increase prevents early instability</li>
<li>Allows model to “settle” into good region</li>
<li>Particularly important for large models</li>
</ul>
<p><strong>Decay benefits</strong>:</p>
<ul>
<li>Fine-tunes solution in later stages</li>
<li>Helps convergence to better minima</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="gradient-clipping-and-numerical-stability" class="slide level2">
<h2>Gradient Clipping and Numerical Stability</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Gradient clipping</strong>: Prevent exploding gradients</p>
<p><strong>Global norm clipping</strong>: <span class="math display">\[g = \sqrt{\sum_i ||\nabla W_i||^2}\]</span> <span class="math display">\[c = \min\left(1, \frac{\tau}{g}\right)\]</span> <span class="math display">\[\nabla W_i \leftarrow c \cdot \nabla W_i\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(g\)</span> = global gradient norm</li>
<li><span class="math inline">\(\tau\)</span> = maximum allowed norm (threshold)</li>
<li><span class="math inline">\(c\)</span> = clipping factor</li>
</ul></li>
<li><p><strong>Alternative: Value clipping</strong>: <span class="math display">\[\nabla W_i = \text{clip}(\nabla W_i, -\tau, +\tau)\]</span></p>
<p>Where <span class="math inline">\(\tau\)</span> is the clipping threshold</p></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Numerical stability techniques</strong>:</p>
<p><strong>Mixed precision training</strong>:</p>
<ul>
<li>FP16 for forward pass (faster, less memory)</li>
<li>FP32 for gradients (numerical stability)</li>
<li>Loss scaling to prevent underflow</li>
</ul>
<p><strong>Layer normalization</strong>:</p>
<ul>
<li>Stabilizes activations across layers</li>
<li>Reduces internal covariate shift</li>
<li>Enables higher learning rates</li>
</ul>
<p><strong>Gradient accumulation</strong>:</p>
<ul>
<li>Simulate larger batch sizes</li>
<li>Average gradients over multiple mini-batches</li>
<li>Reduces memory requirements</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="loss-landscapes-and-optimization-challenges" class="slide level2">
<h2>Loss Landscapes and Optimization Challenges</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Loss landscape visualization</strong>:</p>
<ul>
<li>High-dimensional, non-convex surface</li>
<li>Multiple local minima and saddle points</li>
<li>Path-dependent optimization</li>
</ul></li>
<li><p><strong>Challenges in LLM training</strong>:</p>
<p><strong>Saddle points</strong>: <span class="math inline">\(\nabla L = 0\)</span> but not minimum</p>
<ul>
<li>More common in high dimensions</li>
<li>Adam helps escape through momentum</li>
</ul>
<p><strong>Plateaus</strong>: Flat regions with small gradients</p>
<ul>
<li>Slow convergence</li>
<li>Learning rate scheduling helps</li>
</ul>
<p><strong>Sharp vs.&nbsp;flat minima</strong>:</p>
<ul>
<li>Flat minima generalize better</li>
<li>SGD noise helps find flat minima</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Optimization strategies</strong>:</p>
<p><strong>Batch size effects</strong>:</p>
<ul>
<li>Large batches: More accurate gradients, sharp minima</li>
<li>Small batches: Noisy gradients, flat minima</li>
<li>Sweet spot: 32-512 for most LLMs</li>
</ul>
<p><strong>Initialization importance</strong>:</p>
<ul>
<li>Xavier/He initialization for weights</li>
<li>Zero initialization for biases</li>
<li>Proper scaling prevents gradient explosion</li>
</ul>
<p><strong>Second-order methods</strong>:</p>
<ul>
<li>Newton’s method: <span class="math inline">\(\theta_{t+1} = \theta_t - H^{-1} \nabla L\)</span></li>
<li>Too expensive for LLMs (Hessian <span class="math inline">\(O(n^2)\)</span>)</li>
<li>Approximations like L-BFGS occasionally used</li>
</ul>
<p>Where:</p>
<ul>
<li><span class="math inline">\(H\)</span> = Hessian matrix (second derivatives)</li>
<li><span class="math inline">\(n\)</span> = number of parameters</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="the-connection-entropy-gradients-updates" class="slide level2">
<h2>The Connection: Entropy → Gradients → Updates</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Complete training pipeline</strong>:</p>
<ol type="1">
<li><strong>Forward pass</strong>: Compute logits <span class="math inline">\(z = f(x; \theta)\)</span></li>
<li><strong>Softmax</strong>: Convert to probabilities <span class="math inline">\(p = \text{softmax}(z)\)</span></li>
<li><strong>Cross-entropy</strong>: Compute loss <span class="math inline">\(L = -\log p_{target}\)</span></li>
<li><strong>Backward pass</strong>: Compute <span class="math inline">\(\nabla_\theta L\)</span> via backpropagation</li>
<li><strong>Optimization step</strong>: Update <span class="math inline">\(\theta\)</span> using optimizer</li>
<li><strong>Repeat</strong>: Next batch/epoch</li>
</ol>
<p>Where:</p>
<ul>
<li><span class="math inline">\(f(x; \theta)\)</span> = model function with parameters <span class="math inline">\(\theta\)</span></li>
<li><span class="math inline">\(p_{target}\)</span> = probability of the correct token</li>
</ul></li>
<li><p><strong>Information flow</strong>: <span class="math display">\[\text{Data} \xrightarrow{\text{model}} \text{Logits} \xrightarrow{\text{softmax}} \text{Probabilities} \xrightarrow{\text{entropy}} \text{Loss}\]</span> <span class="math display">\[\text{Loss} \xrightarrow{\text{backprop}} \text{Gradients} \xrightarrow{\text{optimizer}} \text{Parameter Updates}\]</span></p></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Key mathematical insights</strong>:</p>
<p><strong>Entropy drives learning</strong>:</p>
<ul>
<li>Model learns to reduce uncertainty</li>
<li>Cross-entropy measures this directly</li>
<li>Optimization minimizes prediction entropy</li>
</ul>
<p><strong>Gradients provide direction</strong>:</p>
<ul>
<li>Point toward steepest descent</li>
<li>Magnitude indicates update size</li>
<li>Chain rule connects output to all parameters</li>
</ul>
<p><strong>Optimizers determine path</strong>:</p>
<ul>
<li>How to use gradient information</li>
<li>Balance exploration vs.&nbsp;exploitation</li>
<li>Adapt to landscape properties</li>
</ul></li>
<li><p><strong>This connects to reasoning models</strong>: Higher entropy allows exploration of reasoning paths</p></li>
</ul>
</div></div>
</section>
<section id="mathematical-foundations-summary" class="slide level2">
<h2>Mathematical Foundations Summary</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Core mathematical concepts</strong>:
<ul>
<li><strong>Entropy</strong>: Measures uncertainty/information content</li>
<li><strong>Cross-entropy</strong>: Training objective connecting predictions to truth</li>
<li><strong>Softmax</strong>: Converts raw outputs to probability distributions</li>
<li><strong>Gradients</strong>: Direction of steepest change in loss landscape</li>
<li><strong>Optimizers</strong>: Algorithms for following gradients efficiently</li>
</ul></li>
<li><strong>Why understanding matters</strong>:
<ul>
<li>Debugging training issues (vanishing/exploding gradients)</li>
<li>Choosing appropriate hyperparameters</li>
<li>Understanding model behavior and limitations</li>
<li>Designing better training procedures</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Practical implications</strong>:</p>
<p><strong>For model training</strong>:</p>
<ul>
<li>Learning rate needs careful tuning</li>
<li>Gradient clipping prevents instability</li>
<li>Adam usually works well for LLMs</li>
</ul>
<p><strong>For inference</strong>:</p>
<ul>
<li>Temperature controls entropy/creativity</li>
<li>Softmax probabilities enable reasoning</li>
<li>Understanding helps with prompt engineering</li>
</ul>
<p><strong>For research</strong>:</p>
<ul>
<li>Foundation for advanced techniques</li>
<li>Understanding enables innovation</li>
<li>Mathematical rigor guides experimentation</li>
</ul></li>
</ul>
</div></div>
</section></section>
<section>
<section id="reasoning-models-beyond-next-token-prediction" class="title-slide slide level1 center">
<h1>Reasoning Models: Beyond Next Token Prediction</h1>

</section>
<section id="from-token-generation-to-reasoning-chains" class="slide level2">
<h2>From Token Generation to Reasoning Chains</h2>
<ul>
<li><p><strong>Traditional LLM prediction</strong>:</p>
<ul>
<li>Single forward pass: <span class="math inline">\(P(x_{t+1} | x_1, x_2, ..., x_t)\)</span></li>
<li>Direct mapping from context to next token</li>
<li>Temperature controls randomness in sampling</li>
<li>No explicit intermediate reasoning steps</li>
</ul></li>
<li><p><strong>Reasoning models approach</strong>:</p>
<ul>
<li>Multi-step generation: <span class="math inline">\(P(goal | reasoning\_chain)\)</span></li>
<li>Explicit intermediate thought tokens</li>
<li>Chain-of-thought: <span class="math inline">\(P(answer | context, thoughts)\)</span></li>
<li>Goal-oriented sequence generation</li>
</ul></li>
<li><p><strong>Mathematical distinction</strong>:</p>
<p><strong>Standard generation</strong>: <span class="math display">\[P(x_{t+1}) = \text{softmax}(W_o h_t)\]</span></p>
<p><strong>Reasoning generation</strong>: <span class="math display">\[P(answer) = \prod_{i=1}^{n} P(t_i | c, t_{1:i-1}) \times P(a | c, t_{1:n})\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(t_i\)</span> = thought/reasoning step <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(c\)</span> = context</li>
<li><span class="math inline">\(a\)</span> = final answer</li>
<li><span class="math inline">\(n\)</span> = number of reasoning steps</li>
</ul></li>
<li><p><strong>Key difference</strong>: Intermediate reasoning states influence final outcome</p></li>
</ul>
</section>
<section id="mathematical-framework-of-reasoning-models" class="slide level2">
<h2>Mathematical Framework of Reasoning Models</h2>
<ul>
<li><p><strong>Standard next-token prediction</strong>: <span class="math display">\[P(x_{t+1} | x_{1:t}) = \frac{e^{z_{t+1}}}{\sum_{v \in V} e^{z_v}}\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(z_{t+1} = W_o h_t + b_o\)</span> = logits for next token</li>
<li><span class="math inline">\(V\)</span> = vocabulary</li>
<li><span class="math inline">\(h_t\)</span> = hidden state at position <span class="math inline">\(t\)</span></li>
</ul></li>
<li><p><strong>Reasoning chain generation</strong>: <span class="math display">\[P(R) = \prod_{i=1}^{n} P(r_i | c, r_{1:i-1})\]</span></p>
<p><span class="math display">\[P(A | R) = P(a | c, r_{1:n})\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(R\)</span> = reasoning chain</li>
<li><span class="math inline">\(r_i\)</span> = reasoning step <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(A\)</span> = final answer</li>
</ul></li>
</ul>
<div class="column" style="width:40%;">
<ul>
<li><p><strong>Temperature in reasoning context</strong>:</p>
<p><strong>Standard softmax with temperature</strong>: <span class="math display">\[P(x_i) = \frac{e^{z_i/T}}{\sum_{j} e^{z_j/T}}\]</span></p>
<p><strong>Reasoning requires <span class="math inline">\(T &gt; 0\)</span> because</strong>:</p>
<ul>
<li>Need exploration of reasoning paths</li>
<li>Multiple valid intermediate steps</li>
<li>Diversity in thought processes</li>
<li>Avoiding reasoning dead-ends</li>
</ul></li>
</ul>
</div><p>::::</p>
</section>
<section id="why-temperature-cannot-be-zero-in-reasoning" class="slide level2">
<h2>Why Temperature Cannot Be Zero in Reasoning</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Mathematical constraint</strong>: <span class="math inline">\(T \to 0\)</span> leads to deterministic selection <span class="math display">\[\lim_{T \to 0} P(x_i) = \begin{cases}
1 &amp; \text{if } i = \arg\max_j z_j \\
0 &amp; \text{otherwise}
\end{cases}\]</span></p></li>
<li><p><strong>Problem with deterministic reasoning</strong>:</p>
<ul>
<li>Only one reasoning path explored</li>
<li>No recovery from sub-optimal intermediate steps</li>
<li>Reasoning becomes brittle and inflexible</li>
<li>Cannot explore alternative valid approaches</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Reasoning examples</strong>:</p>
<p><strong>Problem</strong>: Solve a complex multi-step problem</p>
<p><strong>With <span class="math inline">\(T = 0\)</span> (deterministic)</strong>:</p>
<ul>
<li>Always chooses same first reasoning step</li>
<li>Cannot explore alternative approaches</li>
<li>Gets stuck if initial assumption is suboptimal</li>
</ul>
<p><strong>With <span class="math inline">\(T &gt; 0\)</span> (stochastic)</strong>:</p>
<ul>
<li>Can explore multiple solution methods</li>
<li>Considers different assumptions</li>
<li>Allows for uncertainty quantification</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="reasoning-chain-decomposition" class="slide level2">
<h2>Reasoning Chain Decomposition</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Multi-step probability factorization</strong>: <span class="math display">\[P(A | Q) = \sum_{R} P(A | R) \times P(R | Q)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(A\)</span> = answer</li>
<li><span class="math inline">\(Q\)</span> = question<br>
</li>
<li><span class="math inline">\(R\)</span> = reasoning path</li>
</ul></li>
<li><p><strong>Each reasoning step</strong>: <span class="math display">\[P(r_i | c, r_{1:i-1}) = \text{softmax}\left(\frac{f_\theta(c, r_{1:i-1})}{T}\right)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(r_i\)</span> = reasoning step <span class="math inline">\(i\)</span></li>
<li><span class="math inline">\(c\)</span> = context</li>
<li><span class="math inline">\(T\)</span> = temperature</li>
</ul></li>
<li><p><strong>Final answer conditioning</strong>: <span class="math display">\[P(A | R) = \text{softmax}\left(\frac{g_\theta(R)}{T_f}\right)\]</span></p>
<p>Where <span class="math inline">\(T_f\)</span> = final temperature</p></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Temperature effects across reasoning</strong>:</p>
<p><strong>High temperature (<span class="math inline">\(T &gt; 1\)</span>)</strong>:</p>
<ul>
<li>More diverse reasoning paths</li>
<li>Higher creativity in problem-solving</li>
<li>Risk of incoherent reasoning</li>
</ul>
<p><strong>Moderate temperature (<span class="math inline">\(0.3 &lt; T &lt; 1\)</span>)</strong>:</p>
<ul>
<li>Balanced exploration vs.&nbsp;exploitation</li>
<li>Coherent but diverse reasoning</li>
<li>Optimal for most complex problems</li>
</ul>
<p><strong>Low temperature (<span class="math inline">\(T \to 0\)</span>)</strong>:</p>
<ul>
<li>Deterministic, rigid reasoning</li>
<li>Cannot recover from mistakes</li>
<li>Limited problem-solving capability</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="information-theory-of-reasoning" class="slide level2">
<h2>Information Theory of Reasoning</h2>
<ul>
<li><p><strong>Entropy in reasoning steps</strong>: <span class="math display">\[H(R_i) = -\sum_{r_i} P(r_i \mid \text{context},\, r_{1:i-1}) \log P(r_i \mid \text{context},\, r_{1:i-1})\]</span></p></li>
<li><p><strong>Total reasoning entropy</strong>: <span class="math display">\[H(\text{Reasoning}) = \sum_{i=1}^{n} H(R_i \mid R_{1:i-1})\]</span></p></li>
<li><p><strong>Conditional mutual information</strong>: <span class="math display">\[I(A;\, R \mid Q) = H(A \mid Q) - H(A \mid Q,\, R)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(A\)</span> = Answer</li>
<li><span class="math inline">\(R\)</span> = Reasoning Path</li>
<li><span class="math inline">\(Q\)</span> = Question</li>
<li><span class="math inline">\(I(A;\, R \mid Q)\)</span> = Mutual information between Answer and Reasoning Path given Question</li>
<li><span class="math inline">\(H(A \mid Q)\)</span> = Entropy of Answer given Question</li>
<li><span class="math inline">\(H(A \mid Q,\, R)\)</span> = Entropy of Answer given Question and Reasoning Path</li>
</ul></li>
<li><p><strong>Why entropy matters in reasoning</strong>:</p>
<ul>
<li>Zero entropy (<span class="math inline">\(T = 0\)</span>): <span class="math inline">\(H(R_i) = 0\)</span> for all reasoning steps — deterministic, no exploration, cannot adapt to new information</li>
<li>Positive entropy (<span class="math inline">\(T &gt; 0\)</span>): <span class="math inline">\(H(R_i) &gt; 0\)</span> allows exploration, can discover optimal reasoning paths, adapts to intermediate findings</li>
<li>General implications: complex analysis requires uncertainty handling; multiple valid analytical approaches; need to explore scenario variations ::::</li>
</ul></li>
</ul>
</section>
<section id="reasoning-vs.-direct-prediction-mathematical-comparison" class="slide level2">
<h2>Reasoning vs.&nbsp;Direct Prediction: Mathematical Comparison</h2>
<ul>
<li><p><strong>Direct prediction complexity</strong>: <span class="math display">\[P(answer \mid question) = \text{softmax}(W_o h_{final})\]</span></p>
<p>Single forward pass, <span class="math inline">\(O(|V|)\)</span> complexity for final softmax</p></li>
<li><p><strong>Reasoning model complexity</strong>: <span class="math display">\[P(answer \mid question) = \sum_{paths} \prod_{i=1}^{n} P(r_i \mid context, r_{1:i-1}) \times P(answer \mid reasoning)\]</span></p>
<p>Multiple forward passes, <span class="math inline">\(O(n \times |V|)\)</span> complexity</p></li>
<li><p><strong>Computational trade-offs</strong>:</p>
<p><strong>Direct prediction</strong>:</p>
<ul>
<li>Fast inference: <span class="math inline">\(O(1)\)</span> forward pass</li>
<li>Limited reasoning capability</li>
<li>Suitable for simple tasks</li>
</ul>
<p><strong>Reasoning models</strong>:</p>
<ul>
<li>Slower inference: <span class="math inline">\(O(n)\)</span> forward passes</li>
<li>Enhanced problem-solving</li>
<li>Required for complex analytical tasks</li>
</ul></li>
<li><p><strong>Quality vs.&nbsp;efficiency trade-off</strong>:</p>
<ul>
<li>Reasoning models: higher accuracy, slower</li>
<li>Direct models: faster, limited capability</li>
</ul></li>
</ul>
</section>
<section id="temperature-scheduling-in-reasoning" class="slide level2">
<h2>Temperature Scheduling in Reasoning</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Adaptive temperature across reasoning steps</strong>: <span class="math display">\[T_i = T_0 \times \text{schedule}(i, context, partial\_reasoning)\]</span></p></li>
<li><p><strong>Common scheduling strategies</strong>:</p>
<p><strong>Linear decay</strong>: <span class="math inline">\(T_i = T_0 \times (1 - \frac{i}{n})\)</span></p>
<p><strong>Exponential decay</strong>: <span class="math inline">\(T_i = T_0 \times e^{-\lambda i}\)</span></p>
<p><strong>Content-aware</strong>: <span class="math inline">\(T_i = f(confidence, complexity, stakes)\)</span></p></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Temperature strategies for different task types</strong>:</p>
<p><strong>High-precision tasks</strong> (mathematical computation):</p>
<ul>
<li>Start with <span class="math inline">\(T_0 = 0.3\)</span> (conservative)</li>
<li>Decay to <span class="math inline">\(T_{final} = 0.1\)</span> (precise)</li>
</ul>
<p><strong>Exploratory analysis</strong> (research, brainstorming):</p>
<ul>
<li>Start with <span class="math inline">\(T_0 = 0.8\)</span> (creative)</li>
<li>Maintain <span class="math inline">\(T &gt; 0.5\)</span> throughout</li>
</ul>
<p><strong>Mixed reasoning tasks</strong>:</p>
<ul>
<li>Variable temperature based on step type</li>
<li>Lower <span class="math inline">\(T\)</span> for computational steps</li>
<li>Higher <span class="math inline">\(T\)</span> for interpretation steps</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="reasoning-path-optimization" class="slide level2">
<h2>Reasoning Path Optimization</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Beam search in reasoning space</strong>: Keep top-<span class="math inline">\(k\)</span> reasoning paths at each step</p>
<p><span class="math display">\[\text{Score}(path) = \frac{1}{|path|} \sum_{i=1}^{|path|} \log P(r_i | context, r_{1:i-1})\]</span></p></li>
<li><p><strong>Path pruning criteria</strong>:</p>
<ul>
<li>Coherence score below threshold</li>
<li>Logical inconsistency detection</li>
<li>Factual accuracy verification</li>
<li>Domain-specific validity checks</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Multi-path reasoning benefits</strong>:</p>
<p><strong>Robustness</strong>:</p>
<ul>
<li>Multiple valid approaches to same problem</li>
<li>Cross-validation of reasoning steps</li>
<li>Error detection through inconsistency</li>
</ul>
<p><strong>Uncertainty quantification</strong>:</p>
<ul>
<li>Confidence from path agreement</li>
<li>Range estimation from path diversity</li>
<li>Risk assessment from reasoning variance</li>
</ul></li>
<li><p><strong>Example</strong>: Multi-step problem solving</p>
<ul>
<li>Path 1: Conservative assumptions</li>
<li>Path 2: Optimistic conditions</li>
<li>Path 3: Stress-test scenarios</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="self-consistency-and-verification" class="slide level2">
<h2>Self-Consistency and Verification</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Self-consistency scoring</strong>: Generate <span class="math inline">\(m\)</span> reasoning paths, measure agreement</p>
<p><span class="math display">\[\text{Consistency} = \frac{1}{m(m-1)} \sum_{i \neq j} \text{similarity}(answer_i, answer_j)\]</span></p></li>
<li><p><strong>Reasoning verification</strong>: <span class="math display">\[P(correct | reasoning) = \sigma(W_v \cdot h_r)\]</span></p>
<p>Where:</p>
<ul>
<li><span class="math inline">\(W_v\)</span> = verification network weights</li>
<li><span class="math inline">\(h_r\)</span> = reasoning embedding</li>
<li><span class="math inline">\(\sigma\)</span> = sigmoid function</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Why temperature enables verification</strong>:</p>
<p><strong>With <span class="math inline">\(T &gt; 0\)</span></strong>:</p>
<ul>
<li>Generate diverse reasoning samples</li>
<li>Check consistency across approaches</li>
<li>Identify robust vs.&nbsp;fragile conclusions</li>
</ul>
<p><strong>With <span class="math inline">\(T = 0\)</span></strong>:</p>
<ul>
<li>Only one reasoning path</li>
<li>No verification possible</li>
<li>Cannot assess answer reliability</li>
</ul></li>
<li><p><strong>Verification example</strong>:</p>
<ul>
<li>Multiple solution approaches</li>
<li>Cross-check with alternative methods</li>
<li>Sensitivity analysis across scenarios</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="reasoning-model-architectures" class="slide level2">
<h2>Reasoning Model Architectures</h2>
<ul>
<li><strong>Chain-of-Thought (CoT)</strong>: <span class="math display">\[P(answer \mid question) = P(answer \mid question, thoughts) \times P(thoughts \mid question)\]</span>
<ul>
<li>Linear reasoning progression, simple to implement and understand, good for sequential analytical tasks</li>
</ul></li>
<li><strong>Tree-of-Thoughts (ToT)</strong>: <span class="math display">\[P(answer \mid question) = \max_{tree} \left( P(answer \mid question, tree) \times P(tree \mid question) \right)\]</span>
<ul>
<li>Branching reasoning exploration, better for scenario analysis, higher computational cost</li>
</ul></li>
<li><strong>Graph-of-Thoughts (GoT)</strong>: <span class="math display">\[P(answer \mid question) = \sum_{graph} P(answer \mid question, graph) \times P(graph \mid question)\]</span>
<ul>
<li>Complex relationship modeling, best for interconnected systems, highest computational complexity</li>
</ul></li>
</ul>
</section>
<section id="generic-applications-of-reasoning-models" class="slide level2">
<h2>Generic Applications of Reasoning Models</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Mathematical problem solving</strong>:
<ol type="1">
<li>Problem decomposition and analysis</li>
<li>Method selection and justification</li>
<li>Step-by-step solution development</li>
<li>Result verification and validation</li>
<li>Alternative approach exploration</li>
</ol></li>
<li><strong>Complex decision making</strong>:
<ol type="1">
<li>Information gathering and analysis</li>
<li>Criteria identification and weighting</li>
<li>Option evaluation and comparison</li>
<li>Trade-off analysis and reasoning</li>
<li>Final decision justification</li>
</ol></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Mathematical requirements</strong>:</p>
<p><strong>Each reasoning step needs <span class="math inline">\(T &gt; 0\)</span></strong>:</p>
<ul>
<li>Explore alternative analytical approaches</li>
<li>Consider multiple scenario outcomes</li>
<li>Adapt to intermediate findings</li>
<li>Maintain reasoning flexibility</li>
</ul>
<p><strong>Example temperature settings</strong>:</p>
<ul>
<li>Mathematical computation: <span class="math inline">\(T = 0.4\)</span></li>
<li>Creative problem solving: <span class="math inline">\(T = 0.7\)</span></li>
<li>Logical analysis: <span class="math inline">\(T = 0.3\)</span></li>
<li>Hypothesis generation: <span class="math inline">\(T = 0.6\)</span></li>
</ul></li>
</ul>
</div></div>
</section>
<section id="limitations-and-future-directions" class="slide level2">
<h2>Limitations and Future Directions</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Current limitations</strong>:
<ul>
<li>Reasoning length constraints</li>
<li>Computational cost scaling</li>
<li>Quality assessment challenges</li>
<li>Integration with external knowledge</li>
</ul></li>
<li><strong>Mathematical challenges</strong>:
<ul>
<li>Optimizing temperature schedules</li>
<li>Balancing exploration vs.&nbsp;exploitation</li>
<li>Measuring reasoning quality objectively</li>
<li>Handling reasoning contradictions</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Future research directions</strong>:</p>
<p><strong>Adaptive reasoning</strong>:</p>
<ul>
<li>Dynamic reasoning length</li>
<li>Context-aware temperature setting</li>
<li>Real-time quality assessment</li>
</ul>
<p><strong>Hybrid approaches</strong>:</p>
<ul>
<li>Reasoning + retrieval integration</li>
<li>Multi-modal reasoning incorporation</li>
<li>External tool usage in reasoning</li>
</ul>
<p><strong>Domain-specific developments</strong>:</p>
<ul>
<li>Domain-constrained reasoning</li>
<li>Quality assurance verification</li>
<li>Context-aware reasoning temperature</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="practical-implementation-considerations" class="slide level2">
<h2>Practical Implementation Considerations</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><p><strong>Temperature selection guidelines</strong>:</p>
<p><strong>High-precision tasks</strong>: <span class="math inline">\(T \in [0.1, 0.4]\)</span></p>
<ul>
<li>Mathematical computation</li>
<li>Logical verification</li>
<li>Rule-based reasoning</li>
</ul>
<p><strong>Analytical tasks</strong>: <span class="math inline">\(T \in [0.4, 0.7]\)</span></p>
<ul>
<li>Problem decomposition</li>
<li>Method selection</li>
<li>Strategy development</li>
</ul>
<p><strong>Creative tasks</strong>: <span class="math inline">\(T \in [0.7, 1.0]\)</span></p>
<ul>
<li>Hypothesis generation</li>
<li>Alternative exploration</li>
<li>Innovation brainstorming</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><p><strong>Quality control mechanisms</strong>:</p>
<p><strong>Reasoning validation</strong>:</p>
<ul>
<li>Logical consistency checking</li>
<li>Factual accuracy verification</li>
<li>Domain knowledge compliance</li>
<li>Mathematical correctness validation</li>
</ul>
<p><strong>Temperature optimization</strong>:</p>
<ul>
<li>A/B testing different temperature ranges</li>
<li>Performance monitoring across tasks</li>
<li>Expert evaluation of reasoning quality</li>
<li>Automated reasoning assessment metrics</li>
</ul></li>
<li><p><strong>Production considerations</strong>:</p>
<ul>
<li>Latency vs.&nbsp;quality trade-offs</li>
<li>Caching common reasoning patterns</li>
<li>Fallback to simpler models when needed</li>
</ul></li>
</ul>
</div></div>
</section></section>
<section>
<section id="how-are-llms-trained" class="title-slide slide level1 center">
<h1>How Are LLMs Trained?</h1>

</section>
<section id="general-idea" class="slide level2">
<h2>General Idea</h2>
<ul>
<li>When you start with a transformer architecture, every single weight/bias inside of the neural networks is initialized to a random value.</li>
<li>The training process consists of providing the model with “examples” of input and output pairs.</li>
<li>An insample measure of how well the model is doing is called the <strong>loss</strong>, and this loss is then minimized using some sort of <strong>optimizer</strong>.</li>
<li>However, there is a deep difference between letting the model predict the next token in a sentence, and actually having a LLM able to answer questions, write essays, or even code.</li>
</ul>
</section>
<section id="first-step-pretraining" class="slide level2">
<h2>First step: Pretraining</h2>
<ul>
<li>Your LLM needs to first learn to <strong>speak</strong> the language, understand complex relationships between words, and posses a <strong>general knowledge</strong>.</li>
</ul>
</section>
<section id="common-data-sources" class="slide level2">
<h2>Common Data Sources</h2>
<table class="caption-top">
<thead>
<tr class="header">
<th>#</th>
<th>Corpus (family)</th>
<th>Typical size†</th>
<th>What it is</th>
<th>Prominent models that rely on it</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td><strong>Common Crawl</strong></td>
<td>250 B+ pages ‡</td>
<td>Raw monthly web crawl released since 2007; the raw source many teams curate from.</td>
<td>GPT-3/4, Claude, Gemini, Llama-3/4, Falcon, BLOOM, PaLM-2, Mistral</td>
<td>(<a href="https://commoncrawl.org/?utm_source=chatgpt.com" title="Common Crawl - Open Repository of Web Crawl Data">commoncrawl.org</a>)</td>
</tr>
<tr class="even">
<td>2</td>
<td><strong>C4 (Colossal Clean Crawled)</strong></td>
<td>~750 GB</td>
<td>Google’s filtered English slice of one CC snapshot (Apr-2019).</td>
<td>T5 / FLAN-T5, PaLM-2, Gemma</td>
<td>(<a href="https://paperswithcode.com/dataset/c4?utm_source=chatgpt.com" title="C4 Dataset | Papers With Code">paperswithcode.com</a>)</td>
</tr>
<tr class="odd">
<td>3</td>
<td><strong>CC-100 / CC-Net family</strong></td>
<td>2 TB+ (100 langs)</td>
<td>Per-language cleaned Common Crawl—crucial for multilingual LLMs.</td>
<td>Llama-1/2/3, XLM-R, BLOOM-z</td>
<td>(<a href="https://paperswithcode.com/dataset/cc100?utm_source=chatgpt.com" title="CC100 Dataset - Papers With Code">paperswithcode.com</a>)</td>
</tr>
<tr class="even">
<td>4</td>
<td><strong>Wikipedia</strong></td>
<td>6 GB (en)</td>
<td>Snapshot of all encyclopedia articles; high-quality factual prose.</td>
<td>Nearly every model (GPT-3 mix: 3 % of tokens)</td>
<td>(<a href="https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/?utm_source=chatgpt.com" title="How did OpenAI scrap the entire Internet for training Chat GPT?">reddit.com</a>)</td>
</tr>
<tr class="odd">
<td>5</td>
<td><strong>BookCorpus / Books 1 &amp; 2</strong></td>
<td>16 GB + 45 GB</td>
<td>Public-domain &amp; self-published novels; offers long-form narrative.</td>
<td>GPT-2/3, PaLM, many Instruct models</td>
<td>(<a href="https://www.reddit.com/r/webscraping/comments/1bapx0j/how_did_openai_scrap_the_entire_internet_for/?utm_source=chatgpt.com" title="How did OpenAI scrap the entire Internet for training Chat GPT?">reddit.com</a>)</td>
</tr>
<tr class="even">
<td>6</td>
<td><strong>Books 3 / LibGen</strong></td>
<td>196 k books (~100 GB)</td>
<td>Shadow-library scrape—controversial but common in recent LLMs.</td>
<td>Llama-1/2/3, StableLM, Mistral (per lawsuits)</td>
<td>(<a href="https://news.bloomberglaw.com/ip-law/meta-hit-with-another-ai-model-copyright-lawsuit-from-author?utm_source=chatgpt.com" title="Meta Hit With Another AI Model Copyright Lawsuit from Author">news.bloomberglaw.com</a>)</td>
</tr>
<tr class="odd">
<td>7</td>
<td><strong>WebText &amp; OpenWebText</strong></td>
<td>40 GB</td>
<td>Reddit-filtered outbound links; original GPT-2 corpus and its open clone.</td>
<td>GPT-2, GPT-3 (WebText 2 = 22 % of tokens), many hobby GPT-2 replicas</td>
<td>(<a href="https://en.wikipedia.org/wiki/GPT-2?utm_source=chatgpt.com" title="GPT-2">en.wikipedia.org</a>, <a href="https://huggingface.co/datasets/Skylion007/openwebtext?utm_source=chatgpt.com" title="Skylion007/openwebtext · Datasets at Hugging Face">huggingface.co</a>)</td>
</tr>
<tr class="even">
<td>8</td>
<td><strong>The Pile</strong></td>
<td>825 GB</td>
<td>22-source mega-mix (arXiv, PubMed, GitHub, StackExchange, etc.).</td>
<td>GPT-Neo/J/NeoX, BLOOM, MPT-7B, Llama-3-open derivative</td>
<td>(<a href="https://huggingface.co/EleutherAI/gpt-neox-20b?utm_source=chatgpt.com" title="EleutherAI/gpt-neox-20b - Hugging Face">huggingface.co</a>)</td>
</tr>
<tr class="odd">
<td>9</td>
<td><strong>RedPajama → SlimPajama</strong></td>
<td>1.2 T → 627 B tokens</td>
<td>Together AI’s recreation of the GPT-3 mixture; Slim version is deduped.</td>
<td>Cerebras-GPT, Snowflake Arctic, Mistral-medium</td>
<td>(<a href="https://www.cerebras.ai/blog/slimpajama-a-627b-token-cleaned-and-deduplicated-version-of-redpajama?utm_source=chatgpt.com" title="SlimPajama: A 627B token, cleaned and deduplicated version of ...">cerebras.ai</a>)</td>
</tr>
<tr class="even">
<td>10</td>
<td><strong>RefinedWeb</strong></td>
<td>5 T tokens</td>
<td>Heavily filtered web-only dataset built for Falcon models.</td>
<td>Falcon-40B/180B</td>
<td>(<a href="https://kili-technology.com/large-language-models-llms/9-open-sourced-datasets-for-training-large-language-models?utm_source=chatgpt.com" title="Open-Sourced Training Datasets for Large Language Models (LLMs)">kili-technology.com</a>)</td>
</tr>
<tr class="odd">
<td>11</td>
<td><strong>Dolma</strong></td>
<td>3 T tokens</td>
<td>Allen AI’s fully open 200-TB → 11-TB cleaned corpus (web, code, papers, books).</td>
<td>OLMo-7B/65B; used as a research benchmark</td>
<td>(<a href="https://github.com/allenai/dolma?utm_source=chatgpt.com" title="allenai/dolma: Data and tools for generating and inspecting ... - GitHub">github.com</a>)</td>
</tr>
<tr class="even">
<td>12</td>
<td><strong>MassiveText</strong></td>
<td>10 TB</td>
<td>Proprietary quality-filtered mix (web, books, news, code).</td>
<td>Reportedly part of GPT-3, Claude-v1</td>
<td>(<a href="https://paperswithcode.com/dataset/massivetext?utm_source=chatgpt.com" title="MassiveText Dataset - Papers With Code">paperswithcode.com</a>)</td>
</tr>
<tr class="odd">
<td>13</td>
<td><strong>The Stack / Stack v2</strong></td>
<td>3-4 T tokens code</td>
<td>Permissively-licensed GitHub code, notebooks, issues.</td>
<td>StarCoder-(1&amp;2), Code Llama, Qwen-Coder</td>
<td>(<a href="https://oumi.ai/docs/en/latest/api/oumi.datasets.pretraining.html?utm_source=chatgpt.com" title="oumi.datasets.pretraining">oumi.ai</a>)</td>
</tr>
<tr class="even">
<td>14</td>
<td><strong>StackOverflow / StackExchange dumps</strong></td>
<td>35 GB</td>
<td>QA pairs useful for dialogue and code reasoning.</td>
<td>PaLM-Code, WizardCoder, many RLHF bases</td>
<td>(<a href="https://github.com/Zjh-819/LLMDataHub?utm_source=chatgpt.com" title="LLMDataHub: Awesome Datasets for LLM Training - GitHub">github.com</a>)</td>
</tr>
</tbody>
</table>
</section>
<section id="training-process-depends-on-the-architecture" class="slide level2">
<h2>Training process depends on the architecture</h2>
<table class="caption-top">
<colgroup>
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
<col style="width: 20%">
</colgroup>
<thead>
<tr class="header">
<th>Architecture</th>
<th>Pre-training view of a sample sentence <em>“The quick brown fox jumps …”</em></th>
<th>Typical objective &amp; loss</th>
<th>Why that matches the architecture</th>
<th>Core strengths after pre-training</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>Decoder-only (causal)</strong><br>e.g.&nbsp;GPT-3/4, Llama, Claude</td>
<td>Treat the prefix <strong>The quick brown fox</strong> as <strong>visible</strong> and the next token <strong>jumps</strong> as the one to predict. At each position <em>t</em> you see tokens ≤ <em>t</em> only.</td>
<td>Causal / autoregressive LM:<br> maximize P(tokenₜ | tokens &lt; t) via cross-entropy.</td>
<td>A single Transformer block with <strong>masked self-attention</strong> causal mask (triangular) already enforces “look-left-only,” so training = generation.</td>
<td>Free-form generation, long-context reasoning, code completion, RLHF chat.</td>
</tr>
<tr class="even">
<td><strong>Encoder-only (bidirectional)</strong><br>e.g.&nbsp;BERT, DeBERTa</td>
<td>Randomly replace ≈15 % of tokens with <strong>[MASK]</strong> → “The quick <strong>[MASK]</strong> fox jumps …”; model sees <em>all</em> tokens simultaneously (no causal mask).</td>
<td>Masked LM (MLM):<br> maximize P(original_token | all <em>other</em> tokens).</td>
<td>Bidirectional attention lets every position attend to both left &amp; right context, so masking is required to stop trivial copying.</td>
<td>Sentence/paragraph understanding, retrieval, classification, embeddings.</td>
</tr>
<tr class="odd">
<td><strong>Encoder-decoder (seq-to-seq / denoising)</strong><br>e.g.&nbsp;T5, BART, UL2</td>
<td>Encoder sees a <em>corrupted</em> (“noised”) input: spans dropped, shuffled, or masked.<br>Decoder must reconstruct the <em>clean</em> version token-by-token.</td>
<td>Span-corruption / text-infilling:<br> cross-entropy over target sequence generated by the decoder.</td>
<td>Split architecture: encoder encodes <em>source</em>; decoder autoregressively produces <em>target</em>. Any corruption that yields “source→target” pairs works.</td>
<td>Translation, summarisation, Q&amp;A, instruction-following; easier to fine-tune on supervised seq-to-seq tasks.</td>
</tr>
<tr class="even">
<td><strong>Mixture-of-experts (MoE)</strong><br>e.g.&nbsp;Google Switch, DeepMind GLaM</td>
<td>Same objectives as above (usually causal), but each token is routed to a small subset of expert FFNs.</td>
<td>Still cross-entropy; additional load-balancing loss.</td>
<td>Sparse routing lets you scale parameters without proportional FLOPs.</td>
<td>Generation at lower inference cost per parameter.</td>
</tr>
<tr class="odd">
<td><strong>Retrieval-augmented decoders</strong><br>e.g.&nbsp;RETRO, Atlas</td>
<td>At each step the model queries an external index and conditions on retrieved passages.</td>
<td>Joint loss: cross-entropy on next token + contrastive/KL term aligning internal and retrieved representations.</td>
<td>Decoder cross-attends to retrieved text; retrieval bridge provides fresh knowledge without storing it in weights.</td>
<td>Up-to-date factual QA, long-tail knowledge, smaller base model.</td>
</tr>
</tbody>
</table>
</section>
<section id="what-next" class="slide level2">
<h2>What next?</h2>
<ul>
<li>Once the gargantuan self-supervised pass is finished, the model can speak fluent “Internet,” but it is not yet an aligned assistant, nor is it specialised for any industry task. Current pipelines add three big, partly overlapping phases:</li>
</ul>
<ol type="1">
<li>Instruction-tuning (a.k.a. supervised fine-tuning, SFT)</li>
<li>Preference alignment</li>
<li>Post-alignment specialisation</li>
</ol>
</section>
<section id="instruction-tuning-in-practice" class="slide level2">
<h2>Instruction-tuning in practice</h2>
<ul>
<li><strong>Open models</strong>: FLAN-T5, Llama-2-Chat, Mistral-Instruct all do a 1–3 epoch SFT pass on 5M–25M instruction–response pairs.</li>
<li><strong>Commercial models</strong>: OpenAI and Anthropic report hundreds of specialised SFT sets covering tools, code, safety scenarios, etc.</li>
</ul>
</section>
<section id="preference-alignment-in-practice" class="slide level2">
<h2>Preference alignment in practice</h2>
<ul>
<li>Preference alignment typically involves:
<ul>
<li>Collecting human preferences on model outputs</li>
<li>Training a reward model on these preferences</li>
<li>Optimizing the model using reinforcement learning</li>
</ul></li>
<li>Most commercial models use some form of RLHF (Reinforcement Learning from Human Feedback)</li>
<li>Constitutional AI (CAI) represents an evolution of this approach</li>
</ul>
</section>
<section id="post-alignment-specialisation" class="slide level2">
<h2>Post-alignment specialisation</h2>
<ul>
<li>Domain adaptation: Financial models, healthcare models, legal models</li>
<li>Tool usage: Models trained to use specific tools (calculators, APIs, etc.)</li>
<li>Multimodality: Models that can process images, audio, and video alongside text</li>
<li>Fine-tuning for specific tasks: Summarization, translation, code generation, etc.</li>
</ul>
</section></section>
<section>
<section id="zero-one-and-multiple-shot-prompts" class="title-slide slide level1 center">
<h1>Zero, One, and Multiple-Shot Prompts</h1>

</section>
<section id="understanding-prompt-engineering" class="slide level2">
<h2>Understanding Prompt Engineering</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Prompts: instructions that guide LLM outputs</li>
<li>Art and science of communicating with LLMs</li>
<li>Critical skill for effective LLM utilization</li>
<li>Influences the quality, relevance, and accuracy of responses</li>
<li>Key to unlocking LLM capabilities across domains</li>
</ul>
</div></div>
</section>
<section id="zero-shot-prompting-basics" class="slide level2">
<h2>Zero-Shot Prompting: Basics</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Model responds without examples</li>
<li>Example: “Explain the concept of gravity”</li>
<li>Benefits: Simplicity and directness</li>
<li>Limitations: Less control over output format and style</li>
<li>Applications: Quick insights, basic definitions, general queries</li>
</ul>
</div><div class="column" style="width:40%;">
<pre><code>Explain the concept of 
photosynthesis and its 
importance for life on Earth.</code></pre>
<pre><code>Analyze the potential impact 
of climate change on ocean 
temperatures and currents.</code></pre>
</div></div>
</section>
<section id="zero-shot-prompting-applications" class="slide level2">
<h2>Zero-Shot Prompting: Applications</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Technical foundation</strong>: Model uses pre-training distribution to generate responses</li>
<li><strong>Success factors</strong>: Clarity, specificity, and proper framing</li>
<li><strong>General use cases</strong>:
<ul>
<li>Concept explanations</li>
<li>Quick factual queries</li>
<li>Basic analysis requests</li>
<li>Initial research questions</li>
<li>Educational content</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="one-shot-prompting-basics" class="slide level2">
<h2>One-Shot Prompting: Basics</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Providing a single example before the query</li>
<li>Guides the model on expected output format and style</li>
<li>Example:
<ul>
<li>“Q: What is gravity? A: Gravity is a fundamental force that attracts objects…”</li>
<li>“Now, what is electromagnetic force?”</li>
</ul></li>
<li>Applications: Consistent reports, standardized analyses, format control</li>
</ul>
</div><div class="column" style="width:40%;">
<pre><code>Q: What is photosynthesis?
A: Photosynthesis is the process 
by which plants convert sunlight, 
carbon dioxide, and water into 
glucose and oxygen. It's essential 
for life on Earth as it produces 
the oxygen we breathe.

Q: What is cellular respiration?
A:</code></pre>
</div></div>
</section>
<section id="one-shot-prompting-applications" class="slide level2">
<h2>One-Shot Prompting: Applications</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Cognitive foundations</strong>: Creates local context that guides prediction</li>
<li><strong>Why one-shot works better than zero-shot</strong>:
<ul>
<li><strong>Pattern recognition</strong>: LLMs excel at identifying and following patterns from examples</li>
<li><strong>Format specification</strong>: Examples clarify desired output structure</li>
<li><strong>Disambiguation</strong>: Reduces ambiguity about what type of response is expected</li>
<li><strong>Context priming</strong>: Examples activate relevant neural pathways in the model</li>
</ul></li>
<li><strong>When to use</strong>:
<ul>
<li>When format consistency is critical</li>
<li>For standardized analyses across different topics</li>
<li>To control response length and structure</li>
<li>For regular reporting or documentation</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<pre><code>BOOK REVIEW: 1984
Pages: 328
Genre: Dystopian Fiction
Rating: 4.5/5
Summary: Powerful exploration 
of totalitarianism and surveillance

BOOK REVIEW: Dune</code></pre>
</div></div>
</section>
<section id="one-shot-case-studies-part-1" class="slide level2">
<h2>One-Shot Case Studies: Part 1</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Product reviews</strong>: Standardized format across different products</li>
<li><strong>Research summaries</strong>: Consistent structure for different academic papers</li>
<li><strong>Technical documentation</strong>: Uniform format for different software tools</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Empirical observations</strong>:
<ul>
<li>42% improvement in format consistency</li>
<li>37% reduction in hallucinated information</li>
<li>63% higher user satisfaction ratings</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="one-shot-case-studies-part-2" class="slide level2">
<h2>One-Shot Case Studies: Part 2</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Meeting minutes</strong>: Maintaining consistent structure across different meetings</li>
<li><strong>Scientific analyses</strong>: Standard template applied to different experiments</li>
<li><strong>Content summaries</strong>: Consistent presentation across different media types</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Efficiency metrics</strong>:
<ul>
<li>29% faster production of standardized reports</li>
<li>Significant improvement in accuracy</li>
<li>34% higher quality approval rate</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="few-shot-prompting-concepts" class="slide level2">
<h2>Few-Shot Prompting: Concepts</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Multiple examples before the target query</li>
<li>Establishes patterns for the model to follow</li>
<li>Creates stronger “mental models” within LLM context</li>
<li>Especially useful for specialized or complex tasks</li>
<li>Applications: Complex analysis templates, specialized formats, domain-specific tasks</li>
</ul>
</div><div class="column" style="width:40%;">
<pre><code>Analyze these movies:

Inception: Complex narrative, 
excellent cinematography, 
thought-provoking themes. 
Rating: 9/10

Interstellar: Stunning visuals, 
emotional depth, scientific 
accuracy. Rating: 8.5/10

The Matrix: Groundbreaking 
effects, philosophical depth, 
cultural impact. Rating: 9/10

Blade Runner 2049:</code></pre>
</div></div>
</section>
<section id="few-shot-prompting-implementation" class="slide level2">
<h2>Few-Shot Prompting: Implementation</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Technical mechanism</strong>: Reinforces pattern recognition within the attention layers</li>
<li><strong>Why multiple-shot works better than one-shot</strong>:
<ul>
<li><strong>Stronger pattern establishment</strong>: More examples create more robust patterns</li>
<li><strong>Variance reduction</strong>: Multiple examples reduce the chance of following an outlier pattern</li>
<li><strong>Complex pattern recognition</strong>: Some patterns require multiple instances to be clear</li>
<li><strong>Edge case coverage</strong>: More examples can demonstrate how to handle variations</li>
</ul></li>
<li><strong>Optimal examples</strong>: 3-5 diverse but consistent demonstrations</li>
<li><strong>Key advantage</strong>: Balance between flexibility and standardization</li>
<li><strong>Best practices</strong>:
<ul>
<li>Provide relevant and diverse examples</li>
<li>Maintain consistent format across examples</li>
<li>Order examples from simple to complex</li>
<li>Include diverse but related scenarios</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<pre><code>Mathematical equation solving:

Quadratic: ax² + bx + c = 0
Solution: x = (-b ± √(b²-4ac)) / 2a
Example: x² - 5x + 6 = 0
Result: x = 2 or x = 3

Linear: ax + b = 0
Solution: x = -b/a
Example: 3x - 9 = 0
Result: x = 3

Exponential: aˣ = b
Solution: x = log_a(b)
Example: 2ˣ = 8
Result: x = 3

Logarithmic: log_a(x) = b</code></pre>
</div></div>
</section>
<section id="complex-few-shot-scenarios-part-1" class="slide level2">
<h2>Complex Few-Shot Scenarios: Part 1</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Scientific hypothesis testing</strong>: Demonstrating experimental design across different fields</li>
<li><strong>Literary analysis</strong>: Multiple example analyses before target text</li>
<li><strong>Problem-solving frameworks</strong>: Pattern of systematic approaches across scenarios</li>
</ul>
</div><div class="column" style="width:40%;">
<pre><code>Research Proposal Analysis:

Topic: Climate Change Impact
Methodology: Longitudinal study
Sample Size: 1000 participants
Expected Duration: 3 years
Recommendation: APPROVE
Rationale: Strong methodology, 
adequate sample, feasible timeline

Topic: AI Ethics Framework
Methodology: Mixed methods
Sample Size: 500 interviews
Expected Duration: 18 months
Recommendation: REVISE
Rationale: Needs clearer metrics, 
broader stakeholder inclusion

Topic: Urban Planning Optimization</code></pre>
</div></div>
</section>
<section id="complex-few-shot-scenarios-part-2" class="slide level2">
<h2>Complex Few-Shot Scenarios: Part 2</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Algorithm optimization</strong>: Examples of different optimization approaches</li>
<li><strong>Quality assessment</strong>: Systematic evaluation procedure demonstrations</li>
<li><strong>Pattern recognition</strong>: Examples of identifying anomalies or trends</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Implementation considerations</strong>:
<ul>
<li>Example diversity for robust generalization</li>
<li>Domain-specific terminology and conventions</li>
<li>Recent examples to capture current best practices</li>
<li>Progressive complexity in demonstrations</li>
<li>Clear delineation between examples and query</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="chain-of-thought-prompting-concepts" class="slide level2">
<h2>Chain-of-Thought Prompting: Concepts</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Encourages step-by-step reasoning</li>
<li>Prompt: “Think through this problem step by step”</li>
<li>Dramatically improves performance on complex tasks</li>
<li>Reduces logical errors and hallucinations</li>
<li><strong>Research foundation</strong>: Wei et al.&nbsp;(2022) demonstrated 20-40% improvement on mathematical problems</li>
</ul>
</div><div class="column" style="width:40%;">
<pre><code>To solve this physics problem:

1) First, identify the given 
   variables and what we need to find
2) Choose the appropriate 
   physics equation or principle
3) Substitute the known values
4) Solve for the unknown variable
5) Check units and reasonableness
6) State the final answer with 
   proper significant figures</code></pre>
</div></div>
</section>
<section id="chain-of-thought-prompting-financial-applications" class="slide level2">
<h2>Chain-of-Thought Prompting: Financial Applications</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Problem-solving impact</strong>:
<ul>
<li>52% reduction in calculation errors</li>
<li>63% improvement in logical consistency</li>
<li>47% better alignment with established principles</li>
</ul></li>
<li><strong>Key use cases</strong>:
<ul>
<li>Complex analytical problems</li>
<li>Multi-step calculations</li>
<li>Logical reasoning tasks</li>
<li>Decision-making processes</li>
<li>Research methodology frameworks</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<pre><code>Analyze whether we should 
implement this new software system:

Let me think step by step:
1. Assess current system limitations
2. Evaluate proposed benefits
3. Calculate implementation costs
4. Consider training requirements
5. Analyze potential risks
6. Determine timeline feasibility
7. Weigh alternatives</code></pre>
</div></div>
</section>
<section id="cot-implementation-techniques" class="slide level2">
<h2>CoT Implementation Techniques</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Explicit CoT</strong>: “Walk through the solution to this differential equation step by step”</li>
<li><strong>Few-shot CoT</strong>: Providing examples of reasoning steps before the task</li>
<li><strong>Self-consistency CoT</strong>: Generate multiple reasoning paths and select the most consistent result</li>
<li><strong>Structured CoT</strong>: Predefined framework for analysis (e.g., problem identification, hypothesis, testing, conclusion)</li>
<li><strong>Recursive CoT</strong>: Breaking complex problems into subproblems</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>General applications</strong>:
<ul>
<li>Mathematical problem solving</li>
<li>Scientific method applications</li>
<li>Engineering design processes</li>
<li>Quality assurance procedures</li>
<li>Research methodology</li>
<li>Decision analysis frameworks</li>
<li>Strategic planning</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="why-do-these-techniques-work" class="slide level2">
<h2>Why Do These Techniques Work?</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>In-context learning</strong>: LLMs can adapt behavior based on examples in the prompt</li>
<li><strong>Statistical pattern recognition</strong>: Examples create local patterns that guide prediction</li>
<li><strong>Attention mechanism</strong>: Examples direct model attention to relevant features</li>
<li><strong>Task disambiguation</strong>: Examples clarify the exact task required</li>
<li><strong>Emergent reasoning</strong>: Complex prompting unlocks latent reasoning capabilities</li>
<li><strong>Cognitive alignment</strong>: Prompts create structures similar to human reasoning</li>
<li><strong>Distribution steering</strong>: Examples shift the output distribution toward desired regions</li>
<li><strong>Activation pattern guidance</strong>: Examples activate specific neural pathways</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>The science behind it</strong>:
<ul>
<li><a href="https://arxiv.org/abs/2201.11903">Wei et al.&nbsp;2022</a>: Chain-of-thought prompting</li>
<li><a href="https://arxiv.org/abs/2005.14165">Brown et al.&nbsp;2020</a>: Few-shot learning in GPT-3</li>
<li><a href="https://arxiv.org/abs/2205.11916">Kojima et al.&nbsp;2022</a>: Zero-shot reasoning</li>
<li><a href="https://arxiv.org/abs/2210.09261">Wang et al.&nbsp;2022</a>: Self-consistency improvements</li>
<li><a href="https://arxiv.org/abs/2305.10601">Yao et al.&nbsp;2023</a>: Tree of Thoughts reasoning</li>
<li><a href="https://arxiv.org/abs/2302.04023">Anil et al.&nbsp;2023</a>: Gemini capabilities and reasoning</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="neuroscience-perspective-on-prompt-engineering" class="slide level2">
<h2>Neuroscience Perspective on Prompt Engineering</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Working memory augmentation</strong>: Prompts provide external memory buffer</li>
<li><strong>Cognitive priming</strong>: Examples prime the model for specific response patterns</li>
<li><strong>Neural pathway activation</strong>: Specific prompt structures activate relevant neural circuits</li>
<li><strong>Attentional focus</strong>: Examples direct computational resources to relevant features</li>
<li><strong>Conceptual scaffolding</strong>: Prompts provide structured frameworks for complex reasoning</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>General parallels</strong>:
<ul>
<li>Researchers use systematic methodologies</li>
<li>Engineers follow structured design processes</li>
<li>Quality control requires systematic approaches</li>
<li>Problem-solving follows established patterns</li>
<li>Analysis builds on standard frameworks</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="advanced-prompting-techniques" class="slide level2">
<h2>Advanced Prompting Techniques</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Role-based prompting</strong>: “As a research scientist, evaluate…”</li>
<li><strong>System-user framework</strong>: Setting persistent system context for specialized applications</li>
<li><strong>Self-critique prompting</strong>: Having the model critique its own analysis</li>
<li><strong>ReAct prompting</strong>: Reasoning → Action → Observation loop for problem-solving</li>
<li><strong>Tree of Thoughts</strong>: Exploring multiple reasoning branches for complex decisions</li>
<li><strong>Constrained generation</strong>: Enforcing specific output formats for consistency</li>
</ul>
</div><div class="column" style="width:40%;">
<pre><code>You are a peer reviewer for a 
scientific journal. Ensure all 
responses follow academic 
standards including proper 
citation format and objective 
evaluation criteria.

[SYSTEM CONTEXT]

User: Please review this research 
proposal on renewable energy 
storage solutions.</code></pre>
</div></div>
</section>
<section id="prompting-for-accuracy-in-general-contexts" class="slide level2">
<h2>Prompting for Accuracy in General Contexts</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Factual anchoring</strong>: Providing verifiable data in prompts</li>
<li><strong>Anti-hallucination techniques</strong>: “Only use information explicitly provided”</li>
<li><strong>Uncertainty expression</strong>: Encouraging models to express confidence levels</li>
<li><strong>Citation prompting</strong>: Requesting sources for claims</li>
<li><strong>Verification requests</strong>: Including accuracy checks in analysis</li>
<li><strong>Temporal awareness</strong>: Clarifying time-sensitive information</li>
</ul>
</div><div class="column" style="width:40%;">
<pre><code>Today's date is 2024-12-15.

Using only the following data 
about renewable energy adoption 
(do not use any other information):
- Solar capacity: 150 GW
- Wind capacity: 120 GW
- Growth rate: 15% annually
- Cost reduction: 8% per year

Provide an analysis of trends.
If you are uncertain about any 
aspect, explicitly state your 
uncertainty.</code></pre>
</div></div>
</section>
<section id="measuring-prompt-effectiveness" class="slide level2">
<h2>Measuring Prompt Effectiveness</h2>
<ul>
<li><strong>Performance metrics</strong>: Accuracy, consistency, relevance, clarity</li>
<li><strong>A/B testing</strong>: Comparing different prompting strategies on identical tasks</li>
<li><strong>Benchmarking</strong>: Domain-specific reasoning benchmarks and evaluation datasets</li>
<li><strong>Expert evaluation</strong>: Domain professional assessment of outputs</li>
<li><strong>Quality standards</strong>: Compliance with established guidelines and best practices</li>
<li><strong>Impact assessment</strong>: Real-world outcomes and effectiveness of model outputs</li>
</ul>
</section>
<section id="prompt-engineering-best-practices" class="slide level2">
<h2>Prompt Engineering Best Practices</h2>
<ul>
<li>Establish clear context (domain, time period, constraints)</li>
<li>Define explicit requirements and expectations</li>
<li>Include relevant data and authoritative sources</li>
<li>Specify required output format for consistency</li>
<li>Use domain-appropriate terminology and conventions</li>
<li>Implement verification steps for accuracy</li>
<li>Clarify quality and standard expectations</li>
<li>Consider ethical implications and responsible use</li>
</ul>
</section>
<section id="general-prompt-template-library" class="slide level2">
<h2>General Prompt Template Library</h2>
<ul>
<li>Research analysis framework</li>
<li>Quality assessment protocol</li>
<li>Problem-solving methodology</li>
<li>Report structure templates</li>
<li>Decision-making approaches</li>
<li>Data analysis workflows</li>
<li>Evaluation criteria checklists</li>
</ul>
</section>
<section id="key-takeaways-on-prompt-engineering" class="slide level2">
<h2>Key Takeaways on Prompt Engineering</h2>
<ul>
<li>Prompt engineering is both an art and a science</li>
<li>Different prompting techniques serve different purposes</li>
<li>Chain-of-thought prompting dramatically improves reasoning</li>
<li>Financial applications benefit from structured, systematic prompting</li>
<li>Best practices evolve as LLM capabilities advance</li>
</ul>
</section></section>
<section>
<section id="tracing-thoughts-in-llms" class="title-slide slide level1 center">
<h1>Tracing Thoughts in LLMs</h1>

</section>
<section id="understanding-llm-reasoning-processes" class="slide level2">
<h2>Understanding LLM Reasoning Processes</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>LLMs don’t “think” linearly like humans</li>
<li>Attention mechanisms connect distant tokens</li>
<li>Hidden reasoning in complex tasks</li>
<li>Importance of making reasoning explicit</li>
<li>Impact on reliability in financial applications</li>
<li><strong>Neural activation patterns</strong>: How financial concepts activate model neurons</li>
<li><strong>Computational graph interpretation</strong>: Mapping the flow of financial reasoning</li>
<li><strong>Attribution analysis</strong>: Which inputs most influence financial outputs</li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="../../images/attention_paper.png"></p>
<figcaption>Thought Tracing</figcaption>
</figure>
</div>
<ul>
<li><strong>Key interpretability methods</strong>:
<ul>
<li>Attention visualization</li>
<li>Neuron activation mapping</li>
<li>Integrated gradients</li>
<li>SHAP values for finance</li>
<li>Counterfactual analysis</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="anthropics-research-on-interpretability" class="slide level2">
<h2>Anthropic’s Research on Interpretability</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Claude team pioneering work on LLM interpretability</li>
<li><strong>Mechanistic interpretability</strong>: Understanding computation through circuit analysis</li>
<li><strong>Activation engineering</strong>: Manipulating internal activations to understand behavior</li>
<li><strong>Causal tracing</strong>: Identifying which parts of a prompt impact specific parts of the response</li>
<li><strong>Logit lens</strong>: Examining how token probabilities evolve through model layers</li>
<li><strong>Constitution AI approach</strong>: Using principles to guide model behavior</li>
<li><strong>RLHF interpretability</strong>: Understanding how human feedback shapes model behavior</li>
<li><strong>Interpretability in-the-wild</strong>: Studying deployed models in financial contexts</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li>Key insights from research:
<ul>
<li>LLMs develop internal “representations”</li>
<li>Attention patterns reveal reasoning steps</li>
<li>Different layers handle different abstractions</li>
<li>Financial concepts have distinctive activation patterns</li>
<li>Specific neurons activate for financial terms</li>
<li>Early layers process syntax, later layers handle semantics</li>
<li>Model capabilities emerge from collective neuron behavior</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="key-papers-and-findings" class="slide level2">
<h2>Key Papers and Findings</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>“Discovering Latent Knowledge”</strong> (Burns et al., 2022)
<ul>
<li>Extracting implicit knowledge from model weights</li>
<li>Implications for discovering market insights</li>
</ul></li>
<li><strong>“Language Models Can Teach Themselves to Program”</strong> (Haluptzok et al., 2023)
<ul>
<li>Self-improvement capabilities</li>
<li>Applications to algorithmic trading systems</li>
</ul></li>
<li><strong>“Finding Neurons in a Haystack”</strong> (Anthropic, 2023)
<ul>
<li>Identifying specific neurons that recognize financial concepts</li>
<li>Potential for targeted model editing</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Financial applications of research</strong>:
<ul>
<li>Auditing financial advice for bias</li>
<li>Verifying compliance with regulations</li>
<li>Extracting implicit market knowledge</li>
<li>Understanding model decision boundaries</li>
<li>Identifying potential failure modes</li>
<li>Detecting concept drift in financial data</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="chain-of-thought-prompting" class="slide level2">
<h2>Chain-of-Thought Prompting</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Explicitly asking models to “think step by step”</li>
<li>Dramatic improvement in complex financial calculations</li>
<li>Example: “Walk through the calculation of a company’s free cash flow”</li>
<li>Enables verification of intermediary steps</li>
<li>Reduces mathematical errors in financial analyses</li>
<li><strong>Internal mechanics</strong>: Forces alignment between model reasoning and output</li>
<li><strong>Technical implementation</strong>:
<ul>
<li>Zero-shot CoT: “Let’s think step by step”</li>
<li>Few-shot CoT: Examples of reasoning chains</li>
<li>Self-consistency CoT: Multiple reasoning paths</li>
<li>Verified CoT: External verification of steps</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<pre><code>I'll calculate WACC step by step:

1. Cost of equity = 11.6%
2. After-tax cost of debt = 3.75%
3. Equity weight = 70%
4. Debt weight = 30%
5. WACC = 11.6%(70%) + 3.75%(30%)
   = 9.25%</code></pre>
<pre><code>Let me determine if this bond is 
fairly priced:

1. Identify the bond's features:
   - 5-year maturity
   - 4% coupon, semi-annual
   - $1,000 face value
   - Current price: $980

2. Calculate YTM:
   ...</code></pre>
</div></div>
</section>
<section id="financial-reasoning-traces" class="slide level2">
<h2>Financial Reasoning Traces</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Option pricing calculations</strong>: Tracing Black-Scholes reasoning</li>
<li><strong>Credit risk assessment</strong>: Step-by-step evaluation of default probability</li>
<li><strong>Capital budgeting decisions</strong>: Explicit NPV and IRR calculation steps</li>
<li><strong>Portfolio optimization</strong>: Tracing efficient frontier calculations</li>
<li><strong>Valuation model selection</strong>: Reasoning through appropriate methodology</li>
<li><strong>Financial anomaly detection</strong>: Systematic identification of inconsistencies</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Empirical benefits in finance</strong>:
<ul>
<li>57% reduction in mathematical errors</li>
<li>43% improvement in logical consistency</li>
<li>62% better alignment with financial theory</li>
<li>38% increase in regulatory compliance</li>
<li>71% higher analyst confidence in results</li>
<li>44% better detection of edge cases</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="tree-of-thoughts" class="slide level2">
<h2>Tree of Thoughts</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Exploring multiple reasoning paths simultaneously</li>
<li>Evaluating different analytical approaches</li>
<li>Critical for risk assessment and scenario analysis</li>
<li>Applications: Portfolio optimization, investment strategy evaluation</li>
<li>Enhanced decision-making in uncertain market conditions</li>
<li><strong>Technical foundation</strong>: Extension of Chain-of-Thought with branching</li>
<li><strong>Implementation approaches</strong>:
<ul>
<li>Breadth-first exploration of financial scenarios</li>
<li>Depth-first analysis of complex financial problems</li>
<li>Monte Carlo Tree Search for decision optimization</li>
<li>Pruning ineffective financial reasoning branches</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Key research</strong>:
<ul>
<li><a href="https://arxiv.org/abs/2305.10601">Yao et al.&nbsp;2023</a>: Tree of Thoughts framework</li>
<li><a href="https://arxiv.org/abs/2305.08291">Long 2023</a>: Financial decision trees with LLMs</li>
<li><a href="https://arxiv.org/abs/2310.01061">Feng et al.&nbsp;2023</a>: Self-verification in financial analysis</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="financial-decision-trees-in-practice" class="slide level2">
<h2>Financial Decision Trees in Practice</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>M&amp;A decision analysis</strong>:
<ul>
<li>Branch 1: Full acquisition scenario</li>
<li>Branch 2: Partial stake investment</li>
<li>Branch 3: Strategic partnership</li>
<li>Branch 4: Organic growth alternative</li>
</ul></li>
<li><strong>Investment strategy evaluation</strong>:
<ul>
<li>Branch 1: Value investing approach</li>
<li>Branch 2: Growth-oriented strategy</li>
<li>Branch 3: Income-focused portfolio</li>
<li>Branch 4: Market-neutral position</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Risk management application</strong>:
<ul>
<li>Branch 1: High inflation scenario</li>
<li>Branch 2: Recession possibility</li>
<li>Branch 3: Industry disruption</li>
<li>Branch 4: Regulatory changes</li>
<li>Each with sub-branches for response strategies</li>
<li>Probability-weighted outcome analysis</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="implementing-tree-of-thoughts-for-financial-analysis" class="slide level2">
<h2>Implementing Tree of Thoughts for Financial Analysis</h2>
<ul>
<li><strong>Tree of Thoughts framework</strong>:
<ul>
<li>Generate diverse initial perspectives on financial problems</li>
<li>Evaluate reasoning quality using financial criteria</li>
<li>Explore multiple analytical paths with configurable breadth and depth</li>
<li>Select optimal reasoning pathway based on evaluation scores</li>
</ul></li>
<li><strong>Implementation components</strong>:
<ul>
<li>Financial problem formulation</li>
<li>Perspective generation function</li>
<li>Path exploration mechanism</li>
<li>Evaluation criteria for financial reasoning</li>
<li>Best path selection algorithm</li>
</ul></li>
<li><strong>Key parameters</strong>:
<ul>
<li>Breadth: Number of alternative viewpoints to consider</li>
<li>Depth: Number of analytical steps to explore</li>
<li>Evaluation function: Quality assessment criteria</li>
<li>Exploration strategy: Systematic vs.&nbsp;heuristic search</li>
</ul></li>
</ul>
</section>
<section id="visualizing-attention-patterns" class="slide level2">
<h2>Visualizing Attention Patterns</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Attention maps reveal which inputs influence which outputs</li>
<li>Tools like BertViz and OpenAI’s Attention tool</li>
<li>Financial applications:
<ul>
<li>See which financial terms most influence predictions</li>
<li>Visualize relationships between financial concepts</li>
<li>Identify when models focus on relevant vs.&nbsp;irrelevant information</li>
</ul></li>
<li><strong>Technical approaches</strong>:
<ul>
<li>Head-level attention visualization</li>
<li>Layer-wise relevance propagation</li>
<li>Integrated gradients for feature attribution</li>
<li>SHAP values for financial outputs</li>
<li>Attention flow analysis across layers</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="../../images/encoder-decoder-attention.png"></p>
<figcaption>Attention Visualization</figcaption>
</figure>
</div>
<ul>
<li><strong>Visualization tools</strong>:
<ul>
<li><a href="https://github.com/jessevig/bertviz">BertViz</a></li>
<li><a href="https://github.com/jalammar/ecco">Ecco</a></li>
<li><a href="https://pair-code.github.io/lit/">LIT (Language Interpretability Tool)</a></li>
<li><a href="https://captum.ai/">Captum</a> for financial NLP</li>
<li><a href="https://github.com/interpretml/interpret">InterpretML</a></li>
</ul></li>
</ul>
</div></div>
</section>
<section id="financial-attention-case-studies" class="slide level2">
<h2>Financial Attention Case Studies</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Earnings call analysis</strong>:
<ul>
<li>Which phrases capture model attention?</li>
<li>Forward-looking statements vs.&nbsp;historical results</li>
<li>Management tone and sentiment detection</li>
<li>Hidden signals of financial distress</li>
</ul></li>
<li><strong>Financial news impact assessment</strong>:
<ul>
<li>Attention to specific market events</li>
<li>Entity relationships in financial networks</li>
<li>Sentiment propagation across news items</li>
<li>Temporal attention patterns in market reactions</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Regulatory document analysis</strong>:
<ul>
<li>Critical clauses in financial regulations</li>
<li>Compliance requirement identification</li>
<li>Risk disclosure attention patterns</li>
<li>Cross-references in regulatory frameworks</li>
<li>Exception and qualification detection</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="attention-interpretation-in-financial-contexts" class="slide level2">
<h2>Attention Interpretation in Financial Contexts</h2>
<ul>
<li><strong>Financial attention analysis framework</strong>:
<ul>
<li>Process financial texts through transformer models</li>
<li>Extract attention patterns from specified layers and heads</li>
<li>Identify token-to-token attention relationships</li>
<li>Calculate attention weights for financial entities</li>
</ul></li>
<li><strong>Analysis components</strong>:
<ul>
<li>Text tokenization and model input preparation</li>
<li>Attention pattern extraction from transformer layers</li>
<li>Financial entity recognition and mapping</li>
<li>Attention weight calculation for identified entities</li>
<li>Key influence identification for financial reasoning</li>
</ul></li>
<li><strong>Output interpretation</strong>:
<ul>
<li>Attention matrices showing token relationships</li>
<li>Entity-specific attention scores</li>
<li>Most influential tokens for financial analysis</li>
<li>Layer and head-specific attention patterns</li>
<li>Financial reasoning pathway visualization</li>
</ul></li>
</ul>
</section>
<section id="self-consistency-verification" class="slide level2">
<h2>Self-Consistency &amp; Verification</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Generating multiple solutions to the same problem</li>
<li>Identifying inconsistencies in financial reasoning</li>
<li>Cross-checking numerical results and conclusions</li>
<li>Improved accuracy for high-stakes financial decisions</li>
<li>Essential for regulatory compliance and audit trails</li>
<li><strong>Implementation techniques</strong>:
<ul>
<li>Ensemble of reasoning paths</li>
<li>Majority voting on financial decisions</li>
<li>Confidence-weighted aggregation</li>
<li>Sensitivity analysis of financial conclusions</li>
<li>Multi-model cross-verification</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Self-consistent valuation methodology</strong>:
<ul>
<li>Generate multiple independent valuation analyses</li>
<li>Apply different analytical approaches to same company</li>
<li>Extract numerical valuation estimates from each analysis</li>
<li>Calculate consistency metrics across approaches</li>
</ul></li>
<li><strong>Implementation process</strong>:
<ul>
<li>Multi-path reasoning generation</li>
<li>Diverse analytical framework application</li>
<li>Valuation extraction and aggregation</li>
<li>Variance analysis for consistency assessment</li>
<li>Weighted consensus calculation</li>
</ul></li>
<li><strong>Output components</strong>:
<ul>
<li>Consensus valuation estimate</li>
<li>Consistency confidence measure</li>
<li>Multiple reasoning pathway documentation</li>
<li>Approach-specific valuation ranges</li>
<li>Cross-validation reliability metrics</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="real-time-tracing-techniques" class="slide level2">
<h2>Real-time Tracing Techniques</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Token-by-token analysis</strong>: Examining probabilities as each token is generated</li>
<li><strong>Alternative path exploration</strong>: What would the model do differently with small changes?</li>
<li><strong>Prompt sensitivity analysis</strong>: How do slight prompt variations affect reasoning?</li>
<li><strong>Layerwise relevance propagation</strong>: Tracking contribution of each input to outputs</li>
<li><strong>Financial applications</strong>:
<ul>
<li>Regulatory compliance verification</li>
<li>Auditing financial model decisions</li>
<li>Identifying potential biases in financial analysis</li>
<li>Early detection of reasoning failures</li>
<li>Confidence calibration for financial advice</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Tools and frameworks</strong>:
<ul>
<li><a href="https://github.com/EleutherAI/lm-evaluation-harness">Eleuther AI’s language model evaluation harness</a></li>
<li><a href="https://github.com/krishnap25/mauve">MAUVE</a> for distribution comparison</li>
<li><a href="https://github.com/jalammar/ecco">Ecco</a> for token probability analysis</li>
<li><a href="https://github.com/ProsusAI/finBERT">FinBERT</a> for financial sentiment</li>
<li>Custom financial reasoning trackers</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="implementing-llm-tracing-in-production-systems" class="slide level2">
<h2>Implementing LLM Tracing in Production Systems</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Logging strategies</strong>: Capturing model reasoning for later analysis</li>
<li><strong>Monitoring frameworks</strong>: Real-time assessment of reasoning quality</li>
<li><strong>Alert systems</strong>: Flagging potential reasoning failures</li>
<li><strong>Explainability APIs</strong>: Making reasoning transparent to users</li>
<li><strong>Compliance documentation</strong>: Generating audit trails for regulators</li>
<li><strong>Feedback loops</strong>: Improving model reasoning based on outcomes</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>System architecture components</strong>:
<ul>
<li>Reasoning quality metrics dashboard</li>
<li>Token-level confidence visualization</li>
<li>Alternative reasoning path explorer</li>
<li>Financial domain knowledge verification</li>
<li>Model uncertainty quantification</li>
<li>Drift detection for financial concepts</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="advanced-interpretability-research" class="slide level2">
<h2>Advanced Interpretability Research</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Neural Circuit Analysis</strong>: Identifying specific circuits for financial concepts</li>
<li><strong>Causal Mediation Analysis</strong>: Understanding how interventions affect outcomes</li>
<li><strong>Activation Steering</strong>: Guiding model reasoning in desired directions</li>
<li><strong>Adversarial Testing</strong>: Probing for weaknesses in financial reasoning</li>
<li><strong>Controlled Generation</strong>: Ensuring outputs follow regulatory constraints</li>
<li><strong>Faithful Explanations</strong>: Creating truly representative explanations</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Research frontiers</strong>:
<ul>
<li>Identifying financial concept neurons</li>
<li>Transformer interpretability at scale</li>
<li>Causal inference in financial models</li>
<li>Alignment with financial regulations</li>
<li>Financial concept representation learning</li>
<li>Robust reasoning in market volatility</li>
</ul></li>
</ul>
</div></div>
</section></section>
<section>
<section id="ghost-in-the-machine" class="title-slide slide level1 center">
<h1>Ghost in the Machine</h1>

</section>
<section id="understanding-llm-emergent-behaviors" class="slide level2">
<h2>Understanding LLM Emergent Behaviors</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Emergent capabilities beyond explicit training</li>
<li>Scale-dependent phenomena in large models</li>
<li>Unexpected reasoning and generalization abilities</li>
<li>“Sparks of AGI” debate</li>
<li>Implications for financial applications</li>
<li><strong>Scientific foundations</strong>:
<ul>
<li>Phase transitions in complex systems</li>
<li>Information theory perspectives</li>
<li>Computational complexity emergence</li>
<li>Collective neuron behavior models</li>
<li>Self-organizing systems research</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Key emergent capability thresholds</strong>:
<ul>
<li>~100M parameters: Basic pattern recognition</li>
<li>~1B parameters: Robust in-context learning</li>
<li>~10B parameters: Complex reasoning abilities</li>
<li>~100B parameters: Zero-shot task generalization</li>
<li>~1T parameters: Advanced reasoning integration</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="the-emergence-debate-in-ai" class="slide level2">
<h2>The Emergence Debate in AI</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Continuity hypothesis</strong>: Capabilities improve smoothly with scale</li>
<li><strong>Phase transition hypothesis</strong>: Sudden jumps in capabilities at thresholds</li>
<li><strong>Implicit knowledge hypothesis</strong>: Capabilities hidden until prompted correctly</li>
<li><strong>Multi-system hypothesis</strong>: Different cognitive systems emerge independently</li>
<li><strong>Perspectives from leading AI labs</strong>:
<ul>
<li>Google DeepMind on emergence in Gemini models</li>
<li>Anthropic on capabilities in Claude models</li>
<li>OpenAI’s observations in GPT model series</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Empirical evidence</strong>:
<ul>
<li>MMLU score discontinuities</li>
<li>Chain-of-thought effectiveness jumps</li>
<li>Tool use capability thresholds</li>
<li>Theory of mind test performances</li>
<li>Mathematical reasoning breakthroughs</li>
<li>Financial analysis capability shifts</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="unexpected-capabilities" class="slide level2">
<h2>Unexpected Capabilities</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Tool use</strong>: Models learn to use calculators, databases without specific training</li>
<li><strong>Translation</strong>: Zero-shot translation between language pairs never seen together</li>
<li><strong>Mathematical reasoning</strong>: Complex calculations beyond training examples</li>
<li><strong>Meta-learning</strong>: Learning how to learn from few examples</li>
<li><strong>Financial analysis</strong>: Applying general reasoning to specialized financial problems</li>
<li><strong>Algorithmic thinking</strong>: Solving novel computational problems</li>
<li><strong>Analogical reasoning</strong>: Transferring insights across domains</li>
<li><strong>Temporal reasoning</strong>: Understanding time-dependent relationships</li>
<li><strong>Counterfactual analysis</strong>: Evaluating hypothetical scenarios</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Examples in finance</strong>:
<ul>
<li>Discovering arbitrage opportunities</li>
<li>Complex risk assessment</li>
<li>Regulatory compliance checking</li>
<li>Financial anomaly detection</li>
<li>Sophisticated market analysis</li>
<li>Synthesizing economic indicators</li>
<li>Identifying market regime shifts</li>
<li>Cross-asset correlation analysis</li>
<li>Macroeconomic impact forecasting</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="financial-reasoning-case-studies" class="slide level2">
<h2>Financial Reasoning Case Studies</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Option pricing strategies</strong>:
<ul>
<li>GPT-4 solving Black-Scholes equations without training</li>
<li>Correctly identifying volatility smile implications</li>
<li>Understanding Greeks relationships without definitions</li>
</ul></li>
<li><strong>Credit risk assessment</strong>:
<ul>
<li>Detecting subtle default risk indicators</li>
<li>Creating novel early warning systems</li>
<li>Connecting macroeconomic factors to credit events</li>
</ul></li>
<li><strong>Portfolio optimization</strong>:
<ul>
<li>Reinventing modern portfolio theory concepts</li>
<li>Suggesting non-obvious diversification strategies</li>
<li>Discovering factor exposures in complex portfolios</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Market anomaly detection</strong>:
<ul>
<li>Identifying statistical arbitrage opportunities</li>
<li>Spotting potential market manipulation patterns</li>
<li>Flagging unusual trading activity</li>
</ul></li>
<li><strong>Regulatory compliance</strong>:
<ul>
<li>Interpreting complex regulatory frameworks</li>
<li>Identifying potential compliance issues</li>
<li>Suggesting implementation approaches</li>
</ul></li>
<li><strong>Financial forecasting</strong>:
<ul>
<li>Integrating disparate data sources</li>
<li>Identifying leading indicators</li>
<li>Recognizing pattern shifts in time series</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="scaling-laws-and-emergent-abilities" class="slide level2">
<h2>Scaling Laws and Emergent Abilities</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Certain abilities only appear above specific model sizes</li>
<li><strong>Discontinuous improvements</strong>: Sudden jumps in capability with scale</li>
<li><strong>Kaplan et al.&nbsp;scaling laws</strong>: Predictable improvements with model size</li>
<li><strong>Chinchilla scaling</strong>: Optimal data-to-parameter ratios</li>
<li><strong>Implications</strong>: Larger models for finance may unlock new capabilities</li>
<li><strong>Power law scaling</strong>: Performance scaling as a power of compute</li>
<li><strong>Data scaling relationship</strong>: More data needed as models grow</li>
<li><strong>Transfer learning implications</strong>: Better pre-training improves downstream tasks</li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Key scaling research</strong>:
<ul>
<li><a href="https://arxiv.org/abs/2001.08361">Kaplan et al.&nbsp;2020</a>: Original scaling laws</li>
<li><a href="https://arxiv.org/abs/2203.15556">Hoffmann et al.&nbsp;2022</a>: Chinchilla scaling</li>
<li><a href="https://arxiv.org/abs/2206.04615">Wei et al.&nbsp;2022</a>: Emergent abilities</li>
<li><a href="https://arxiv.org/abs/2204.02311">Srivastava et al.&nbsp;2022</a>: BIG-Bench</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="financial-capabilities-emergence-thresholds" class="slide level2">
<h2>Financial Capabilities Emergence Thresholds</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Basic financial understanding</strong>: ~1B parameters
<ul>
<li>Financial terminology comprehension</li>
<li>Simple financial calculations</li>
<li>Basic market concepts</li>
</ul></li>
<li><strong>Intermediate financial analysis</strong>: ~10B parameters
<ul>
<li>DCF valuation execution</li>
<li>Multi-factor model application</li>
<li>Financial statement analysis</li>
<li>Basic risk assessment</li>
</ul></li>
<li><strong>Advanced financial reasoning</strong>: ~100B parameters
<ul>
<li>Multi-step financial strategy development</li>
<li>Complex scenario analysis</li>
<li>Integrated market system understanding</li>
<li>Nuanced regulatory interpretation</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Specialized financial capabilities</strong>:
<ul>
<li>Options strategy design: ~20B parameters</li>
<li>Merger arbitrage analysis: ~50B parameters</li>
<li>Macroeconomic forecasting: ~75B parameters</li>
<li>Systematic trading strategy design: ~100B parameters</li>
<li>Central bank policy impact assessment: ~175B parameters</li>
</ul></li>
<li><strong>Performance examples</strong>:
<ul>
<li>FINQA benchmark: 32% → 76% → 92%</li>
<li>Financial NLI: 61% → 83% → 94%</li>
<li>Financial sentiment: 72% → 89% → 97%</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="computational-implications-for-financial-institutions" class="slide level2">
<h2>Computational Implications for Financial Institutions</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Infrastructure requirements</strong>:
<ul>
<li>GPU/TPU clusters for large model training</li>
<li>Edge deployment for low-latency inference</li>
<li>Memory-efficient serving architectures</li>
<li>Batch processing for efficiency</li>
</ul></li>
<li><strong>Cost considerations</strong>:
<ul>
<li>Training costs: $500K-$10M for large models</li>
<li>Inference optimization techniques</li>
<li>Model distillation for deployment</li>
<li>Hardware acceleration requirements</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Practical strategies</strong>:
<ul>
<li>Fine-tuning existing large models</li>
<li>Parameter-efficient adaptation (LoRA)</li>
<li>Strategic API usage vs.&nbsp;internal deployment</li>
<li>Domain-specific medium-sized models</li>
<li>Hybrid architecture with specialized components</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="hallucinations-and-financial-risk" class="slide level2">
<h2>Hallucinations and Financial Risk</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Confident generation of false information</li>
<li>Potential impacts on financial decision-making</li>
<li>Detection strategies in financial contexts:
<ul>
<li>Consistency checks</li>
<li>External verification</li>
<li>Probability thresholds</li>
<li>Multi-model consensus</li>
</ul></li>
<li>Risk mitigation approaches</li>
<li><strong>Root causes of hallucinations</strong>:
<ul>
<li>Pattern completion gone wrong</li>
<li>Over-extrapolation from training data</li>
<li>Lack of uncertainty calibration</li>
<li>Statistical artifacts in training</li>
<li>Distribution shift from training to application</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Financial hallucination examples</strong>:
<ul>
<li>Fabricated financial metrics</li>
<li>Non-existent regulations</li>
<li>Imaginary market events</li>
<li>False historical performance</li>
<li>Invented company information</li>
<li>Fictional financial instruments</li>
<li>Made-up economic data</li>
<li>Incorrect tax regulations</li>
<li>Fabricated trading strategies</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="hallucination-mitigation-in-financial-applications" class="slide level2">
<h2>Hallucination Mitigation in Financial Applications</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>RAG (Retrieval Augmented Generation)</strong>:
<ul>
<li>Grounding in verified financial data sources</li>
<li>Real-time financial database integration</li>
<li>Citation of primary financial sources</li>
<li>Retrieval verification loops</li>
</ul></li>
<li><strong>Controlled generation techniques</strong>:
<ul>
<li>Constraint-based generation</li>
<li>Template-based financial outputs</li>
<li>Fact checking against known databases</li>
<li>Output filtering with expert systems</li>
<li>Calibrated uncertainty expressions</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Financial fact-checking framework</strong>:
<ul>
<li>Extract financial claims from generated content</li>
<li>Verify claims against trusted financial databases</li>
<li>Cross-reference with authoritative sources</li>
<li>Calculate confidence scores for accuracy assessment</li>
</ul></li>
<li><strong>Verification components</strong>:
<ul>
<li>Financial claim identification and extraction</li>
<li>Database lookup and cross-referencing</li>
<li>Source credibility assessment</li>
<li>Consistency checking across multiple sources</li>
<li>Overall confidence calculation methodology</li>
</ul></li>
<li><strong>Output validation</strong>:
<ul>
<li>Claim-by-claim verification results</li>
<li>Source attribution for verified facts</li>
<li>Confidence scoring for uncertain claims</li>
<li>Flagging of unverifiable assertions</li>
<li>Recommendation for human review thresholds</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="practical-implementation-strategies" class="slide level2">
<h2>Practical Implementation Strategies</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Human-in-the-loop design</strong>:
<ul>
<li>Expert verification of critical financial outputs</li>
<li>Confidence thresholds for human review</li>
<li>Staged generation with checkpoints</li>
<li>Expert feedback incorporation</li>
</ul></li>
<li><strong>Technical implementation patterns</strong>:
<ul>
<li>Self-critique generation before final output</li>
<li>Adversarial validation techniques</li>
<li>Multi-model verification ensemble</li>
<li>Probabilistic output calibration</li>
<li>Output consistency checks</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Financial risk framework</strong>:
<ul>
<li>Critical vs.&nbsp;advisory information classification</li>
<li>Risk-weighted verification allocation</li>
<li>Compliance-sensitive content identification</li>
<li>Uncertainty-aware decision support</li>
<li>Transparency in confidence levels</li>
<li>Auditable generation process</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="financial-regulatory-considerations" class="slide level2">
<h2>Financial Regulatory Considerations</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>SEC requirements</strong>:
<ul>
<li>AI-generated content disclosure</li>
<li>Material information verification</li>
<li>Audit trail requirements</li>
<li>Regulatory reporting considerations</li>
</ul></li>
<li><strong>FINRA guidance</strong>:
<ul>
<li>AI supervision requirements</li>
<li>Financial advice restrictions</li>
<li>Risk disclosure mandates</li>
<li>Record-keeping obligations</li>
</ul></li>
<li><strong>Global regulatory landscape</strong>:
<ul>
<li>EU AI Act implications</li>
<li>UK FCA guidelines</li>
<li>Singapore MAS framework</li>
<li>International coordination efforts</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Practical compliance approaches</strong>:
<ul>
<li>Model documentation standards</li>
<li>Verification process documentation</li>
<li>Human oversight frameworks</li>
<li>Regular audit procedures</li>
<li>Output sampling and review</li>
<li>Continuous monitoring systems</li>
<li>Responsible AI governance</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="interpretability-challenges" class="slide level2">
<h2>Interpretability Challenges</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Black box nature of large language models</li>
<li>Difficulty tracing specific outputs to training data</li>
<li>Regulatory concerns in financial services:
<ul>
<li>SEC disclosure requirements</li>
<li>EU AI Act transparency provisions</li>
<li>FINRA guidance on AI explainability</li>
</ul></li>
<li>Methods for improving transparency</li>
<li>The tension between performance and explainability</li>
<li><strong>Fundamental challenges</strong>:
<ul>
<li>Distributed representations across neurons</li>
<li>Superposition of concepts in weights</li>
<li>Non-linear interactions between components</li>
<li>Emergent behaviors from collective activity</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Explainability techniques</strong>:
<ul>
<li>LIME and SHAP for financial NLP</li>
<li>Attention visualization</li>
<li>Input perturbation analysis</li>
<li>Counterfactual explanations</li>
<li>Feature attribution methods</li>
<li>Local explanation generation</li>
<li>Rule extraction approaches</li>
<li>Concept activation vectors</li>
<li>Neuron interpretation tools</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="financial-explainability-case-studies" class="slide level2">
<h2>Financial Explainability Case Studies</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>JPMorgan’s XAI Initiative</strong>:
<ul>
<li>Post-hoc explanation generation</li>
<li>Investment recommendation justification</li>
<li>Regulatory compliance documentation</li>
</ul></li>
<li><strong>BlackRock’s Aladdin Explain</strong>:
<ul>
<li>Portfolio decision attribution</li>
<li>Risk factor decomposition</li>
<li>Model confidence visualization</li>
</ul></li>
<li><strong>Citadel’s Interpretable ML</strong>:
<ul>
<li>Trading strategy explanation</li>
<li>Anomaly detection justification</li>
<li>Model monitoring dashboards</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Goldman Sachs’ Transparency Tools</strong>:
<ul>
<li>Client-facing explanation generation</li>
<li>Regulatory reporting automation</li>
<li>Model governance framework</li>
</ul></li>
<li><strong>Bank of England’s Model Transparency</strong>:
<ul>
<li>Stress testing visualization</li>
<li>Scenario analysis explanation</li>
<li>Policy impact assessment</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="alignment-with-financial-values" class="slide level2">
<h2>Alignment with Financial Values</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li>Ensuring LLMs optimize for beneficial financial outcomes</li>
<li>Safety measures for automated financial systems</li>
<li>Ethics of financial advice from LLMs</li>
<li>Preventing market manipulation</li>
<li>Balancing innovation with responsible deployment</li>
<li><strong>Value alignment techniques</strong>:
<ul>
<li>Constitutional AI approaches</li>
<li>Reinforcement learning from human feedback</li>
<li>Red-teaming for financial misuse</li>
<li>Value-sensitive design principles</li>
<li>Stakeholder-inclusive development</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Financial domain alignment</strong>:
<ul>
<li>Fiduciary responsibility principles</li>
<li>Fair market conduct guidelines</li>
<li>Consumer protection standards</li>
<li>Systemic risk considerations</li>
<li>Financial inclusion objectives</li>
<li>Sustainability and ESG integration</li>
<li>Ethical investment frameworks</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="frontier-research-directions" class="slide level2">
<h2>Frontier Research Directions</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Mechanistic interpretability</strong>:
<ul>
<li>Understanding financial concept encoding</li>
<li>Mapping causal paths in model reasoning</li>
<li>Circuit analysis for financial decisions</li>
</ul></li>
<li><strong>Neurosymbolic approaches</strong>:
<ul>
<li>Combining neural and symbolic methods</li>
<li>Explicit reasoning with LLM capabilities</li>
<li>Verifiable financial calculations</li>
</ul></li>
<li><strong>Alignment techniques</strong>:
<ul>
<li>Constitutional AI for financial services</li>
<li>Beneficial financial assistants</li>
<li>Long-term financial welfare optimization</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Multi-modal financial AI</strong>:
<ul>
<li>Market data + text understanding</li>
<li>Financial chart interpretation</li>
<li>Document + data integration</li>
</ul></li>
<li><strong>Financial reasoning enhancement</strong>:
<ul>
<li>External tools and augmentation</li>
<li>Causal reasoning improvements</li>
<li>Temporal reasoning capabilities</li>
</ul></li>
<li><strong>Responsible deployment</strong>:
<ul>
<li>Progressive disclosure frameworks</li>
<li>Capability control mechanisms</li>
<li>Impact assessment methodologies</li>
</ul></li>
</ul>
</div></div>
</section>
<section id="the-future-financial-ai-landscape" class="slide level2">
<h2>The Future Financial AI Landscape</h2>
<div class="columns">
<div class="column" style="width:60%;">
<ul>
<li><strong>Near-term developments</strong> (1-2 years):
<ul>
<li>Specialized financial fine-tuning</li>
<li>Domain-specific reasoning improvements</li>
<li>Integration with existing workflows</li>
<li>Enhanced regulatory compliance tools</li>
</ul></li>
<li><strong>Medium-term outlook</strong> (3-5 years):
<ul>
<li>Multi-modal financial reasoning</li>
<li>Autonomous financial assistants</li>
<li>Enhanced causal understanding</li>
<li>Reliable uncertainty quantification</li>
</ul></li>
</ul>
</div><div class="column" style="width:40%;">
<ul>
<li><strong>Long-term possibilities</strong> (5-10 years):
<ul>
<li>Human-level financial reasoning</li>
<li>General-purpose financial advisors</li>
<li>System-wide financial optimization</li>
<li>Algorithmic financial governance</li>
<li>Novel financial system design</li>
<li>Emergent economic behaviors</li>
</ul></li>
</ul>
</div></div>
<div class="quarto-auto-generated-content">
<p><img src="../../images/logo_header.svg" class="slide-logo"></p>
<div class="footer footer-default">

</div>
</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="index_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="index_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="index_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="index_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="index_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="index_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="index_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="index_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': true,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const isCodeAnnotation = (el) => {
        for (const clz of el.classList) {
          if (clz.startsWith('code-annotation-')) {                     
            return true;
          }
        }
        return false;
      }
      const onCopySuccess = function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      }
      const getTextToCopy = function(trigger) {
          const codeEl = trigger.previousElementSibling.cloneNode(true);
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
      }
      const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
        text: getTextToCopy
      });
      clipboard.on('success', onCopySuccess);
      if (window.document.getElementById('quarto-embedded-source-code-modal')) {
        // For code content inside modals, clipBoardJS needs to be initialized with a container option
        // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
        const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
          text: getTextToCopy,
          container: window.document.getElementById('quarto-embedded-source-code-modal')
        });
        clipboardModal.on('success', onCopySuccess);
      }
        var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
        var mailtoRegex = new RegExp(/^mailto:/);
          var filterRegex = new RegExp('/' + window.location.host + '/');
        var isInternal = (href) => {
            return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
        }
        // Inspect non-navigation links and adorn them if external
     	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
        for (var i=0; i<links.length; i++) {
          const link = links[i];
          if (!isInternal(link.href)) {
            // undo the damage that might have been done by quarto-nav.js in the case of
            // links that we want to consider external
            if (link.dataset.originalHref !== undefined) {
              link.href = link.dataset.originalHref;
            }
          }
        }
      function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
        const config = {
          allowHTML: true,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start',
        };
        if (contentFn) {
          config.content = contentFn;
        }
        if (onTriggerFn) {
          config.onTrigger = onTriggerFn;
        }
        if (onUntriggerFn) {
          config.onUntrigger = onUntriggerFn;
        }
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note) {
            return note.innerHTML;
          } else {
            return "";
          }
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>