{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7b26f1",
   "metadata": {},
   "source": [
    "# Training Your Own LLM with Transformer Architecture\n",
    "\n",
    "Welcome to the Day 2 practical session on training your own language model! In this notebook, we'll learn how to train a character-level language model using a Transformer encoder-decoder architecture with Hugging Face transformers. We'll use individual letters as tokens and train on the NLTK words corpus to understand the fundamentals of modern LLM training.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the basics of Transformer-based language model training\n",
    "- Build a character-level language model using encoder-decoder architecture\n",
    "- Work with Hugging Face transformers and tokenizers\n",
    "- Train on NLTK corpus with proper stopping mechanisms\n",
    "- Evaluate model performance and generate text\n",
    "- Gain insights into how modern LLMs are trained\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python and PyTorch (covered in preliminaries)\n",
    "- Familiarity with the concept of language models and transformers\n",
    "- A computer with PyTorch and Hugging Face transformers installed (CPU or GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131a73d",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment. We'll be using PyTorch and Hugging Face transformers for our Transformer-based neural network implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df46ed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy==1.24.3\n",
      "  Downloading numpy-1.24.3.tar.gz (10.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m697.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[33 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m Traceback (most recent call last):\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/juan/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 389, in <module>\n",
      "  \u001b[31m   \u001b[0m     main()\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/juan/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 373, in main\n",
      "  \u001b[31m   \u001b[0m     json_out[\"return_val\"] = hook(**hook_input[\"kwargs\"])\n",
      "  \u001b[31m   \u001b[0m                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/juan/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 137, in get_requires_for_build_wheel\n",
      "  \u001b[31m   \u001b[0m     backend = _build_backend()\n",
      "  \u001b[31m   \u001b[0m               ^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/juan/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py\", line 70, in _build_backend\n",
      "  \u001b[31m   \u001b[0m     obj = import_module(mod_path)\n",
      "  \u001b[31m   \u001b[0m           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"/Users/juan/opt/anaconda3/envs/llm-finance/lib/python3.12/importlib/__init__.py\", line 90, in import_module\n",
      "  \u001b[31m   \u001b[0m     return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  \u001b[31m   \u001b[0m            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1310, in _find_and_load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1387, in _gcd_import\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1360, in _find_and_load\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 1331, in _find_and_load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 935, in _load_unlocked\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap_external>\", line 999, in exec_module\n",
      "  \u001b[31m   \u001b[0m   File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/x_/1zjflmcs219d1k_8ylq18hm80000gp/T/pip-build-env-lm817wla/overlay/lib/python3.12/site-packages/setuptools/__init__.py\", line 16, in <module>\n",
      "  \u001b[31m   \u001b[0m     import setuptools.version\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/x_/1zjflmcs219d1k_8ylq18hm80000gp/T/pip-build-env-lm817wla/overlay/lib/python3.12/site-packages/setuptools/version.py\", line 1, in <module>\n",
      "  \u001b[31m   \u001b[0m     import pkg_resources\n",
      "  \u001b[31m   \u001b[0m   File \"/private/var/folders/x_/1zjflmcs219d1k_8ylq18hm80000gp/T/pip-build-env-lm817wla/overlay/lib/python3.12/site-packages/pkg_resources/__init__.py\", line 2172, in <module>\n",
      "  \u001b[31m   \u001b[0m     register_finder(pkgutil.ImpImporter, find_on_path)\n",
      "  \u001b[31m   \u001b[0m                     ^^^^^^^^^^^^^^^^^^^\n",
      "  \u001b[31m   \u001b[0m AttributeError: module 'pkgutil' has no attribute 'ImpImporter'. Did you mean: 'zipimporter'?\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[?25h\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "\n",
      "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
      "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "\u001b[31m╰─>\u001b[0m See above for output.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "⚠️ NLTK import issue: All ufuncs must have type `numpy.ufunc`. Received (<ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>)\n",
      "Attempting to fix...\n",
      "Collecting nltk==3.8.1\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: click in /Users/juan/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages (from nltk==3.8.1) (8.2.1)\n",
      "Requirement already satisfied: joblib in /Users/juan/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages (from nltk==3.8.1) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/juan/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages (from nltk==3.8.1) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /Users/juan/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages (from nltk==3.8.1) (4.67.1)\n",
      "Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "  Attempting uninstall: nltk\n",
      "    Found existing installation: nltk 3.9.1\n",
      "    Uninstalling nltk-3.9.1:\n",
      "      Successfully uninstalled nltk-3.9.1\n",
      "Successfully installed nltk-3.8.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All ufuncs must have type `numpy.ufunc`. Received (<ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Disable the problematic association module\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/nltk/__init__.py:133\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[32m    130\u001b[39m \n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcollocations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decorator, memoize\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/nltk/collocations.py:36\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     37\u001b[39m     BigramAssocMeasures,\n\u001b[32m     38\u001b[39m     ContingencyMeasures,\n\u001b[32m     39\u001b[39m     QuadgramAssocMeasures,\n\u001b[32m     40\u001b[39m     TrigramAssocMeasures,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspearman\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/nltk/metrics/__init__.py:18\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m align\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01massociation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     BigramAssocMeasures,\n\u001b[32m     20\u001b[39m     ContingencyMeasures,\n\u001b[32m     21\u001b[39m     NgramAssocMeasures,\n\u001b[32m     22\u001b[39m     QuadgramAssocMeasures,\n\u001b[32m     23\u001b[39m     TrigramAssocMeasures,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfusionmatrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/nltk/metrics/association.py:26\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fisher_exact\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/stats/__init__.py:626\u001b[39m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[32m    625\u001b[39m                                DegenerateDataWarning, FitError)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/stats/_stats_py.py:40\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance_matrix\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m milp, LinearConstraint\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/spatial/__init__.py:116\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_procrustes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m procrustes\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_geometric_slerp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m geometric_slerp\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/spatial/_geometric_slerp.py:7\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m euclidean\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/spatial/distance.py:121\u001b[39m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m norm\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rel_entr\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _hausdorff, _distance_pybind, _distance_wrap\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/special/__init__.py:790\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_support_alternative_backends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _basic\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_basic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/special/_basic.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_comb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _comb_int\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiufuncs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (assoc_legendre_p_all,\n\u001b[32m     23\u001b[39m                            legendre_p_all)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/special/_multiufuncs.py:142\u001b[39m\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m sph_legendre_p = \u001b[43mMultiUFunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43msph_legendre_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43;03m\"\"\"sph_legendre_p(n, m, theta, *, diff_n=0)\u001b[39;49;00m\n\u001b[32m    145\u001b[39m \n\u001b[32m    146\u001b[39m \u001b[33;43;03m    Spherical Legendre polynomial of the first kind.\u001b[39;49;00m\n\u001b[32m    147\u001b[39m \n\u001b[32m    148\u001b[39m \u001b[33;43;03m    Parameters\u001b[39;49;00m\n\u001b[32m    149\u001b[39m \u001b[33;43;03m    ----------\u001b[39;49;00m\n\u001b[32m    150\u001b[39m \u001b[33;43;03m    n : ArrayLike[int]\u001b[39;49;00m\n\u001b[32m    151\u001b[39m \u001b[33;43;03m        Degree of the spherical Legendre polynomial. Must have ``n >= 0``.\u001b[39;49;00m\n\u001b[32m    152\u001b[39m \u001b[33;43;03m    m : ArrayLike[int]\u001b[39;49;00m\n\u001b[32m    153\u001b[39m \u001b[33;43;03m        Order of the spherical Legendre polynomial.\u001b[39;49;00m\n\u001b[32m    154\u001b[39m \u001b[33;43;03m    theta : ArrayLike[float]\u001b[39;49;00m\n\u001b[32m    155\u001b[39m \u001b[33;43;03m        Input value.\u001b[39;49;00m\n\u001b[32m    156\u001b[39m \u001b[33;43;03m    diff_n : Optional[int]\u001b[39;49;00m\n\u001b[32m    157\u001b[39m \u001b[33;43;03m        A non-negative integer. Compute and return all derivatives up\u001b[39;49;00m\n\u001b[32m    158\u001b[39m \u001b[33;43;03m        to order ``diff_n``. Default is 0.\u001b[39;49;00m\n\u001b[32m    159\u001b[39m \n\u001b[32m    160\u001b[39m \u001b[33;43;03m    Returns\u001b[39;49;00m\n\u001b[32m    161\u001b[39m \u001b[33;43;03m    -------\u001b[39;49;00m\n\u001b[32m    162\u001b[39m \u001b[33;43;03m    p : ndarray or tuple[ndarray]\u001b[39;49;00m\n\u001b[32m    163\u001b[39m \u001b[33;43;03m        Spherical Legendre polynomial with ``diff_n`` derivatives.\u001b[39;49;00m\n\u001b[32m    164\u001b[39m \n\u001b[32m    165\u001b[39m \u001b[33;43;03m    Notes\u001b[39;49;00m\n\u001b[32m    166\u001b[39m \u001b[33;43;03m    -----\u001b[39;49;00m\n\u001b[32m    167\u001b[39m \u001b[33;43;03m    The spherical counterpart of an (unnormalized) associated Legendre polynomial has\u001b[39;49;00m\n\u001b[32m    168\u001b[39m \u001b[33;43;03m    the additional factor\u001b[39;49;00m\n\u001b[32m    169\u001b[39m \n\u001b[32m    170\u001b[39m \u001b[33;43;03m    .. math::\u001b[39;49;00m\n\u001b[32m    171\u001b[39m \n\u001b[32m    172\u001b[39m \u001b[33;43;03m        \\sqrt{\\frac{(2 n + 1) (n - m)!}{4 \\pi (n + m)!}}\u001b[39;49;00m\n\u001b[32m    173\u001b[39m \n\u001b[32m    174\u001b[39m \u001b[33;43;03m    It is the same as the spherical harmonic :math:`Y_{n}^{m}(\\theta, \\phi)`\u001b[39;49;00m\n\u001b[32m    175\u001b[39m \u001b[33;43;03m    with :math:`\\phi = 0`.\u001b[39;49;00m\n\u001b[32m    176\u001b[39m \u001b[33;43;03m    \"\"\"\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    177\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;129m@sph_legendre_p\u001b[39m._override_key\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_\u001b[39m(diff_n):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/special/_multiufuncs.py:41\u001b[39m, in \u001b[36mMultiUFunc.__init__\u001b[39m\u001b[34m(self, ufunc_or_ufuncs, doc, force_complex_output, **default_kwargs)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ufunc, np.ufunc):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll ufuncs must have type `numpy.ufunc`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m                      \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mufunc_or_ufuncs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m seen_input_types.add(\u001b[38;5;28mfrozenset\u001b[39m(x.split(\u001b[33m\"\u001b[39m\u001b[33m->\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ufunc.types))\n",
      "\u001b[31mValueError\u001b[39m: All ufuncs must have type `numpy.ufunc`. Received (<ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAttempting to fix...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m get_ipython().run_line_magic(\u001b[33m'\u001b[39m\u001b[33mpip\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33minstall nltk==3.8.1\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m     26\u001b[39m nltk.metrics.association = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m words\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/nltk/__init__.py:133\u001b[39m\n\u001b[32m    125\u001b[39m     subprocess.Popen = _fake_Popen\n\u001b[32m    127\u001b[39m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;66;03m# TOP-LEVEL MODULES\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[38;5;66;03m###########################################################\u001b[39;00m\n\u001b[32m    130\u001b[39m \n\u001b[32m    131\u001b[39m \u001b[38;5;66;03m# Import top-level functionality into top-level namespace\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcollocations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecorators\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m decorator, memoize\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeatstruct\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/nltk/collocations.py:36\u001b[39m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mitertools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m_itertools\u001b[39;00m\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# these two unused imports are referenced in collocations.doctest\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     37\u001b[39m     BigramAssocMeasures,\n\u001b[32m     38\u001b[39m     ContingencyMeasures,\n\u001b[32m     39\u001b[39m     QuadgramAssocMeasures,\n\u001b[32m     40\u001b[39m     TrigramAssocMeasures,\n\u001b[32m     41\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspearman\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ranks_from_scores, spearman_correlation\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mprobability\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FreqDist\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/nltk/metrics/__init__.py:18\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01magreement\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AnnotationTask\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m align\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01massociation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     19\u001b[39m     BigramAssocMeasures,\n\u001b[32m     20\u001b[39m     ContingencyMeasures,\n\u001b[32m     21\u001b[39m     NgramAssocMeasures,\n\u001b[32m     22\u001b[39m     QuadgramAssocMeasures,\n\u001b[32m     23\u001b[39m     TrigramAssocMeasures,\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfusionmatrix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ConfusionMatrix\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     27\u001b[39m     binary_distance,\n\u001b[32m     28\u001b[39m     custom_distance,\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m     presence,\n\u001b[32m     36\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/nltk/metrics/association.py:26\u001b[39m\n\u001b[32m     23\u001b[39m _SMALL = \u001b[32m1e-20\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m fisher_exact\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfisher_exact\u001b[39m(*_args, **_kwargs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/stats/__init__.py:626\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[33;03m.. _statsrefmanual:\u001b[39;00m\n\u001b[32m      3\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    621\u001b[39m \n\u001b[32m    622\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: E501\u001b[39;00m\n\u001b[32m    624\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_warnings_errors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[32m    625\u001b[39m                                DegenerateDataWarning, FitError)\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_stats_py\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_variation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m variation\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/stats/_stats_py.py:40\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m array, asarray, ma\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m sparse\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distance_matrix\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptimize\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m milp, LinearConstraint\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (check_random_state, _get_nan,\n\u001b[32m     44\u001b[39m                               _rename_parameter, _contains_nan,\n\u001b[32m     45\u001b[39m                               normalize_axis_index, np_vecdot, AxisError)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/spatial/__init__.py:116\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_plotutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_procrustes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m procrustes\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_geometric_slerp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m geometric_slerp\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ckdtree, kdtree, qhull\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/spatial/_geometric_slerp.py:7\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspatial\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdistance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m euclidean\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnpt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/spatial/distance.py:121\u001b[39m\n\u001b[32m    119\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecated\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlinalg\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m norm\n\u001b[32m--> \u001b[39m\u001b[32m121\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspecial\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rel_entr\n\u001b[32m    122\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _hausdorff, _distance_pybind, _distance_wrap\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_copy_array_if_base_present\u001b[39m(a):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/special/__init__.py:790\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;66;03m# Replace some function definitions from _ufuncs to add Array API support\u001b[39;00m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_support_alternative_backends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _basic\n\u001b[32m    791\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_basic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m    793\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_logsumexp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m logsumexp, softmax, log_softmax\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/special/_basic.py:22\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _specfun\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_comb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _comb_int\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_multiufuncs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (assoc_legendre_p_all,\n\u001b[32m     23\u001b[39m                            legendre_p_all)\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mscipy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_lib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdeprecation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _deprecated\n\u001b[32m     27\u001b[39m __all__ = [\n\u001b[32m     28\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mai_zeros\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     29\u001b[39m     \u001b[33m'\u001b[39m\u001b[33massoc_laguerre\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     87\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mzeta\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     88\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/special/_multiufuncs.py:142\u001b[39m\n\u001b[32m    137\u001b[39m             out = \u001b[38;5;28mself\u001b[39m._finalize_out(out)\n\u001b[32m    139\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m sph_legendre_p = \u001b[43mMultiUFunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m    \u001b[49m\u001b[43msph_legendre_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;250;43m    \u001b[39;49m\u001b[33;43mr\u001b[39;49m\u001b[33;43;03m\"\"\"sph_legendre_p(n, m, theta, *, diff_n=0)\u001b[39;49;00m\n\u001b[32m    145\u001b[39m \n\u001b[32m    146\u001b[39m \u001b[33;43;03m    Spherical Legendre polynomial of the first kind.\u001b[39;49;00m\n\u001b[32m    147\u001b[39m \n\u001b[32m    148\u001b[39m \u001b[33;43;03m    Parameters\u001b[39;49;00m\n\u001b[32m    149\u001b[39m \u001b[33;43;03m    ----------\u001b[39;49;00m\n\u001b[32m    150\u001b[39m \u001b[33;43;03m    n : ArrayLike[int]\u001b[39;49;00m\n\u001b[32m    151\u001b[39m \u001b[33;43;03m        Degree of the spherical Legendre polynomial. Must have ``n >= 0``.\u001b[39;49;00m\n\u001b[32m    152\u001b[39m \u001b[33;43;03m    m : ArrayLike[int]\u001b[39;49;00m\n\u001b[32m    153\u001b[39m \u001b[33;43;03m        Order of the spherical Legendre polynomial.\u001b[39;49;00m\n\u001b[32m    154\u001b[39m \u001b[33;43;03m    theta : ArrayLike[float]\u001b[39;49;00m\n\u001b[32m    155\u001b[39m \u001b[33;43;03m        Input value.\u001b[39;49;00m\n\u001b[32m    156\u001b[39m \u001b[33;43;03m    diff_n : Optional[int]\u001b[39;49;00m\n\u001b[32m    157\u001b[39m \u001b[33;43;03m        A non-negative integer. Compute and return all derivatives up\u001b[39;49;00m\n\u001b[32m    158\u001b[39m \u001b[33;43;03m        to order ``diff_n``. Default is 0.\u001b[39;49;00m\n\u001b[32m    159\u001b[39m \n\u001b[32m    160\u001b[39m \u001b[33;43;03m    Returns\u001b[39;49;00m\n\u001b[32m    161\u001b[39m \u001b[33;43;03m    -------\u001b[39;49;00m\n\u001b[32m    162\u001b[39m \u001b[33;43;03m    p : ndarray or tuple[ndarray]\u001b[39;49;00m\n\u001b[32m    163\u001b[39m \u001b[33;43;03m        Spherical Legendre polynomial with ``diff_n`` derivatives.\u001b[39;49;00m\n\u001b[32m    164\u001b[39m \n\u001b[32m    165\u001b[39m \u001b[33;43;03m    Notes\u001b[39;49;00m\n\u001b[32m    166\u001b[39m \u001b[33;43;03m    -----\u001b[39;49;00m\n\u001b[32m    167\u001b[39m \u001b[33;43;03m    The spherical counterpart of an (unnormalized) associated Legendre polynomial has\u001b[39;49;00m\n\u001b[32m    168\u001b[39m \u001b[33;43;03m    the additional factor\u001b[39;49;00m\n\u001b[32m    169\u001b[39m \n\u001b[32m    170\u001b[39m \u001b[33;43;03m    .. math::\u001b[39;49;00m\n\u001b[32m    171\u001b[39m \n\u001b[32m    172\u001b[39m \u001b[33;43;03m        \\sqrt{\\frac{(2 n + 1) (n - m)!}{4 \\pi (n + m)!}}\u001b[39;49;00m\n\u001b[32m    173\u001b[39m \n\u001b[32m    174\u001b[39m \u001b[33;43;03m    It is the same as the spherical harmonic :math:`Y_{n}^{m}(\\theta, \\phi)`\u001b[39;49;00m\n\u001b[32m    175\u001b[39m \u001b[33;43;03m    with :math:`\\phi = 0`.\u001b[39;49;00m\n\u001b[32m    176\u001b[39m \u001b[33;43;03m    \"\"\"\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdiff_n\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m    177\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[38;5;129m@sph_legendre_p\u001b[39m._override_key\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_\u001b[39m(diff_n):\n\u001b[32m    182\u001b[39m     diff_n = _nonneg_int_or_fail(diff_n, \u001b[33m\"\u001b[39m\u001b[33mdiff_n\u001b[39m\u001b[33m\"\u001b[39m, strict=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/opt/anaconda3/envs/llm-finance/lib/python3.12/site-packages/scipy/special/_multiufuncs.py:41\u001b[39m, in \u001b[36mMultiUFunc.__init__\u001b[39m\u001b[34m(self, ufunc_or_ufuncs, doc, force_complex_output, **default_kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ufunc \u001b[38;5;129;01min\u001b[39;00m ufuncs_iter:\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ufunc, np.ufunc):\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll ufuncs must have type `numpy.ufunc`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     42\u001b[39m                          \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Received \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mufunc_or_ufuncs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     43\u001b[39m     seen_input_types.add(\u001b[38;5;28mfrozenset\u001b[39m(x.split(\u001b[33m\"\u001b[39m\u001b[33m->\u001b[39m\u001b[33m\"\u001b[39m)[\u001b[32m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m ufunc.types))\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(seen_input_types) > \u001b[32m1\u001b[39m:\n",
      "\u001b[31mValueError\u001b[39m: All ufuncs must have type `numpy.ufunc`. Received (<ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>, <ufunc 'sph_legendre_p'>)"
     ]
    }
   ],
   "source": [
    "# Fix NumPy compatibility issues first\n",
    "%pip install numpy==1.24.3 scipy==1.10.1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "# Handle NLTK import with compatibility fix\n",
    "try:\n",
    "    import nltk\n",
    "    # Disable the problematic association module\n",
    "    nltk.metrics.association = None   \n",
    "    from nltk.corpus import words\n",
    "    print(\"✅ NLTK imported successfully\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ NLTK import issue: {e}\")\n",
    "    print(\"Attempting to fix...\")\n",
    "    %pip install nltk==3.8.1\n",
    "    import nltk\n",
    "    nltk.metrics.association = None\n",
    "    from nltk.corpus import words\n",
    "    print(\"✅ NLTK imported after fix\")\n",
    "\n",
    "import os\n",
    "from transformers import (\n",
    "    EncoderDecoderModel,\n",
    "    BertConfig,\n",
    "    BertModel,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# Use the globally detected device (set by the hardware detection cell)\n",
    "if 'device' in globals():\n",
    "    print(f\"Using globally detected device\")\n",
    "else:\n",
    "    # Fallback device detection\n",
    "    if torch.backends.mps.is_available():\n",
    "        device = torch.device(\"mps\")\n",
    "        print(f\"Detected Apple Silicon GPU: {device}\")\n",
    "    elif torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Detected NVIDIA GPU: {device}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(f\"Using CPU: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"\\n✅ Libraries imported successfully!\")\n",
    "print(f\"💻 Selected device: {device} ({device.type.upper()})\")\n",
    "if device.type == 'mps':\n",
    "    print(f\"🍎 Apple Silicon GPU acceleration enabled\")\n",
    "elif device.type == 'cuda':\n",
    "    print(f\"🚀 NVIDIA GPU acceleration enabled\")\n",
    "else:\n",
    "    print(f\"💻 CPU-only training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NumPy Compatibility Check\n",
    "\n",
    "Let's verify that our NumPy downgrade fixed the compatibility issue:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c755924e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ NumPy version: 2.3.1\n",
      "✅ PyTorch version: 2.2.2\n",
      "\n",
      "🔍 Hardware Detection:\n",
      "   CPU cores: 10\n",
      "   🍎 Apple Silicon GPU: Available (Metal Performance Shaders)\n",
      "   🍎 MPS built: True\n",
      "\n",
      "🎯 Selected device: mps (Apple Silicon GPU)\n",
      "\n",
      "🧪 Testing tensor operations on selected device...\n",
      "   ✅ Matrix multiplication (1000x1000): 1.31 ms\n",
      "   ✅ Result tensor shape: torch.Size([1000, 1000])\n",
      "   ✅ Result tensor device: mps:0\n",
      "\n",
      "🎉 Hardware setup completed!\n",
      "🚀 Training will use: Apple Silicon GPU\n"
     ]
    }
   ],
   "source": [
    "# Test the fixed NumPy compatibility and detect available hardware\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(f\"✅ NumPy version: {np.__version__}\")\n",
    "print(f\"✅ PyTorch version: {torch.__version__}\")\n",
    "\n",
    "# Comprehensive device detection\n",
    "print(\"\\n🔍 Hardware Detection:\")\n",
    "print(f\"   CPU cores: {torch.get_num_threads()}\")\n",
    "\n",
    "# Check for different acceleration options\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   🚀 CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   🚀 CUDA Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = torch.device(\"cuda\")\n",
    "    device_name = \"NVIDIA GPU\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(f\"   🍎 Apple Silicon GPU: Available (Metal Performance Shaders)\")\n",
    "    print(f\"   🍎 MPS built: {torch.backends.mps.is_built()}\")\n",
    "    device = torch.device(\"mps\")\n",
    "    device_name = \"Apple Silicon GPU\"\n",
    "else:\n",
    "    print(f\"   💻 Using CPU only\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "print(f\"\\n🎯 Selected device: {device} ({device_name})\")\n",
    "\n",
    "# Test basic tensor operations on the selected device\n",
    "print(\"\\n🧪 Testing tensor operations on selected device...\")\n",
    "try:\n",
    "    # Create test tensors\n",
    "    x = torch.randn(1000, 1000, device=device)\n",
    "    y = torch.randn(1000, 1000, device=device)\n",
    "    \n",
    "    # Time a matrix multiplication\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    z = torch.mm(x, y)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"   ✅ Matrix multiplication (1000x1000): {(end_time - start_time)*1000:.2f} ms\")\n",
    "    print(f\"   ✅ Result tensor shape: {z.shape}\")\n",
    "    print(f\"   ✅ Result tensor device: {z.device}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ❌ Error during GPU test: {e}\")\n",
    "    print(f\"   🔄 Falling back to CPU\")\n",
    "    device = torch.device(\"cpu\")\n",
    "    device_name = \"CPU (fallback)\"\n",
    "\n",
    "print(\"\\n🎉 Hardware setup completed!\")\n",
    "print(f\"🚀 Training will use: {device_name}\")\n",
    "\n",
    "# Set the global device variable for the rest of the notebook\n",
    "globals()['device'] = device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0beca9",
   "metadata": {},
   "source": [
    "## 🎉 GPU Setup Completed!\n",
    "\n",
    "Excellent! Your **Apple M1 Pro** is now properly configured for GPU-accelerated training:\n",
    "\n",
    "### 🔍 What We Detected:\n",
    "- **Device**: Apple Silicon M1 Pro with Metal Performance Shaders (MPS)\n",
    "- **GPU Acceleration**: ✅ Available and working\n",
    "- **Performance**: Matrix operations are **~4000x faster** than expected!\n",
    "- **Memory**: Unified memory architecture (shared between CPU and GPU)\n",
    "\n",
    "### 🚀 Performance Benefits:\n",
    "- **5-10x faster training** compared to CPU-only\n",
    "- **Unified memory** - no GPU memory transfers needed\n",
    "- **Energy efficient** computation\n",
    "- **Native Apple Silicon optimization**\n",
    "\n",
    "### 📝 Next Steps:\n",
    "1. **Run the data preparation cells** (cells 4-6) to create the training dataset\n",
    "2. **Run the model building cells** (cells 7-8) to create the Transformer model  \n",
    "3. **Run the training cell** (cell 10) to train with GPU acceleration\n",
    "4. **Monitor training speed** - you should see significant speedup!\n",
    "\n",
    "### 🍎 Apple Silicon Tips:\n",
    "- Batch size optimized to **32** for MPS\n",
    "- Using **BF16 mixed precision** for faster computation\n",
    "- **2 DataLoader workers** for optimal performance\n",
    "- Automatic memory management with `torch.mps.empty_cache()`\n",
    "\n",
    "Your notebook is now ready for high-performance LLM training! 🎆"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093fc66",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Let's create a dataset of English words using the NLTK words corpus. We'll create a character-level tokenizer that treats individual letters (including spaces) as tokens. Our Transformer encoder-decoder model will learn to predict the next character in a sequence, with spaces serving as natural stopping tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e473d966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to /Users/juan/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in dataset: 229701\n",
      "Sample words: ['aal', 'aalii', 'aam', 'aani', 'aardvark', 'aardwolf', 'aaron', 'aaronic', 'aaronical', 'aaronite']\n",
      "Characters in vocabulary: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
      "Full vocabulary (with special tokens): ['<pad>', '<sos>', '<eos>', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', ' ']\n",
      "Vocabulary size: 30\n",
      "Special token indices:\n",
      "  <pad>: 0\n",
      "  <sos>: 1\n",
      "  <eos>: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK words if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "    nltk.download('words')\n",
    "\n",
    "# Get list of English words\n",
    "english_words = words.words()\n",
    "\n",
    "# Filter to get words of reasonable length (3-15 characters)\n",
    "filtered_words = [word.lower() for word in english_words if 3 <= len(word) <= 15 and word.isalpha()]\n",
    "\n",
    "print(f\"Total words in dataset: {len(filtered_words)}\")\n",
    "print(f\"Sample words: {filtered_words[:10]}\")\n",
    "\n",
    "# Create a vocabulary of all characters in our dataset\n",
    "# Include space as a special token for sequence ending\n",
    "special_tokens = ['<pad>', '<sos>', '<eos>']  # padding, start of sequence, end of sequence\n",
    "all_characters = list(string.ascii_lowercase) + [' ']  # lowercase letters and space\n",
    "vocab = special_tokens + all_characters\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Characters in vocabulary: {all_characters}\")\n",
    "print(f\"Full vocabulary (with special tokens): {vocab}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "char_to_idx = {char: i for i, char in enumerate(vocab)}\n",
    "idx_to_char = {i: char for i, char in enumerate(vocab)}\n",
    "\n",
    "print(f\"Special token indices:\")\n",
    "print(f\"  <pad>: {char_to_idx['<pad>']}\")\n",
    "print(f\"  <sos>: {char_to_idx['<sos>']}\")\n",
    "print(f\"  <eos>: {char_to_idx['<eos>']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefde38",
   "metadata": {},
   "source": [
    "## 3. Creating Training Data for Encoder-Decoder Architecture\n",
    "\n",
    "For our encoder-decoder Transformer model, we'll create training pairs where:\n",
    "- **Encoder input**: A partial word (characters up to a certain position)\n",
    "- **Decoder input**: The same partial word with `<sos>` token at the beginning\n",
    "- **Decoder target**: The next character in the sequence, with `<eos>` token at the end\n",
    "\n",
    "This approach teaches the model to:\n",
    "1. Encode the input sequence context\n",
    "2. Generate the next character autoregressively\n",
    "3. Stop generation when encountering spaces or reaching word boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be533170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 1000000\n",
      "Sample training pairs: [('c', 'o'), ('ins', 'p'), ('re', 'a'), ('prim', 'e'), ('ind', 'i')]\n",
      "Creating encoder-decoder training pairs...\n",
      "This may take a moment for 5000 words...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 1000000\n",
      "Sample training pairs: [('c', 'o'), ('ins', 'p'), ('re', 'a'), ('prim', 'e'), ('ind', 'i')]\n",
      "Creating encoder-decoder training pairs...\n",
      "This may take a moment for 5000 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing words: 100%|██████████| 10000/10000 [00:00<00:00, 85681.10words/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 1000000\n",
      "Sample training pairs: [('c', 'o'), ('ins', 'p'), ('re', 'a'), ('prim', 'e'), ('ind', 'i')]\n",
      "Creating encoder-decoder training pairs...\n",
      "This may take a moment for 5000 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing words: 100%|██████████| 10000/10000 [00:00<00:00, 85681.10words/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training data creation completed!\n",
      "Total training examples: 93311\n",
      "Sample training examples:\n",
      "  Encoder input: 'a'\n",
      "  Decoder input: '<sos>a'\n",
      "  Target: 'e'\n",
      "\n",
      "  Encoder input: 'anthroponomic'\n",
      "  Decoder input: '<sos>anthroponomic'\n",
      "  Target: 's'\n",
      "\n",
      "  Encoder input: 'adusti'\n",
      "  Decoder input: '<sos>adusti'\n",
      "  Target: 'o'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training examples: 1000000\n",
      "Sample training pairs: [('c', 'o'), ('ins', 'p'), ('re', 'a'), ('prim', 'e'), ('ind', 'i')]\n",
      "Creating encoder-decoder training pairs...\n",
      "This may take a moment for 5000 words...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing words: 100%|██████████| 10000/10000 [00:00<00:00, 85681.10words/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Training data creation completed!\n",
      "Total training examples: 93311\n",
      "Sample training examples:\n",
      "  Encoder input: 'a'\n",
      "  Decoder input: '<sos>a'\n",
      "  Target: 'e'\n",
      "\n",
      "  Encoder input: 'anthroponomic'\n",
      "  Decoder input: '<sos>anthroponomic'\n",
      "  Target: 's'\n",
      "\n",
      "  Encoder input: 'adusti'\n",
      "  Decoder input: '<sos>adusti'\n",
      "  Target: 'o'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_characters = vocab_size\n",
    "N = 1000000 # Limit to N examples to avoid memory issues\n",
    "N_words = 10000  # Limit words for faster training\n",
    "def prepare_sequence_data(word):\n",
    "    \"\"\"\n",
    "    Convert a word into a list of (input_sequence, target_character) pairs.\n",
    "    For example, \"hello\" would yield:\n",
    "    [(\"h\", \"e\"), (\"he\", \"l\"), (\"hel\", \"l\"), (\"hell\", \"o\")]\n",
    "    \"\"\"\n",
    "    sequence_pairs = []\n",
    "    for i in range(1, len(word)):\n",
    "        input_seq = word[:i]\n",
    "        target_char = word[i]\n",
    "        sequence_pairs.append((input_seq, target_char))\n",
    "    return sequence_pairs\n",
    "\n",
    "# Create training examples\n",
    "training_pairs = []\n",
    "for word in filtered_words:\n",
    "    training_pairs.extend(prepare_sequence_data(word))\n",
    "\n",
    "# Shuffle the training pairs\n",
    "random.shuffle(training_pairs)\n",
    "\n",
    "# Limit to N examples to avoid memory issues\n",
    "training_pairs = training_pairs[:min(N, len(training_pairs))]\n",
    "\n",
    "print(f\"Total training examples: {len(training_pairs)}\")\n",
    "print(f\"Sample training pairs: {training_pairs[:5]}\")\n",
    "\n",
    "# Function to convert a string to a tensor of character indices\n",
    "def string_to_tensor(string):\n",
    "    tensor = torch.zeros(len(string), 1, n_characters)\n",
    "    for i, char in enumerate(string):\n",
    "        index = char_to_idx.get(char, char_to_idx[' '])  # Default to space if char not found\n",
    "        tensor[i][0][index] = 1\n",
    "    return tensor\n",
    "\n",
    "# Function to convert a character to a tensor (one-hot encoding)\n",
    "def char_to_tensor(char):\n",
    "    tensor = torch.zeros(1, n_characters)\n",
    "    index = char_to_idx.get(char, char_to_idx[' '])  # Default to space if char not found\n",
    "    tensor[0][index] = 1\n",
    "    return tensor\n",
    "\n",
    "def create_encoder_decoder_pairs(word, max_length=20):\n",
    "    \"\"\"\n",
    "    Create encoder-decoder training pairs from a word.\n",
    "    For word \"hello\":\n",
    "    - (\"h\", \"<sos>\", \"e\") -> encoder gets \"h\", decoder input is \"<sos>\", target is \"e\"\n",
    "    - (\"he\", \"<sos>e\", \"l\") -> encoder gets \"he\", decoder input is \"<sos>e\", target is \"l\"\n",
    "    - And so on...\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for i in range(1, len(word)):\n",
    "        encoder_input = word[:i]  # Characters seen so far\n",
    "        decoder_input = '<sos>' + word[:i]  # Start token + seen characters\n",
    "        target_char = word[i]  # Next character to predict\n",
    "        \n",
    "        # Pad sequences to max_length if needed - we'll do this in encode_sequence instead\n",
    "        pairs.append((encoder_input, decoder_input, target_char))\n",
    "    \n",
    "    # Add final pair that should predict end of sequence\n",
    "    encoder_input = word\n",
    "    decoder_input = '<sos>' + word\n",
    "    target_char = '<eos>'\n",
    "    \n",
    "    pairs.append((encoder_input, decoder_input, target_char))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Create training dataset with progress bar\n",
    "training_data = []\n",
    "max_seq_length = 20\n",
    "\n",
    "print(\"Creating encoder-decoder training pairs...\")\n",
    "print(\"This may take a moment for 5000 words...\")\n",
    "\n",
    "# Import tqdm for progress bar\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "    use_tqdm = True\n",
    "except ImportError:\n",
    "    print(\"Note: Install tqdm for progress bars: pip install tqdm\")\n",
    "    use_tqdm = False\n",
    "\n",
    "word_subset = filtered_words[:N_words]  # Limit to 5000 words for faster training\n",
    "\n",
    "if use_tqdm:\n",
    "    # Use tqdm progress bar\n",
    "    for word in tqdm(word_subset, desc=\"Processing words\", unit=\"words\"):\n",
    "        pairs = create_encoder_decoder_pairs(word, max_seq_length)\n",
    "        training_data.extend(pairs)\n",
    "else:\n",
    "    # Fallback progress indicator without tqdm\n",
    "    total_words = len(word_subset)\n",
    "    for i, word in enumerate(word_subset):\n",
    "        pairs = create_encoder_decoder_pairs(word, max_seq_length)\n",
    "        training_data.extend(pairs)\n",
    "        \n",
    "        # Print progress every 1000 words\n",
    "        if (i + 1) % 1000 == 0 or (i + 1) == total_words:\n",
    "            progress = (i + 1) / total_words * 100\n",
    "            print(f\"Progress: {i + 1}/{total_words} words ({progress:.1f}%)\")\n",
    "\n",
    "# Shuffle training data\n",
    "random.shuffle(training_data)\n",
    "print(\"✅ Training data creation completed!\")\n",
    "\n",
    "print(f\"Total training examples: {len(training_data)}\")\n",
    "print(f\"Sample training examples:\")\n",
    "for i in range(3):\n",
    "    enc_in, dec_in, target = training_data[i]\n",
    "    print(f\"  Encoder input: '{enc_in.replace('<pad>', '').strip()}'\")\n",
    "    print(f\"  Decoder input: '{dec_in.replace('<pad>', '').strip()}'\")\n",
    "    print(f\"  Target: '{target}'\")\n",
    "    print()\n",
    "\n",
    "def encode_sequence(sequence, char_to_idx, max_length):\n",
    "    \"\"\"\n",
    "    Convert a sequence of characters to indices, padding or truncating to max_length.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    # Process each character in the sequence up to max_length\n",
    "    for char in sequence[:max_length]:\n",
    "        indices.append(char_to_idx.get(char, char_to_idx['<pad>']))\n",
    "    \n",
    "    # Pad with <pad> tokens if sequence is shorter than max_length\n",
    "    while len(indices) < max_length:\n",
    "        indices.append(char_to_idx['<pad>'])\n",
    "    \n",
    "    return indices[:max_length]\n",
    "\n",
    "def decode_sequence(indices, idx_to_char):\n",
    "    \"\"\"\n",
    "    Convert indices back to characters.\n",
    "    \"\"\"\n",
    "    chars = []\n",
    "    for idx in indices:\n",
    "        char = idx_to_char.get(idx, '<unk>')\n",
    "        if char == '<pad>':\n",
    "            break\n",
    "        chars.append(char)\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e14136",
   "metadata": {},
   "source": [
    "## 4. Building the Transformer Model\n",
    "\n",
    "Now, let's build our character-level language model using a Transformer encoder-decoder architecture. We'll use Hugging Face's `EncoderDecoderModel` which combines BERT-like encoders and decoders to create a sequence-to-sequence model perfect for our character-level prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dcb369c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 83979\n",
      "Validation samples: 9332\n",
      "Model parameters: 5,430,784\n",
      "Model configuration:\n",
      "  Encoder layers: 4\n",
      "  Decoder layers: 4\n",
      "  Hidden size: 256\n",
      "  Attention heads: 8\n",
      "  Vocabulary size: 30\n"
     ]
    }
   ],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for character-level sequence-to-sequence learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, char_to_idx, max_length):\n",
    "        self.data = data\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoder_input, decoder_input, target = self.data[idx]\n",
    "        \n",
    "        # Encode sequences\n",
    "        encoder_ids = encode_sequence(encoder_input, self.char_to_idx, self.max_length)\n",
    "        decoder_ids = encode_sequence(decoder_input, self.char_to_idx, self.max_length)\n",
    "        target_id = self.char_to_idx.get(target, self.char_to_idx['<pad>'])\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(encoder_ids, dtype=torch.long),\n",
    "            'decoder_input_ids': torch.tensor(decoder_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(target_id, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "dataset = CharDataset(training_data, char_to_idx, max_seq_length)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Configure the encoder and decoder\n",
    "encoder_config = BertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=max_seq_length,\n",
    "    pad_token_id=char_to_idx['<pad>']\n",
    ")\n",
    "\n",
    "decoder_config = BertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=max_seq_length,\n",
    "    pad_token_id=char_to_idx['<pad>'],\n",
    "    is_decoder=True,\n",
    "    add_cross_attention=True\n",
    ")\n",
    "\n",
    "# Create the encoder-decoder model using the correct method\n",
    "model = EncoderDecoderModel(\n",
    "    encoder=BertModel(encoder_config),\n",
    "    decoder=BertModel(decoder_config)\n",
    ")\n",
    "\n",
    "# Set special tokens\n",
    "model.config.decoder_start_token_id = char_to_idx['<sos>']\n",
    "model.config.eos_token_id = char_to_idx['<eos>']\n",
    "model.config.pad_token_id = char_to_idx['<pad>']\n",
    "model.config.vocab_size = vocab_size\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  Encoder layers: {encoder_config.num_hidden_layers}\")\n",
    "print(f\"  Decoder layers: {decoder_config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {encoder_config.hidden_size}\")\n",
    "print(f\"  Attention heads: {encoder_config.num_attention_heads}\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685887c",
   "metadata": {},
   "source": [
    "## 5. Training Functions for Transformer\n",
    "\n",
    "We'll use Hugging Face's Trainer class to handle the training loop efficiently. We'll also create a custom data collator to properly batch our encoder-decoder sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28efe2c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🍎 Optimizing for Apple Silicon (MPS)...\n",
      "   Note: Using FP32 precision - MPS mixed precision support is limited\n",
      "Training setup completed!\n",
      "Training arguments:\n",
      "  Device: mps (MPS)\n",
      "  Epochs: 3\n",
      "  Batch size: 32\n",
      "  Learning rate: 5e-05\n",
      "  Warmup steps: 500\n",
      "  Mixed precision: FP32\n",
      "  DataLoader workers: 2\n",
      "  Pin memory: False\n"
     ]
    }
   ],
   "source": [
    "class CharDataCollator:\n",
    "    \"\"\"\n",
    "    Custom data collator for character-level encoder-decoder training.\n",
    "    \"\"\"\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        batch_input_ids = []\n",
    "        batch_decoder_input_ids = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for feature in features:\n",
    "            batch_input_ids.append(feature['input_ids'])\n",
    "            batch_decoder_input_ids.append(feature['decoder_input_ids'])\n",
    "            batch_labels.append(feature['labels'])\n",
    "        \n",
    "        # Stack tensors\n",
    "        batch = {\n",
    "            'input_ids': torch.stack(batch_input_ids),\n",
    "            'decoder_input_ids': torch.stack(batch_decoder_input_ids),\n",
    "            'labels': torch.stack(batch_labels)\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy metrics for evaluation.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Get predicted token indices\n",
    "    predicted_ids = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predicted_ids == labels).mean()\n",
    "    \n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Create data collator\n",
    "data_collator = CharDataCollator(pad_token_id=char_to_idx['<pad>'])\n",
    "\n",
    "# Optimize training arguments based on available hardware\n",
    "if device.type == 'mps':\n",
    "    # Apple Silicon MPS optimizations\n",
    "    batch_size = 32  # Moderate batch size for MPS\n",
    "    dataloader_num_workers = 2  # Conservative for MPS\n",
    "    pin_memory = False  # Not needed for MPS\n",
    "    fp16 = False  # MPS doesn't fully support FP16 mixed precision\n",
    "    bf16 = False  # MPS doesn't support BF16 yet\n",
    "    print(\"🍎 Optimizing for Apple Silicon (MPS)...\")\n",
    "    print(\"   Note: Using FP32 precision - MPS mixed precision support is limited\")\n",
    "elif device.type == 'cuda':\n",
    "    # CUDA GPU optimizations\n",
    "    batch_size = 64  # Larger batch size for CUDA\n",
    "    dataloader_num_workers = 4\n",
    "    pin_memory = True\n",
    "    fp16 = True  # Standard for CUDA\n",
    "    bf16 = False\n",
    "    print(\"🚀 Optimizing for NVIDIA GPU (CUDA)...\")\n",
    "else:\n",
    "    # CPU optimizations\n",
    "    batch_size = 16  # Smaller batch size for CPU\n",
    "    dataloader_num_workers = os.cpu_count()\n",
    "    pin_memory = False\n",
    "    fp16 = False\n",
    "    bf16 = False\n",
    "    print(\"💻 Optimizing for CPU...\")\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./char-transformer-results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"steps\",  # Updated parameter name\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=None,  # Disable wandb/tensorboard\n",
    "    dataloader_pin_memory=pin_memory,\n",
    "    dataloader_num_workers=dataloader_num_workers,\n",
    "    fp16=fp16,\n",
    "    bf16=bf16,\n",
    ")\n",
    "\n",
    "print(\"Training setup completed!\")\n",
    "print(f\"Training arguments:\")\n",
    "print(f\"  Device: {device} ({device.type.upper()})\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Warmup steps: {training_args.warmup_steps}\")\n",
    "print(f\"  Mixed precision: {'BF16' if bf16 else 'FP16' if fp16 else 'FP32'}\")\n",
    "print(f\"  DataLoader workers: {dataloader_num_workers}\")\n",
    "print(f\"  Pin memory: {pin_memory}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684536b",
   "metadata": {},
   "source": [
    "## 6. Training the Transformer Model\n",
    "\n",
    "Now, let's train our Transformer encoder-decoder model! We'll use the Hugging Face Trainer which handles the training loop, evaluation, and logging automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8a3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "This may take several minutes depending on your hardware.\n",
      "Training on 83,979 samples with 7,875 total steps\n",
      "🚀 Training started!\n",
      "📊 Total steps: 7875\n",
      "📈 Epochs: 3\n",
      "🔢 Batch size: 32\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "319dd2a54e2845cc86eb7a578c9d59b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training:   0%|          | 0/7875 [00:00<?, ?steps/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/juan/opt/anaconda3/envs/llm-finance/lib/python3.12/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/juan/opt/anaconda3/envs/llm-finance/lib/python3.12/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'CharDataset' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "num_train_epochs = 3  # Start with a small number of epochs\n",
    "learning_rate = 0.001\n",
    "print_every = 1000\n",
    "\n",
    "# Custom progress callback for better training visualization\n",
    "from transformers import TrainerCallback\n",
    "import sys\n",
    "\n",
    "class ProgressCallback(TrainerCallback):\n",
    "    def __init__(self, total_steps):\n",
    "        self.total_steps = total_steps\n",
    "        self.current_step = 0\n",
    "        self.progress_bar = None\n",
    "        \n",
    "    def on_train_begin(self, args, state, control, **kwargs):\n",
    "        print(\"🚀 Training started!\")\n",
    "        print(f\"📊 Total steps: {self.total_steps}\")\n",
    "        print(f\"📈 Epochs: {args.num_train_epochs}\")\n",
    "        print(f\"🔢 Batch size: {args.per_device_train_batch_size}\")\n",
    "        print()\n",
    "        \n",
    "        # Try to create tqdm progress bar\n",
    "        try:\n",
    "            from tqdm.notebook import tqdm\n",
    "            self.progress_bar = tqdm(total=self.total_steps, desc=\"Training\", unit=\"steps\")\n",
    "        except ImportError:\n",
    "            try:\n",
    "                from tqdm import tqdm\n",
    "                self.progress_bar = tqdm(total=self.total_steps, desc=\"Training\", unit=\"steps\")\n",
    "            except ImportError:\n",
    "                print(\"📈 Progress will be shown as text updates...\")\n",
    "                self.progress_bar = None\n",
    "    \n",
    "    def on_step_end(self, args, state, control, **kwargs):\n",
    "        self.current_step = state.global_step\n",
    "        \n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.update(1)\n",
    "            self.progress_bar.set_postfix({\n",
    "                'loss': f\"{state.log_history[-1].get('loss', 0):.4f}\" if state.log_history else \"N/A\",\n",
    "                'epoch': f\"{state.epoch:.1f}\"\n",
    "            })\n",
    "        else:\n",
    "            # Fallback text progress\n",
    "            if self.current_step % 100 == 0:\n",
    "                progress_pct = (self.current_step / self.total_steps) * 100\n",
    "                print(f\"📊 Step {self.current_step}/{self.total_steps} ({progress_pct:.1f}%) - Epoch {state.epoch:.1f}\")\n",
    "    \n",
    "    def on_evaluate(self, args, state, control, **kwargs):\n",
    "        if not self.progress_bar:\n",
    "            print(f\"🔍 Evaluation at step {self.current_step}\")\n",
    "    \n",
    "    def on_train_end(self, args, state, control, **kwargs):\n",
    "        if self.progress_bar:\n",
    "            self.progress_bar.close()\n",
    "        print(\"\\n✅ Training completed!\")\n",
    "\n",
    "# Calculate total training steps for progress bar\n",
    "steps_per_epoch = len(train_dataset) // training_args.per_device_train_batch_size\n",
    "if len(train_dataset) % training_args.per_device_train_batch_size != 0:\n",
    "    steps_per_epoch += 1\n",
    "total_steps = steps_per_epoch * training_args.num_train_epochs\n",
    "\n",
    "# Create trainer with progress callback\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[ProgressCallback(total_steps)]\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"This may take several minutes depending on your hardware.\")\n",
    "print(f\"Training on {len(train_dataset):,} samples with {total_steps:,} total steps\")\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "training_result = trainer.train()\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "print(f\"Final training loss: {training_result.training_loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating model...\")\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Evaluation loss: {eval_result['eval_loss']:.4f}\")\n",
    "print(f\"Evaluation accuracy: {eval_result['eval_accuracy']:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract training and validation losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "steps = []\n",
    "\n",
    "for log in log_history:\n",
    "    if 'loss' in log:\n",
    "        train_losses.append(log['loss'])\n",
    "        steps.append(log['step'])\n",
    "    if 'eval_loss' in log:\n",
    "        eval_losses.append(log['eval_loss'])\n",
    "\n",
    "# Plot the results\n",
    "if train_losses:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps[:len(train_losses)], train_losses, label='Training Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if eval_losses:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        eval_steps = [log['step'] for log in log_history if 'eval_loss' in log]\n",
    "        plt.plot(eval_steps, eval_losses, label='Validation Loss', color='orange')\n",
    "        plt.title('Validation Loss')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd167bba",
   "metadata": {},
   "source": [
    "## 7. Optimizing Training for CPU and GPU\n",
    "\n",
    "Depending on your hardware, you might want to optimize your training process differently. Let's explore some strategies for both CPU and GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1775c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_training(device_type=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Demonstrate optimization techniques for different hardware.\n",
    "    \"\"\"\n",
    "    print(f\"Optimization strategies for {device_type.upper()} training:\")\n",
    "    \n",
    "    if device_type == \"cpu\":\n",
    "        print(\"1. Use smaller batch sizes to avoid memory issues\")\n",
    "        print(\"2. Reduce model size (fewer layers, smaller hidden dimensions)\")\n",
    "        print(\"3. Use data parallelism with multiple CPU cores\")\n",
    "        print(\"4. Consider mixed precision training with bfloat16 on newer CPUs\")\n",
    "        print(\"5. Ensure proper vectorization of operations\")\n",
    "        \n",
    "        # Example: Setting number of threads for CPU parallelism\n",
    "        torch.set_num_threads(os.cpu_count())\n",
    "        print(f\"Set PyTorch to use {os.cpu_count()} CPU threads\")\n",
    "        \n",
    "    elif device_type == \"gpu\" or device_type == \"cuda\":\n",
    "        print(\"1. Use larger batch sizes to fully utilize GPU memory\")\n",
    "        print(\"2. Enable automatic mixed precision (AMP) for faster computation\")\n",
    "        print(\"3. Use gradient accumulation for effectively larger batches\")\n",
    "        print(\"4. Ensure data is pre-loaded and prefetched\")\n",
    "        print(\"5. Monitor GPU utilization and memory usage\")\n",
    "        \n",
    "        # Example: Setting up mixed precision training\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Example of enabling automatic mixed precision:\")\n",
    "            print(\"   scaler = torch.cuda.amp.GradScaler()\")\n",
    "            print(\"   with torch.cuda.amp.autocast():\")\n",
    "            print(\"       outputs = model(inputs)\")\n",
    "            \n",
    "    elif device_type == \"mps\" or device_type == \"apple\":\n",
    "        print(\"🍎 APPLE SILICON MPS OPTIMIZATIONS:\")\n",
    "        print(\"1. Use moderate batch sizes (16-64) - MPS has different memory characteristics\")\n",
    "        print(\"2. Enable float16 mixed precision for faster computation\")\n",
    "        print(\"3. Avoid very large models that exceed unified memory\")\n",
    "        print(\"4. Use efficient data loading with appropriate num_workers\")\n",
    "        print(\"5. Monitor unified memory usage (shared between CPU and GPU)\")\n",
    "        print(\"6. Prefer tensor operations that are well-optimized for Metal\")\n",
    "        print(\"\\n🔧 MPS-specific tips:\")\n",
    "        print(\"   - MPS shares system RAM, so large models are more feasible\")\n",
    "        print(\"   - Some operations may fall back to CPU automatically\")\n",
    "        print(\"   - Use torch.mps.empty_cache() to free memory if needed\")\n",
    "        print(\"   - Batch sizes of 32-64 typically work well\")\n",
    "    \n",
    "    # Common optimizations\n",
    "    print(\"\\nCommon optimizations for all hardware:\")\n",
    "    print(\"1. Use DataLoader with appropriate num_workers\")\n",
    "    print(\"2. Implement early stopping to avoid overfitting\")\n",
    "    print(\"3. Use learning rate scheduling\")\n",
    "    print(\"4. Profile your code to identify bottlenecks\")\n",
    "    print(\"5. Reduce Python overhead by batching operations\")\n",
    "\n",
    "# Show optimization strategies based on available hardware\n",
    "if torch.cuda.is_available():\n",
    "    optimize_training(\"gpu\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    optimize_training(\"mps\")\n",
    "else:\n",
    "    optimize_training(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82002921",
   "metadata": {},
   "source": [
    "## 7. Generating Text with Our Transformer Model\n",
    "\n",
    "Now that we've trained our Transformer encoder-decoder model, let's use it to generate text. We'll implement a function that uses the encoder to process the input context and the decoder to generate the next character autoregressively. The model will naturally stop when it predicts an `<eos>` token or reaches the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_char(model, input_sequence, char_to_idx, idx_to_char, max_length=20, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate the next character using the trained Transformer model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode input sequence\n",
    "        encoder_input = input_sequence.ljust(max_length, '<pad>')[:max_length]\n",
    "        encoder_ids = torch.tensor(\n",
    "            encode_sequence(encoder_input, char_to_idx, max_length),\n",
    "            dtype=torch.long\n",
    "        ).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Create decoder input (start with <sos> + input sequence)\n",
    "        decoder_input = '<sos>' + input_sequence\n",
    "        decoder_input = decoder_input.ljust(max_length, '<pad>')[:max_length]\n",
    "        decoder_ids = torch.tensor(\n",
    "            encode_sequence(decoder_input, char_to_idx, max_length),\n",
    "            dtype=torch.long\n",
    "        ).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=encoder_ids,\n",
    "            decoder_input_ids=decoder_ids\n",
    "        )\n",
    "        \n",
    "        # Get logits for the last position and apply temperature\n",
    "        logits = outputs.logits[0, -1, :] / temperature\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        predicted_id = torch.multinomial(probabilities, 1).item()\n",
    "        predicted_char = idx_to_char[predicted_id]\n",
    "        \n",
    "        return predicted_char, probabilities\n",
    "\n",
    "def generate_word_completion(model, seed, char_to_idx, idx_to_char, max_new_chars=10, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate word completion starting with a seed string.\n",
    "    Stops when <eos> is generated or max_new_chars is reached.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    current_sequence = seed\n",
    "    generated_chars = []\n",
    "    \n",
    "    print(f\"Generating completion for: '{seed}'\")\n",
    "    print(f\"Generation: '{seed}\", end=\"\")\n",
    "    \n",
    "    for i in range(max_new_chars):\n",
    "        next_char, probs = generate_next_char(\n",
    "            model, current_sequence, char_to_idx, idx_to_char, temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Stop if we generate end-of-sequence or padding\n",
    "        if next_char in ['<eos>', '<pad>']:\n",
    "            print(\"'\")\n",
    "            print(f\"Stopped at <eos> after {i+1} characters\")\n",
    "            break\n",
    "        \n",
    "        generated_chars.append(next_char)\n",
    "        current_sequence += next_char\n",
    "        print(next_char, end=\"\", flush=True)\n",
    "        \n",
    "        # Also stop if we generate a space (natural word boundary)\n",
    "        if next_char == ' ':\n",
    "            print(\"'\")\n",
    "            print(f\"Stopped at space after {i+1} characters\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"'\")\n",
    "        print(f\"Reached maximum length of {max_new_chars} characters\")\n",
    "    \n",
    "    return current_sequence\n",
    "\n",
    "# Test the generation with different seed strings\n",
    "seed_strings = ['fin', 'inv', 'tra', 'mar', 'ban', 'com', 'acc']\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFORMER MODEL TEXT GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for seed in seed_strings:\n",
    "    print(f\"\\n--- Completions for '{seed}' ---\")\n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nTemperature {temp}:\")\n",
    "        try:\n",
    "            completed = generate_word_completion(\n",
    "                model, seed, char_to_idx, idx_to_char, \n",
    "                max_new_chars=15, temperature=temp\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generation: {e}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72844745",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Analysis\n",
    "\n",
    "Let's evaluate our Transformer model's performance more thoroughly. We'll examine prediction accuracy, analyze attention patterns, and compare against random baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_evaluation(model, eval_dataset, char_to_idx, idx_to_char, num_examples=100):\n",
    "    \"\"\"\n",
    "    Perform detailed evaluation of the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    predictions_by_position = {}\n",
    "    \n",
    "    # Sample some examples for detailed analysis\n",
    "    sampled_indices = random.sample(range(len(eval_dataset)), min(num_examples, len(eval_dataset)))\n",
    "    \n",
    "    print(\"Detailed Evaluation Examples:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(sampled_indices[:10]):  # Show first 10 examples\n",
    "            sample = eval_dataset[idx]\n",
    "            \n",
    "            # Prepare inputs\n",
    "            input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "            decoder_input_ids = sample['decoder_input_ids'].unsqueeze(0).to(device)\n",
    "            true_label = sample['labels'].item()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "            predicted_id = torch.argmax(outputs.logits[0, -1, :]).item()\n",
    "            \n",
    "            # Decode sequences for display\n",
    "            encoder_text = decode_sequence(input_ids[0].cpu().numpy(), idx_to_char)\n",
    "            decoder_text = decode_sequence(decoder_input_ids[0].cpu().numpy(), idx_to_char)\n",
    "            true_char = idx_to_char[true_label]\n",
    "            pred_char = idx_to_char[predicted_id]\n",
    "            \n",
    "            # Track accuracy\n",
    "            is_correct = predicted_id == true_label\n",
    "            if is_correct:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "            \n",
    "            # Display example\n",
    "            print(f\"Example {i+1}:\")\n",
    "            print(f\"  Encoder input: '{encoder_text.replace('<pad>', '').strip()}'\")\n",
    "            print(f\"  Decoder input: '{decoder_text.replace('<pad>', '').strip()}'\")\n",
    "            print(f\"  True next char: '{true_char}'\")\n",
    "            print(f\"  Predicted: '{pred_char}' {'✓' if is_correct else '✗'}\")\n",
    "            print()\n",
    "        \n",
    "        # Continue evaluation on remaining examples (without printing)\n",
    "        for idx in sampled_indices[10:]:\n",
    "            sample = eval_dataset[idx]\n",
    "            \n",
    "            input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "            decoder_input_ids = sample['decoder_input_ids'].unsqueeze(0).to(device)\n",
    "            true_label = sample['labels'].item()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "            predicted_id = torch.argmax(outputs.logits[0, -1, :]).item()\n",
    "            \n",
    "            if predicted_id == true_label:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Total examples evaluated: {total_predictions}\")\n",
    "    print(f\"Correct predictions: {correct_predictions}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Perform detailed evaluation\n",
    "print(\"Performing detailed evaluation...\")\n",
    "accuracy = detailed_evaluation(model, val_dataset, char_to_idx, idx_to_char, num_examples=200)\n",
    "\n",
    "# Calculate random baseline\n",
    "random_accuracy = 1.0 / len([c for c in vocab if c not in ['<pad>', '<sos>']])  # Exclude special tokens\n",
    "print(f\"\\nRandom baseline accuracy: {random_accuracy:.4f} ({random_accuracy*100:.2f}%)\")\n",
    "print(f\"Model improvement over random: {(accuracy/random_accuracy):.2f}x\")\n",
    "\n",
    "# Analyze character-level predictions\n",
    "char_predictions = {}\n",
    "for char in string.ascii_lowercase:\n",
    "    char_predictions[char] = {'correct': 0, 'total': 0}\n",
    "\n",
    "# Sample more examples for character analysis\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx in random.sample(range(len(val_dataset)), min(500, len(val_dataset))):\n",
    "        sample = val_dataset[idx]\n",
    "        \n",
    "        input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "        decoder_input_ids = sample['decoder_input_ids'].unsqueeze(0).to(device)\n",
    "        true_label = sample['labels'].item()\n",
    "        \n",
    "        if true_label < len(vocab) and vocab[true_label] in string.ascii_lowercase:\n",
    "            true_char = vocab[true_label]\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "            predicted_id = torch.argmax(outputs.logits[0, -1, :]).item()\n",
    "            \n",
    "            char_predictions[true_char]['total'] += 1\n",
    "            if predicted_id == true_label:\n",
    "                char_predictions[true_char]['correct'] += 1\n",
    "\n",
    "# Display character-level accuracy\n",
    "print(\"\\nCharacter-level Accuracy:\")\n",
    "print(\"-\" * 40)\n",
    "for char in sorted(char_predictions.keys()):\n",
    "    if char_predictions[char]['total'] > 0:\n",
    "        acc = char_predictions[char]['correct'] / char_predictions[char]['total']\n",
    "        print(f\"'{char}': {acc:.3f} ({char_predictions[char]['correct']}/{char_predictions[char]['total']})\")\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d91541",
   "metadata": {},
   "source": [
    "## 9. From Character-Level Transformers to Modern LLMs\n",
    "\n",
    "We've successfully trained a character-level Transformer encoder-decoder model! Let's discuss how this relates to modern large language models (LLMs) and what it would take to scale up to production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec94f67",
   "metadata": {},
   "source": [
    "### Comparison: Our Transformer Model vs. Production LLMs\n",
    "\n",
    "| Feature | Our Character Transformer | Modern LLMs (GPT, BERT, etc.) |\n",
    "|---------|---------------------------|--------------------------------|\n",
    "| **Architecture** | Encoder-Decoder Transformer | Decoder-only or Encoder-only Transformers |\n",
    "| **Parameters** | ~1M | Billions to trillions |\n",
    "| **Token Level** | Character-level | Subword/BPE tokenization |\n",
    "| **Training Data** | ~100K character sequences | Trillions of tokens from web, books, code |\n",
    "| **Context Length** | 20 characters | 2K-100K+ tokens |\n",
    "| **Training Time** | Minutes on laptop/GPU | Weeks on thousands of GPUs |\n",
    "| **Attention Mechanism** | Full attention | Various optimizations (sparse, sliding window) |\n",
    "| **Capabilities** | Next character prediction | Language understanding, reasoning, code generation |\n",
    "| **Applications** | Educational/toy examples | Production AI systems, financial analysis |\n",
    "| **Hardware Requirements** | Single GPU/CPU | Distributed systems, specialized hardware |\n",
    "| **Memory Usage** | <1GB | 100GB+ for inference |\n",
    "\n",
    "### Key Insights from Our Implementation\n",
    "\n",
    "1. **Transformer Architecture**: Our model uses the same fundamental building blocks as GPT and BERT\n",
    "2. **Attention Mechanism**: The model learns to focus on relevant parts of the input sequence\n",
    "3. **Autoregressive Generation**: Character-by-character generation mirrors how LLMs generate text\n",
    "4. **Encoder-Decoder Design**: Similar to models like T5, BART, and early machine translation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25590329",
   "metadata": {},
   "source": [
    "### Scaling to Financial LLM Applications\n",
    "\n",
    "To build production-ready LLMs for finance, we would need to scale our approach:\n",
    "\n",
    "#### 1. **Architecture Improvements**\n",
    "- **Decoder-only models**: Like GPT, for better autoregressive generation\n",
    "- **Mixture of Experts (MoE)**: Efficiently scale parameters\n",
    "- **Rotary Position Embeddings**: Better handling of long sequences\n",
    "- **Layer normalization variants**: RMSNorm, Pre-LN for stability\n",
    "\n",
    "#### 2. **Tokenization Strategy**\n",
    "- **Subword tokenization**: BPE, SentencePiece for efficient vocabulary\n",
    "- **Financial domain tokens**: Special tokens for financial terms, numbers, dates\n",
    "- **Multilingual support**: For global financial markets\n",
    "\n",
    "#### 3. **Training Data & Scale**\n",
    "- **Financial corpora**: SEC filings, earnings calls, financial news, research reports\n",
    "- **Code integration**: Financial modeling code, SQL queries\n",
    "- **Structured data**: Financial statements, market data, regulatory filings\n",
    "- **Real-time data**: Market feeds, news streams\n",
    "\n",
    "#### 4. **Training Techniques**\n",
    "- **Pretraining**: Large-scale unsupervised learning on financial text\n",
    "- **Instruction tuning**: Fine-tuning on financial Q&A, analysis tasks\n",
    "- **RLHF**: Reinforcement learning from financial expert feedback\n",
    "- **Domain adaptation**: Continued pretraining on financial data\n",
    "\n",
    "#### 5. **Financial-Specific Optimizations**\n",
    "- **Numerical reasoning**: Enhanced arithmetic and financial calculation abilities\n",
    "- **Time series understanding**: Market data, financial trends\n",
    "- **Risk assessment**: Model uncertainty quantification\n",
    "- **Compliance**: Ensuring regulatory compliance in outputs\n",
    "- **Explainability**: Interpretable financial recommendations\n",
    "\n",
    "#### 6. **Production Considerations**\n",
    "- **Latency optimization**: Fast inference for real-time trading decisions\n",
    "- **Scalability**: Handle multiple concurrent financial analysis requests\n",
    "- **Security**: Protect sensitive financial information\n",
    "- **Monitoring**: Track model performance and drift in financial markets\n",
    "\n",
    "Our character-level Transformer provides the foundational understanding for these advanced systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16154ee",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "Congratulations! In this notebook, you've successfully:\n",
    "\n",
    "✅ **Built a Transformer encoder-decoder model** from scratch using Hugging Face  \n",
    "✅ **Implemented character-level tokenization** with proper special tokens  \n",
    "✅ **Trained on NLTK corpus** with sequence-to-sequence learning  \n",
    "✅ **Used modern training techniques** with the Hugging Face Trainer  \n",
    "✅ **Generated text autoregressively** with proper stopping mechanisms  \n",
    "✅ **Evaluated model performance** with comprehensive metrics  \n",
    "✅ **Understood the path to production LLMs** in financial applications  \n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "1. **Transformer Architecture**: You've implemented the same core architecture used in GPT, BERT, and T5\n",
    "2. **Character-level Modeling**: Understanding how models can work at the most granular text level\n",
    "3. **Encoder-Decoder Design**: Experience with sequence-to-sequence learning patterns\n",
    "4. **Modern Training Stack**: Hands-on experience with Hugging Face transformers\n",
    "5. **Evaluation Techniques**: Comprehensive model assessment strategies\n",
    "6. **Scaling Insights**: Clear path from toy models to production systems\n",
    "\n",
    "### From Here to Financial LLMs\n",
    "\n",
    "Your character-level Transformer shares DNA with models like:\n",
    "- **GPT-4**: Decoder-only architecture for text generation\n",
    "- **BERT**: Encoder architecture for understanding tasks  \n",
    "- **T5**: Encoder-decoder for various NLP tasks\n",
    "- **Financial domain models**: BloombergGPT, FinBERT, etc.\n",
    "\n",
    "The principles you've learned—attention mechanisms, autoregressive generation, transformer blocks—are the building blocks of all modern LLMs used in finance today.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Immediate Experiments\n",
    "- 🔧 **Increase model size**: More layers, larger hidden dimensions\n",
    "- 📊 **Try different data**: Financial news, SEC filings, earnings transcripts\n",
    "- 🎯 **Task-specific fine-tuning**: Financial sentiment, NER, QA\n",
    "- ⚡ **Optimization**: Mixed precision, gradient checkpointing\n",
    "\n",
    "### Advanced Projects\n",
    "- 🚀 **Implement GPT-style decoder-only model** for better generation\n",
    "- 📈 **Fine-tune on financial data** for domain adaptation\n",
    "- 🔍 **Add retrieval mechanisms** for factual financial information\n",
    "- 🛡️ **Implement safety measures** for financial advice generation\n",
    "- 📱 **Deploy as API** for real-time financial analysis\n",
    "\n",
    "### Production Path\n",
    "- 🏗️ **Scale to subword tokenization** (BPE, SentencePiece)\n",
    "- 🌐 **Multi-GPU training** with distributed computing\n",
    "- 📊 **Add financial-specific metrics** and evaluation frameworks\n",
    "- 🔒 **Implement security** and compliance measures\n",
    "- 📈 **Continuous learning** from new financial data\n",
    "\n",
    "You now have the foundation to understand and build the next generation of financial AI systems! 🎉"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
