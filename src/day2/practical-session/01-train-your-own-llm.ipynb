{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7b26f1",
   "metadata": {},
   "source": [
    "# Training Your Own LLM with Transformer Architecture\n",
    "\n",
    "Welcome to the Day 2 practical session on training your own language model! In this notebook, we'll learn how to train a character-level language model using a Transformer encoder-decoder architecture with Hugging Face transformers. We'll use individual letters as tokens and train on the NLTK words corpus to understand the fundamentals of modern LLM training.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the basics of Transformer-based language model training\n",
    "- Build a character-level language model using encoder-decoder architecture\n",
    "- Work with Hugging Face transformers and tokenizers\n",
    "- Train on NLTK corpus with proper stopping mechanisms\n",
    "- Evaluate model performance and generate text\n",
    "- Gain insights into how modern LLMs are trained\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python and PyTorch (covered in preliminaries)\n",
    "- Familiarity with the concept of language models and transformers\n",
    "- A computer with PyTorch and Hugging Face transformers installed (CPU or GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131a73d",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment. We'll be using PyTorch and Hugging Face transformers for our Transformer-based neural network implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9adeeed",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01moptim\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import os\n",
    "from transformers import (\n",
    "    EncoderDecoderModel,\n",
    "    BertConfig,\n",
    "    BertModel,\n",
    "    PreTrainedTokenizerFast,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import json\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093fc66",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Let's create a dataset of English words using the NLTK words corpus. We'll create a character-level tokenizer that treats individual letters (including spaces) as tokens. Our Transformer encoder-decoder model will learn to predict the next character in a sequence, with spaces serving as natural stopping tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK words if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "    nltk.download('words')\n",
    "\n",
    "# Get list of English words\n",
    "english_words = words.words()\n",
    "\n",
    "# Filter to get words of reasonable length (3-15 characters)\n",
    "filtered_words = [word.lower() for word in english_words if 3 <= len(word) <= 15 and word.isalpha()]\n",
    "\n",
    "print(f\"Total words in dataset: {len(filtered_words)}\")\n",
    "print(f\"Sample words: {filtered_words[:10]}\")\n",
    "\n",
    "# Create a vocabulary of all characters in our dataset\n",
    "# Include space as a special token for sequence ending\n",
    "special_tokens = ['<pad>', '<sos>', '<eos>']  # padding, start of sequence, end of sequence\n",
    "all_characters = list(string.ascii_lowercase) + [' ']  # lowercase letters and space\n",
    "vocab = special_tokens + all_characters\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "print(f\"Characters in vocabulary: {all_characters}\")\n",
    "print(f\"Full vocabulary (with special tokens): {vocab}\")\n",
    "print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "# Create character-to-index and index-to-character mappings\n",
    "char_to_idx = {char: i for i, char in enumerate(vocab)}\n",
    "idx_to_char = {i: char for i, char in enumerate(vocab)}\n",
    "\n",
    "print(f\"Special token indices:\")\n",
    "print(f\"  <pad>: {char_to_idx['<pad>']}\")\n",
    "print(f\"  <sos>: {char_to_idx['<sos>']}\")\n",
    "print(f\"  <eos>: {char_to_idx['<eos>']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefde38",
   "metadata": {},
   "source": [
    "## 3. Creating Training Data for Encoder-Decoder Architecture\n",
    "\n",
    "For our encoder-decoder Transformer model, we'll create training pairs where:\n",
    "- **Encoder input**: A partial word (characters up to a certain position)\n",
    "- **Decoder input**: The same partial word with `<sos>` token at the beginning\n",
    "- **Decoder target**: The next character in the sequence, with `<eos>` token at the end\n",
    "\n",
    "This approach teaches the model to:\n",
    "1. Encode the input sequence context\n",
    "2. Generate the next character autoregressively\n",
    "3. Stop generation when encountering spaces or reaching word boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be533170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_data(word):\n",
    "    \"\"\"\n",
    "    Convert a word into a list of (input_sequence, target_character) pairs.\n",
    "    For example, \"hello\" would yield:\n",
    "    [(\"h\", \"e\"), (\"he\", \"l\"), (\"hel\", \"l\"), (\"hell\", \"o\")]\n",
    "    \"\"\"\n",
    "    sequence_pairs = []\n",
    "    for i in range(1, len(word)):\n",
    "        input_seq = word[:i]\n",
    "        target_char = word[i]\n",
    "        sequence_pairs.append((input_seq, target_char))\n",
    "    return sequence_pairs\n",
    "\n",
    "# Create training examples\n",
    "training_pairs = []\n",
    "for word in filtered_words:\n",
    "    training_pairs.extend(prepare_sequence_data(word))\n",
    "\n",
    "# Shuffle the training pairs\n",
    "random.shuffle(training_pairs)\n",
    "\n",
    "# Limit to 100,000 examples to avoid memory issues\n",
    "training_pairs = training_pairs[:100000]\n",
    "\n",
    "print(f\"Total training examples: {len(training_pairs)}\")\n",
    "print(f\"Sample training pairs: {training_pairs[:5]}\")\n",
    "\n",
    "# Function to convert a string to a tensor of character indices\n",
    "def string_to_tensor(string):\n",
    "    tensor = torch.zeros(len(string), 1, n_characters)\n",
    "    for i, char in enumerate(string):\n",
    "        index = char_to_idx.get(char, char_to_idx[' '])  # Default to space if char not found\n",
    "        tensor[i][0][index] = 1\n",
    "    return tensor\n",
    "\n",
    "# Function to convert a character to a tensor (one-hot encoding)\n",
    "def char_to_tensor(char):\n",
    "    tensor = torch.zeros(1, n_characters)\n",
    "    index = char_to_idx.get(char, char_to_idx[' '])  # Default to space if char not found\n",
    "    tensor[0][index] = 1\n",
    "    return tensor\n",
    "\n",
    "def create_encoder_decoder_pairs(word, max_length=20):\n",
    "    \"\"\"\n",
    "    Create encoder-decoder training pairs from a word.\n",
    "    For word \"hello\":\n",
    "    - (\"h\", \"<sos>\", \"e\") -> encoder gets \"h\", decoder input is \"<sos>\", target is \"e\"\n",
    "    - (\"he\", \"<sos>e\", \"l\") -> encoder gets \"he\", decoder input is \"<sos>e\", target is \"l\"\n",
    "    - And so on...\n",
    "    \"\"\"\n",
    "    pairs = []\n",
    "    \n",
    "    for i in range(1, len(word)):\n",
    "        encoder_input = word[:i]  # Characters seen so far\n",
    "        decoder_input = '<sos>' + word[:i]  # Start token + seen characters\n",
    "        target_char = word[i]  # Next character to predict\n",
    "        \n",
    "        # Pad sequences to max_length if needed\n",
    "        encoder_input = encoder_input.ljust(max_length, '<pad>')\n",
    "        decoder_input = decoder_input.ljust(max_length, '<pad>')\n",
    "        \n",
    "        pairs.append((encoder_input[:max_length], decoder_input[:max_length], target_char))\n",
    "    \n",
    "    # Add final pair that should predict end of sequence\n",
    "    encoder_input = word.ljust(max_length, '<pad>')\n",
    "    decoder_input = ('<sos>' + word).ljust(max_length, '<pad>')\n",
    "    target_char = '<eos>'\n",
    "    \n",
    "    pairs.append((encoder_input[:max_length], decoder_input[:max_length], target_char))\n",
    "    \n",
    "    return pairs\n",
    "\n",
    "# Create training dataset\n",
    "training_data = []\n",
    "max_seq_length = 20\n",
    "\n",
    "for word in filtered_words[:5000]:  # Limit to 5000 words for faster training\n",
    "    pairs = create_encoder_decoder_pairs(word, max_seq_length)\n",
    "    training_data.extend(pairs)\n",
    "\n",
    "# Shuffle training data\n",
    "random.shuffle(training_data)\n",
    "\n",
    "print(f\"Total training examples: {len(training_data)}\")\n",
    "print(f\"Sample training examples:\")\n",
    "for i in range(3):\n",
    "    enc_in, dec_in, target = training_data[i]\n",
    "    print(f\"  Encoder input: '{enc_in.replace('<pad>', '').strip()}'\")\n",
    "    print(f\"  Decoder input: '{dec_in.replace('<pad>', '').strip()}'\")\n",
    "    print(f\"  Target: '{target}'\")\n",
    "    print()\n",
    "\n",
    "def encode_sequence(sequence, char_to_idx, max_length):\n",
    "    \"\"\"\n",
    "    Convert a sequence of characters to indices.\n",
    "    \"\"\"\n",
    "    indices = []\n",
    "    for char in sequence[:max_length]:\n",
    "        indices.append(char_to_idx.get(char, char_to_idx['<pad>']))\n",
    "    \n",
    "    # Pad if necessary\n",
    "    while len(indices) < max_length:\n",
    "        indices.append(char_to_idx['<pad>'])\n",
    "    \n",
    "    return indices[:max_length]\n",
    "\n",
    "def decode_sequence(indices, idx_to_char):\n",
    "    \"\"\"\n",
    "    Convert indices back to characters.\n",
    "    \"\"\"\n",
    "    chars = []\n",
    "    for idx in indices:\n",
    "        char = idx_to_char.get(idx, '<unk>')\n",
    "        if char == '<pad>':\n",
    "            break\n",
    "        chars.append(char)\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e14136",
   "metadata": {},
   "source": [
    "## 4. Building the Transformer Model\n",
    "\n",
    "Now, let's build our character-level language model using a Transformer encoder-decoder architecture. We'll use Hugging Face's `EncoderDecoderModel` which combines BERT-like encoders and decoders to create a sequence-to-sequence model perfect for our character-level prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb369c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for character-level sequence-to-sequence learning.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, char_to_idx, max_length):\n",
    "        self.data = data\n",
    "        self.char_to_idx = char_to_idx\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        encoder_input, decoder_input, target = self.data[idx]\n",
    "        \n",
    "        # Encode sequences\n",
    "        encoder_ids = encode_sequence(encoder_input, self.char_to_idx, self.max_length)\n",
    "        decoder_ids = encode_sequence(decoder_input, self.char_to_idx, self.max_length)\n",
    "        target_id = self.char_to_idx.get(target, self.char_to_idx['<pad>'])\n",
    "        \n",
    "        return {\n",
    "            'input_ids': torch.tensor(encoder_ids, dtype=torch.long),\n",
    "            'decoder_input_ids': torch.tensor(decoder_ids, dtype=torch.long),\n",
    "            'labels': torch.tensor(target_id, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Create dataset\n",
    "dataset = CharDataset(training_data, char_to_idx, max_seq_length)\n",
    "\n",
    "# Split into train and validation\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Configure the encoder and decoder\n",
    "encoder_config = BertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=max_seq_length,\n",
    "    pad_token_id=char_to_idx['<pad>']\n",
    ")\n",
    "\n",
    "decoder_config = BertConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    hidden_size=256,\n",
    "    num_hidden_layers=4,\n",
    "    num_attention_heads=8,\n",
    "    intermediate_size=512,\n",
    "    max_position_embeddings=max_seq_length,\n",
    "    pad_token_id=char_to_idx['<pad>'],\n",
    "    is_decoder=True,\n",
    "    add_cross_attention=True\n",
    ")\n",
    "\n",
    "# Create the encoder-decoder model\n",
    "model = EncoderDecoderModel.from_encoder_decoder_configs(\n",
    "    encoder_config, decoder_config\n",
    ")\n",
    "\n",
    "# Set special tokens\n",
    "model.config.decoder_start_token_id = char_to_idx['<sos>']\n",
    "model.config.eos_token_id = char_to_idx['<eos>']\n",
    "model.config.pad_token_id = char_to_idx['<pad>']\n",
    "model.config.vocab_size = vocab_size\n",
    "\n",
    "# Move model to device\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Model configuration:\")\n",
    "print(f\"  Encoder layers: {encoder_config.num_hidden_layers}\")\n",
    "print(f\"  Decoder layers: {decoder_config.num_hidden_layers}\")\n",
    "print(f\"  Hidden size: {encoder_config.hidden_size}\")\n",
    "print(f\"  Attention heads: {encoder_config.num_attention_heads}\")\n",
    "print(f\"  Vocabulary size: {vocab_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685887c",
   "metadata": {},
   "source": [
    "## 5. Training Functions for Transformer\n",
    "\n",
    "We'll use Hugging Face's Trainer class to handle the training loop efficiently. We'll also create a custom data collator to properly batch our encoder-decoder sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efe2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharDataCollator:\n",
    "    \"\"\"\n",
    "    Custom data collator for character-level encoder-decoder training.\n",
    "    \"\"\"\n",
    "    def __init__(self, pad_token_id):\n",
    "        self.pad_token_id = pad_token_id\n",
    "    \n",
    "    def __call__(self, features):\n",
    "        batch_input_ids = []\n",
    "        batch_decoder_input_ids = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for feature in features:\n",
    "            batch_input_ids.append(feature['input_ids'])\n",
    "            batch_decoder_input_ids.append(feature['decoder_input_ids'])\n",
    "            batch_labels.append(feature['labels'])\n",
    "        \n",
    "        # Stack tensors\n",
    "        batch = {\n",
    "            'input_ids': torch.stack(batch_input_ids),\n",
    "            'decoder_input_ids': torch.stack(batch_decoder_input_ids),\n",
    "            'labels': torch.stack(batch_labels)\n",
    "        }\n",
    "        \n",
    "        return batch\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"\n",
    "    Compute accuracy metrics for evaluation.\n",
    "    \"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    # Get predicted token indices\n",
    "    predicted_ids = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = (predicted_ids == labels).mean()\n",
    "    \n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "# Create data collator\n",
    "data_collator = CharDataCollator(pad_token_id=char_to_idx['<pad>'])\n",
    "\n",
    "# Set up training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./char-transformer-results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_steps=1000,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    report_to=None,  # Disable wandb/tensorboard\n",
    "    dataloader_pin_memory=False\n",
    ")\n",
    "\n",
    "print(\"Training setup completed!\")\n",
    "print(f\"Training arguments:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Warmup steps: {training_args.warmup_steps}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684536b",
   "metadata": {},
   "source": [
    "## 6. Training the Transformer Model\n",
    "\n",
    "Now, let's train our Transformer encoder-decoder model! We'll use the Hugging Face Trainer which handles the training loop, evaluation, and logging automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "num_train_epochs = 3  # Start with a small number of epochs\n",
    "learning_rate = 0.001\n",
    "print_every = 1000\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(f\"This may take several minutes depending on your hardware.\")\n",
    "\n",
    "# Train the model\n",
    "start_time = time.time()\n",
    "training_result = trainer.train()\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\nTraining completed!\")\n",
    "print(f\"Training time: {training_time:.2f} seconds\")\n",
    "print(f\"Final training loss: {training_result.training_loss:.4f}\")\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"\\nEvaluating model...\")\n",
    "eval_result = trainer.evaluate()\n",
    "print(f\"Evaluation loss: {eval_result['eval_loss']:.4f}\")\n",
    "print(f\"Evaluation accuracy: {eval_result['eval_accuracy']:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Extract training and validation losses\n",
    "train_losses = []\n",
    "eval_losses = []\n",
    "steps = []\n",
    "\n",
    "for log in log_history:\n",
    "    if 'loss' in log:\n",
    "        train_losses.append(log['loss'])\n",
    "        steps.append(log['step'])\n",
    "    if 'eval_loss' in log:\n",
    "        eval_losses.append(log['eval_loss'])\n",
    "\n",
    "# Plot the results\n",
    "if train_losses:\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(steps[:len(train_losses)], train_losses, label='Training Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if eval_losses:\n",
    "        plt.subplot(1, 2, 2)\n",
    "        eval_steps = [log['step'] for log in log_history if 'eval_loss' in log]\n",
    "        plt.plot(eval_steps, eval_losses, label='Validation Loss', color='orange')\n",
    "        plt.title('Validation Loss')\n",
    "        plt.xlabel('Steps')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No training history available for plotting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd167bba",
   "metadata": {},
   "source": [
    "## 7. Optimizing Training for CPU and GPU\n",
    "\n",
    "Depending on your hardware, you might want to optimize your training process differently. Let's explore some strategies for both CPU and GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1775c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_training(device_type=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Demonstrate optimization techniques for different hardware.\n",
    "    \"\"\"\n",
    "    print(f\"Optimization strategies for {device_type.upper()} training:\")\n",
    "    \n",
    "    if device_type == \"cpu\":\n",
    "        print(\"1. Use smaller batch sizes to avoid memory issues\")\n",
    "        print(\"2. Reduce model size (fewer layers, smaller hidden dimensions)\")\n",
    "        print(\"3. Use data parallelism with multiple CPU cores\")\n",
    "        print(\"4. Consider mixed precision training with bfloat16 on newer CPUs\")\n",
    "        print(\"5. Ensure proper vectorization of operations\")\n",
    "        \n",
    "        # Example: Setting number of threads for CPU parallelism\n",
    "        torch.set_num_threads(os.cpu_count())\n",
    "        print(f\"Set PyTorch to use {os.cpu_count()} CPU threads\")\n",
    "        \n",
    "    elif device_type == \"gpu\":\n",
    "        print(\"1. Use larger batch sizes to fully utilize GPU memory\")\n",
    "        print(\"2. Enable automatic mixed precision (AMP) for faster computation\")\n",
    "        print(\"3. Use gradient accumulation for effectively larger batches\")\n",
    "        print(\"4. Ensure data is pre-loaded and prefetched\")\n",
    "        print(\"5. Monitor GPU utilization and memory usage\")\n",
    "        \n",
    "        # Example: Setting up mixed precision training\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Example of enabling automatic mixed precision:\")\n",
    "            print(\"   scaler = torch.cuda.amp.GradScaler()\")\n",
    "            print(\"   with torch.cuda.amp.autocast():\")\n",
    "            print(\"       outputs = model(inputs)\")\n",
    "    \n",
    "    # Common optimizations\n",
    "    print(\"\\nCommon optimizations for both CPU and GPU:\")\n",
    "    print(\"1. Use DataLoader with appropriate num_workers\")\n",
    "    print(\"2. Implement early stopping to avoid overfitting\")\n",
    "    print(\"3. Use learning rate scheduling\")\n",
    "    print(\"4. Profile your code to identify bottlenecks\")\n",
    "    print(\"5. Reduce Python overhead by batching operations\")\n",
    "\n",
    "# Show optimization strategies based on available hardware\n",
    "optimize_training(\"gpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82002921",
   "metadata": {},
   "source": [
    "## 7. Generating Text with Our Transformer Model\n",
    "\n",
    "Now that we've trained our Transformer encoder-decoder model, let's use it to generate text. We'll implement a function that uses the encoder to process the input context and the decoder to generate the next character autoregressively. The model will naturally stop when it predicts an `<eos>` token or reaches the maximum length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_next_char(model, input_sequence, char_to_idx, idx_to_char, max_length=20, temperature=1.0):\n",
    "    \"\"\"\n",
    "    Generate the next character using the trained Transformer model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode input sequence\n",
    "        encoder_input = input_sequence.ljust(max_length, '<pad>')[:max_length]\n",
    "        encoder_ids = torch.tensor(\n",
    "            encode_sequence(encoder_input, char_to_idx, max_length),\n",
    "            dtype=torch.long\n",
    "        ).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Create decoder input (start with <sos> + input sequence)\n",
    "        decoder_input = '<sos>' + input_sequence\n",
    "        decoder_input = decoder_input.ljust(max_length, '<pad>')[:max_length]\n",
    "        decoder_ids = torch.tensor(\n",
    "            encode_sequence(decoder_input, char_to_idx, max_length),\n",
    "            dtype=torch.long\n",
    "        ).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(\n",
    "            input_ids=encoder_ids,\n",
    "            decoder_input_ids=decoder_ids\n",
    "        )\n",
    "        \n",
    "        # Get logits for the last position and apply temperature\n",
    "        logits = outputs.logits[0, -1, :] / temperature\n",
    "        probabilities = F.softmax(logits, dim=-1)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        predicted_id = torch.multinomial(probabilities, 1).item()\n",
    "        predicted_char = idx_to_char[predicted_id]\n",
    "        \n",
    "        return predicted_char, probabilities\n",
    "\n",
    "def generate_word_completion(model, seed, char_to_idx, idx_to_char, max_new_chars=10, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate word completion starting with a seed string.\n",
    "    Stops when <eos> is generated or max_new_chars is reached.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    current_sequence = seed\n",
    "    generated_chars = []\n",
    "    \n",
    "    print(f\"Generating completion for: '{seed}'\")\n",
    "    print(f\"Generation: '{seed}\", end=\"\")\n",
    "    \n",
    "    for i in range(max_new_chars):\n",
    "        next_char, probs = generate_next_char(\n",
    "            model, current_sequence, char_to_idx, idx_to_char, temperature=temperature\n",
    "        )\n",
    "        \n",
    "        # Stop if we generate end-of-sequence or padding\n",
    "        if next_char in ['<eos>', '<pad>']:\n",
    "            print(\"'\")\n",
    "            print(f\"Stopped at <eos> after {i+1} characters\")\n",
    "            break\n",
    "        \n",
    "        generated_chars.append(next_char)\n",
    "        current_sequence += next_char\n",
    "        print(next_char, end=\"\", flush=True)\n",
    "        \n",
    "        # Also stop if we generate a space (natural word boundary)\n",
    "        if next_char == ' ':\n",
    "            print(\"'\")\n",
    "            print(f\"Stopped at space after {i+1} characters\")\n",
    "            break\n",
    "    else:\n",
    "        print(\"'\")\n",
    "        print(f\"Reached maximum length of {max_new_chars} characters\")\n",
    "    \n",
    "    return current_sequence\n",
    "\n",
    "# Test the generation with different seed strings\n",
    "seed_strings = ['fin', 'inv', 'tra', 'mar', 'ban', 'com', 'acc']\n",
    "temperatures = [0.5, 1.0, 1.5]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRANSFORMER MODEL TEXT GENERATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for seed in seed_strings:\n",
    "    print(f\"\\n--- Completions for '{seed}' ---\")\n",
    "    for temp in temperatures:\n",
    "        print(f\"\\nTemperature {temp}:\")\n",
    "        try:\n",
    "            completed = generate_word_completion(\n",
    "                model, seed, char_to_idx, idx_to_char, \n",
    "                max_new_chars=15, temperature=temp\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error during generation: {e}\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72844745",
   "metadata": {},
   "source": [
    "## 8. Model Evaluation and Analysis\n",
    "\n",
    "Let's evaluate our Transformer model's performance more thoroughly. We'll examine prediction accuracy, analyze attention patterns, and compare against random baselines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_evaluation(model, eval_dataset, char_to_idx, idx_to_char, num_examples=100):\n",
    "    \"\"\"\n",
    "    Perform detailed evaluation of the model.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "    predictions_by_position = {}\n",
    "    \n",
    "    # Sample some examples for detailed analysis\n",
    "    sampled_indices = random.sample(range(len(eval_dataset)), min(num_examples, len(eval_dataset)))\n",
    "    \n",
    "    print(\"Detailed Evaluation Examples:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, idx in enumerate(sampled_indices[:10]):  # Show first 10 examples\n",
    "            sample = eval_dataset[idx]\n",
    "            \n",
    "            # Prepare inputs\n",
    "            input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "            decoder_input_ids = sample['decoder_input_ids'].unsqueeze(0).to(device)\n",
    "            true_label = sample['labels'].item()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "            predicted_id = torch.argmax(outputs.logits[0, -1, :]).item()\n",
    "            \n",
    "            # Decode sequences for display\n",
    "            encoder_text = decode_sequence(input_ids[0].cpu().numpy(), idx_to_char)\n",
    "            decoder_text = decode_sequence(decoder_input_ids[0].cpu().numpy(), idx_to_char)\n",
    "            true_char = idx_to_char[true_label]\n",
    "            pred_char = idx_to_char[predicted_id]\n",
    "            \n",
    "            # Track accuracy\n",
    "            is_correct = predicted_id == true_label\n",
    "            if is_correct:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "            \n",
    "            # Display example\n",
    "            print(f\"Example {i+1}:\")\n",
    "            print(f\"  Encoder input: '{encoder_text.replace('<pad>', '').strip()}'\")\n",
    "            print(f\"  Decoder input: '{decoder_text.replace('<pad>', '').strip()}'\")\n",
    "            print(f\"  True next char: '{true_char}'\")\n",
    "            print(f\"  Predicted: '{pred_char}' {'âœ“' if is_correct else 'âœ—'}\")\n",
    "            print()\n",
    "        \n",
    "        # Continue evaluation on remaining examples (without printing)\n",
    "        for idx in sampled_indices[10:]:\n",
    "            sample = eval_dataset[idx]\n",
    "            \n",
    "            input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "            decoder_input_ids = sample['decoder_input_ids'].unsqueeze(0).to(device)\n",
    "            true_label = sample['labels'].item()\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "            predicted_id = torch.argmax(outputs.logits[0, -1, :]).item()\n",
    "            \n",
    "            if predicted_id == true_label:\n",
    "                correct_predictions += 1\n",
    "            total_predictions += 1\n",
    "    \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    \n",
    "    print(f\"\\nEvaluation Results:\")\n",
    "    print(f\"Total examples evaluated: {total_predictions}\")\n",
    "    print(f\"Correct predictions: {correct_predictions}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Perform detailed evaluation\n",
    "print(\"Performing detailed evaluation...\")\n",
    "accuracy = detailed_evaluation(model, val_dataset, char_to_idx, idx_to_char, num_examples=200)\n",
    "\n",
    "# Calculate random baseline\n",
    "random_accuracy = 1.0 / len([c for c in vocab if c not in ['<pad>', '<sos>']])  # Exclude special tokens\n",
    "print(f\"\\nRandom baseline accuracy: {random_accuracy:.4f} ({random_accuracy*100:.2f}%)\")\n",
    "print(f\"Model improvement over random: {(accuracy/random_accuracy):.2f}x\")\n",
    "\n",
    "# Analyze character-level predictions\n",
    "char_predictions = {}\n",
    "for char in string.ascii_lowercase:\n",
    "    char_predictions[char] = {'correct': 0, 'total': 0}\n",
    "\n",
    "# Sample more examples for character analysis\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for idx in random.sample(range(len(val_dataset)), min(500, len(val_dataset))):\n",
    "        sample = val_dataset[idx]\n",
    "        \n",
    "        input_ids = sample['input_ids'].unsqueeze(0).to(device)\n",
    "        decoder_input_ids = sample['decoder_input_ids'].unsqueeze(0).to(device)\n",
    "        true_label = sample['labels'].item()\n",
    "        \n",
    "        if true_label < len(vocab) and vocab[true_label] in string.ascii_lowercase:\n",
    "            true_char = vocab[true_label]\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, decoder_input_ids=decoder_input_ids)\n",
    "            predicted_id = torch.argmax(outputs.logits[0, -1, :]).item()\n",
    "            \n",
    "            char_predictions[true_char]['total'] += 1\n",
    "            if predicted_id == true_label:\n",
    "                char_predictions[true_char]['correct'] += 1\n",
    "\n",
    "# Display character-level accuracy\n",
    "print(\"\\nCharacter-level Accuracy:\")\n",
    "print(\"-\" * 40)\n",
    "for char in sorted(char_predictions.keys()):\n",
    "    if char_predictions[char]['total'] > 0:\n",
    "        acc = char_predictions[char]['correct'] / char_predictions[char]['total']\n",
    "        print(f\"'{char}': {acc:.3f} ({char_predictions[char]['correct']}/{char_predictions[char]['total']})\")\n",
    "\n",
    "print(\"\\nEvaluation completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d91541",
   "metadata": {},
   "source": [
    "## 9. From Character-Level Transformers to Modern LLMs\n",
    "\n",
    "We've successfully trained a character-level Transformer encoder-decoder model! Let's discuss how this relates to modern large language models (LLMs) and what it would take to scale up to production systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec94f67",
   "metadata": {},
   "source": [
    "### Comparison: Our Transformer Model vs. Production LLMs\n",
    "\n",
    "| Feature | Our Character Transformer | Modern LLMs (GPT, BERT, etc.) |\n",
    "|---------|---------------------------|--------------------------------|\n",
    "| **Architecture** | Encoder-Decoder Transformer | Decoder-only or Encoder-only Transformers |\n",
    "| **Parameters** | ~1M | Billions to trillions |\n",
    "| **Token Level** | Character-level | Subword/BPE tokenization |\n",
    "| **Training Data** | ~100K character sequences | Trillions of tokens from web, books, code |\n",
    "| **Context Length** | 20 characters | 2K-100K+ tokens |\n",
    "| **Training Time** | Minutes on laptop/GPU | Weeks on thousands of GPUs |\n",
    "| **Attention Mechanism** | Full attention | Various optimizations (sparse, sliding window) |\n",
    "| **Capabilities** | Next character prediction | Language understanding, reasoning, code generation |\n",
    "| **Applications** | Educational/toy examples | Production AI systems, financial analysis |\n",
    "| **Hardware Requirements** | Single GPU/CPU | Distributed systems, specialized hardware |\n",
    "| **Memory Usage** | <1GB | 100GB+ for inference |\n",
    "\n",
    "### Key Insights from Our Implementation\n",
    "\n",
    "1. **Transformer Architecture**: Our model uses the same fundamental building blocks as GPT and BERT\n",
    "2. **Attention Mechanism**: The model learns to focus on relevant parts of the input sequence\n",
    "3. **Autoregressive Generation**: Character-by-character generation mirrors how LLMs generate text\n",
    "4. **Encoder-Decoder Design**: Similar to models like T5, BART, and early machine translation systems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25590329",
   "metadata": {},
   "source": [
    "### Scaling to Financial LLM Applications\n",
    "\n",
    "To build production-ready LLMs for finance, we would need to scale our approach:\n",
    "\n",
    "#### 1. **Architecture Improvements**\n",
    "- **Decoder-only models**: Like GPT, for better autoregressive generation\n",
    "- **Mixture of Experts (MoE)**: Efficiently scale parameters\n",
    "- **Rotary Position Embeddings**: Better handling of long sequences\n",
    "- **Layer normalization variants**: RMSNorm, Pre-LN for stability\n",
    "\n",
    "#### 2. **Tokenization Strategy**\n",
    "- **Subword tokenization**: BPE, SentencePiece for efficient vocabulary\n",
    "- **Financial domain tokens**: Special tokens for financial terms, numbers, dates\n",
    "- **Multilingual support**: For global financial markets\n",
    "\n",
    "#### 3. **Training Data & Scale**\n",
    "- **Financial corpora**: SEC filings, earnings calls, financial news, research reports\n",
    "- **Code integration**: Financial modeling code, SQL queries\n",
    "- **Structured data**: Financial statements, market data, regulatory filings\n",
    "- **Real-time data**: Market feeds, news streams\n",
    "\n",
    "#### 4. **Training Techniques**\n",
    "- **Pretraining**: Large-scale unsupervised learning on financial text\n",
    "- **Instruction tuning**: Fine-tuning on financial Q&A, analysis tasks\n",
    "- **RLHF**: Reinforcement learning from financial expert feedback\n",
    "- **Domain adaptation**: Continued pretraining on financial data\n",
    "\n",
    "#### 5. **Financial-Specific Optimizations**\n",
    "- **Numerical reasoning**: Enhanced arithmetic and financial calculation abilities\n",
    "- **Time series understanding**: Market data, financial trends\n",
    "- **Risk assessment**: Model uncertainty quantification\n",
    "- **Compliance**: Ensuring regulatory compliance in outputs\n",
    "- **Explainability**: Interpretable financial recommendations\n",
    "\n",
    "#### 6. **Production Considerations**\n",
    "- **Latency optimization**: Fast inference for real-time trading decisions\n",
    "- **Scalability**: Handle multiple concurrent financial analysis requests\n",
    "- **Security**: Protect sensitive financial information\n",
    "- **Monitoring**: Track model performance and drift in financial markets\n",
    "\n",
    "Our character-level Transformer provides the foundational understanding for these advanced systems!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16154ee",
   "metadata": {},
   "source": [
    "## 10. Conclusion\n",
    "\n",
    "Congratulations! In this notebook, you've successfully:\n",
    "\n",
    "âœ… **Built a Transformer encoder-decoder model** from scratch using Hugging Face  \n",
    "âœ… **Implemented character-level tokenization** with proper special tokens  \n",
    "âœ… **Trained on NLTK corpus** with sequence-to-sequence learning  \n",
    "âœ… **Used modern training techniques** with the Hugging Face Trainer  \n",
    "âœ… **Generated text autoregressively** with proper stopping mechanisms  \n",
    "âœ… **Evaluated model performance** with comprehensive metrics  \n",
    "âœ… **Understood the path to production LLMs** in financial applications  \n",
    "\n",
    "### Key Learnings\n",
    "\n",
    "1. **Transformer Architecture**: You've implemented the same core architecture used in GPT, BERT, and T5\n",
    "2. **Character-level Modeling**: Understanding how models can work at the most granular text level\n",
    "3. **Encoder-Decoder Design**: Experience with sequence-to-sequence learning patterns\n",
    "4. **Modern Training Stack**: Hands-on experience with Hugging Face transformers\n",
    "5. **Evaluation Techniques**: Comprehensive model assessment strategies\n",
    "6. **Scaling Insights**: Clear path from toy models to production systems\n",
    "\n",
    "### From Here to Financial LLMs\n",
    "\n",
    "Your character-level Transformer shares DNA with models like:\n",
    "- **GPT-4**: Decoder-only architecture for text generation\n",
    "- **BERT**: Encoder architecture for understanding tasks  \n",
    "- **T5**: Encoder-decoder for various NLP tasks\n",
    "- **Financial domain models**: BloombergGPT, FinBERT, etc.\n",
    "\n",
    "The principles you've learnedâ€”attention mechanisms, autoregressive generation, transformer blocksâ€”are the building blocks of all modern LLMs used in finance today.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "### Immediate Experiments\n",
    "- ðŸ”§ **Increase model size**: More layers, larger hidden dimensions\n",
    "- ðŸ“Š **Try different data**: Financial news, SEC filings, earnings transcripts\n",
    "- ðŸŽ¯ **Task-specific fine-tuning**: Financial sentiment, NER, QA\n",
    "- âš¡ **Optimization**: Mixed precision, gradient checkpointing\n",
    "\n",
    "### Advanced Projects\n",
    "- ðŸš€ **Implement GPT-style decoder-only model** for better generation\n",
    "- ðŸ“ˆ **Fine-tune on financial data** for domain adaptation\n",
    "- ðŸ” **Add retrieval mechanisms** for factual financial information\n",
    "- ðŸ›¡ï¸ **Implement safety measures** for financial advice generation\n",
    "- ðŸ“± **Deploy as API** for real-time financial analysis\n",
    "\n",
    "### Production Path\n",
    "- ðŸ—ï¸ **Scale to subword tokenization** (BPE, SentencePiece)\n",
    "- ðŸŒ **Multi-GPU training** with distributed computing\n",
    "- ðŸ“Š **Add financial-specific metrics** and evaluation frameworks\n",
    "- ðŸ”’ **Implement security** and compliance measures\n",
    "- ðŸ“ˆ **Continuous learning** from new financial data\n",
    "\n",
    "You now have the foundation to understand and build the next generation of financial AI systems! ðŸŽ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alphy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
