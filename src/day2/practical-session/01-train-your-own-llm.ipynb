{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd7b26f1",
   "metadata": {},
   "source": [
    "# Training Your Own LLM\n",
    "\n",
    "Welcome to the Day 2 practical session on training your own language model! In this notebook, we'll learn how to train a small language model from scratch using Python words as our dataset. We'll explore how language models learn to predict words and see the fundamentals of neural language modeling in action.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand the basics of language model training\n",
    "- Build a simple character-level language model from scratch\n",
    "- Optimize training using CPU and GPU resources\n",
    "- Evaluate model performance and generate text\n",
    "- Gain insights into how larger LLMs are trained\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Basic understanding of Python and PyTorch (covered in preliminaries)\n",
    "- Familiarity with the concept of language models\n",
    "- A computer with PyTorch installed (CPU or GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2131a73d",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment. We'll be using PyTorch for our neural network implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9adeeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import words\n",
    "import os\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e093fc66",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Let's create a dataset of English words. We'll use the NLTK words corpus, which contains common English words. For our simple model, we'll focus on character-level prediction rather than word-level, as it requires less computational resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e473d966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK words if not already downloaded\n",
    "try:\n",
    "    nltk.data.find('corpora/words')\n",
    "except LookupError:\n",
    "    nltk.download('words')\n",
    "\n",
    "# Get list of English words\n",
    "english_words = words.words()\n",
    "\n",
    "# Filter to get words of reasonable length (3-10 characters)\n",
    "filtered_words = [word.lower() for word in english_words if 3 <= len(word) <= 10 and word.isalpha()]\n",
    "\n",
    "print(f\"Total words in dataset: {len(filtered_words)}\")\n",
    "print(f\"Sample words: {filtered_words[:10]}\")\n",
    "\n",
    "# Create a vocabulary of all characters in our dataset\n",
    "all_characters = string.ascii_lowercase + ' '  # lowercase letters and space\n",
    "n_characters = len(all_characters)\n",
    "print(f\"Characters in vocabulary: {all_characters}\")\n",
    "print(f\"Vocabulary size: {n_characters}\")\n",
    "\n",
    "# Create dictionaries to convert between characters and indices\n",
    "char_to_idx = {char: i for i, char in enumerate(all_characters)}\n",
    "idx_to_char = {i: char for i, char in enumerate(all_characters)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cefde38",
   "metadata": {},
   "source": [
    "## 3. Creating Training Data\n",
    "\n",
    "We'll now prepare our training data. For each word, we'll create input-output pairs where:\n",
    "- Input: The characters of the word up to a certain position\n",
    "- Output: The next character in the word\n",
    "\n",
    "This way, our model will learn to predict the next character given a sequence of previous characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be533170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequence_data(word):\n",
    "    \"\"\"\n",
    "    Convert a word into a list of (input_sequence, target_character) pairs.\n",
    "    For example, \"hello\" would yield:\n",
    "    [(\"h\", \"e\"), (\"he\", \"l\"), (\"hel\", \"l\"), (\"hell\", \"o\")]\n",
    "    \"\"\"\n",
    "    sequence_pairs = []\n",
    "    for i in range(1, len(word)):\n",
    "        input_seq = word[:i]\n",
    "        target_char = word[i]\n",
    "        sequence_pairs.append((input_seq, target_char))\n",
    "    return sequence_pairs\n",
    "\n",
    "# Create training examples\n",
    "training_pairs = []\n",
    "for word in filtered_words:\n",
    "    training_pairs.extend(prepare_sequence_data(word))\n",
    "\n",
    "# Shuffle the training pairs\n",
    "random.shuffle(training_pairs)\n",
    "\n",
    "# Limit to 100,000 examples to avoid memory issues\n",
    "training_pairs = training_pairs[:100000]\n",
    "\n",
    "print(f\"Total training examples: {len(training_pairs)}\")\n",
    "print(f\"Sample training pairs: {training_pairs[:5]}\")\n",
    "\n",
    "# Function to convert a string to a tensor of character indices\n",
    "def string_to_tensor(string):\n",
    "    tensor = torch.zeros(len(string), 1, n_characters)\n",
    "    for i, char in enumerate(string):\n",
    "        index = char_to_idx.get(char, char_to_idx[' '])  # Default to space if char not found\n",
    "        tensor[i][0][index] = 1\n",
    "    return tensor\n",
    "\n",
    "# Function to convert a character to a tensor (one-hot encoding)\n",
    "def char_to_tensor(char):\n",
    "    tensor = torch.zeros(1, n_characters)\n",
    "    index = char_to_idx.get(char, char_to_idx[' '])  # Default to space if char not found\n",
    "    tensor[0][index] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e14136",
   "metadata": {},
   "source": [
    "## 4. Building the Model\n",
    "\n",
    "Now, let's build our character-level language model. We'll use a simple recurrent neural network (RNN) with GRU (Gated Recurrent Unit) cells, which are good at capturing sequential patterns in data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb369c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers=1):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # GRU layer - processes input sequences and maintains hidden state\n",
    "        self.gru = nn.GRU(input_size, hidden_size, n_layers)\n",
    "        \n",
    "        # Output layer - transforms hidden state to character probabilities\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "        # Softmax layer - converts output to probabilities\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "    \n",
    "    def forward(self, input_tensor, hidden_state):\n",
    "        # Input shape: (seq_len, batch_size, input_size)\n",
    "        output, hidden = self.gru(input_tensor, hidden_state)\n",
    "        \n",
    "        # Take the last output from the sequence\n",
    "        output = self.decoder(output[-1])\n",
    "        \n",
    "        # Apply softmax to get probabilities\n",
    "        output = self.softmax(output)\n",
    "        \n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        # Initialize hidden state with zeros\n",
    "        return torch.zeros(self.n_layers, batch_size, self.hidden_size, device=device)\n",
    "    \n",
    "    def get_prediction(self, output):\n",
    "        # Get the index of the most likely character\n",
    "        _, top_index = output.topk(1)\n",
    "        return top_index.item()\n",
    "\n",
    "# Initialize model\n",
    "n_hidden = 128\n",
    "n_layers = 2\n",
    "model = CharRNN(n_characters, n_hidden, n_characters, n_layers).to(device)\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3685887c",
   "metadata": {},
   "source": [
    "## 5. Training Functions\n",
    "\n",
    "Now, we'll define functions to train our model. We'll use the negative log likelihood loss (NLLLoss) since our model outputs log probabilities with a LogSoftmax layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efe2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(input_tensor, target_tensor, model, optimizer, criterion):\n",
    "    # Initialize hidden state\n",
    "    hidden = model.init_hidden()\n",
    "    \n",
    "    # Zero the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    output, hidden = model(input_tensor, hidden)\n",
    "    \n",
    "    # Calculate loss\n",
    "    loss = criterion(output, target_tensor)\n",
    "    \n",
    "    # Backward pass and optimize\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return output, loss.item()\n",
    "\n",
    "def train_model(model, training_pairs, n_epochs=10, learning_rate=0.005, print_every=1000):\n",
    "    # Initialize optimizer and loss function\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    \n",
    "    # Track progress\n",
    "    all_losses = []\n",
    "    total_loss = 0\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        epoch_loss = 0\n",
    "        \n",
    "        for i, (input_seq, target_char) in enumerate(training_pairs, 1):\n",
    "            # Convert input and target to tensors\n",
    "            input_tensor = string_to_tensor(input_seq).to(device)\n",
    "            target_index = torch.tensor([char_to_idx.get(target_char, char_to_idx[' '])], device=device)\n",
    "            \n",
    "            # Train on this pair\n",
    "            output, loss = train_step(input_tensor, target_index, model, optimizer, criterion)\n",
    "            total_loss += loss\n",
    "            epoch_loss += loss\n",
    "            \n",
    "            # Print progress\n",
    "            if i % print_every == 0:\n",
    "                avg_loss = total_loss / print_every\n",
    "                all_losses.append(avg_loss)\n",
    "                total_loss = 0\n",
    "                \n",
    "                # Calculate elapsed time\n",
    "                elapsed = time.time() - start_time\n",
    "                \n",
    "                # Get prediction\n",
    "                _, top_index = output.topk(1)\n",
    "                predicted_char = idx_to_char[top_index.item()]\n",
    "                target_char_actual = idx_to_char[target_index.item()]\n",
    "                \n",
    "                print(f\"Epoch {epoch}/{n_epochs} | Step {i}/{len(training_pairs)} | Loss: {avg_loss:.4f} | Time: {elapsed:.2f}s\")\n",
    "                print(f\"Input: '{input_seq}' | Target: '{target_char_actual}' | Predicted: '{predicted_char}'\")\n",
    "                print(\"-\" * 50)\n",
    "        \n",
    "        print(f\"Epoch {epoch} completed | Average loss: {epoch_loss/len(training_pairs):.4f}\")\n",
    "    \n",
    "    return all_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7684536b",
   "metadata": {},
   "source": [
    "## 6. Training the Model\n",
    "\n",
    "Now, let's train our model! We'll train for a few epochs and track the loss over time. This might take a while depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "n_epochs = 3  # Start with a small number of epochs\n",
    "learning_rate = 0.001\n",
    "print_every = 1000\n",
    "\n",
    "# Start training\n",
    "losses = train_model(model, training_pairs, n_epochs, learning_rate, print_every)\n",
    "\n",
    "# Plot the loss over time\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Steps (x' + str(print_every) + ')')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd167bba",
   "metadata": {},
   "source": [
    "## 7. Optimizing Training for CPU and GPU\n",
    "\n",
    "Depending on your hardware, you might want to optimize your training process differently. Let's explore some strategies for both CPU and GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1775c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_training(device_type=\"cpu\"):\n",
    "    \"\"\"\n",
    "    Demonstrate optimization techniques for different hardware.\n",
    "    \"\"\"\n",
    "    print(f\"Optimization strategies for {device_type.upper()} training:\")\n",
    "    \n",
    "    if device_type == \"cpu\":\n",
    "        print(\"1. Use smaller batch sizes to avoid memory issues\")\n",
    "        print(\"2. Reduce model size (fewer layers, smaller hidden dimensions)\")\n",
    "        print(\"3. Use data parallelism with multiple CPU cores\")\n",
    "        print(\"4. Consider mixed precision training with bfloat16 on newer CPUs\")\n",
    "        print(\"5. Ensure proper vectorization of operations\")\n",
    "        \n",
    "        # Example: Setting number of threads for CPU parallelism\n",
    "        torch.set_num_threads(os.cpu_count())\n",
    "        print(f\"Set PyTorch to use {os.cpu_count()} CPU threads\")\n",
    "        \n",
    "    elif device_type == \"gpu\":\n",
    "        print(\"1. Use larger batch sizes to fully utilize GPU memory\")\n",
    "        print(\"2. Enable automatic mixed precision (AMP) for faster computation\")\n",
    "        print(\"3. Use gradient accumulation for effectively larger batches\")\n",
    "        print(\"4. Ensure data is pre-loaded and prefetched\")\n",
    "        print(\"5. Monitor GPU utilization and memory usage\")\n",
    "        \n",
    "        # Example: Setting up mixed precision training\n",
    "        if torch.cuda.is_available():\n",
    "            print(\"Example of enabling automatic mixed precision:\")\n",
    "            print(\"   scaler = torch.cuda.amp.GradScaler()\")\n",
    "            print(\"   with torch.cuda.amp.autocast():\")\n",
    "            print(\"       outputs = model(inputs)\")\n",
    "    \n",
    "    # Common optimizations\n",
    "    print(\"\\nCommon optimizations for both CPU and GPU:\")\n",
    "    print(\"1. Use DataLoader with appropriate num_workers\")\n",
    "    print(\"2. Implement early stopping to avoid overfitting\")\n",
    "    print(\"3. Use learning rate scheduling\")\n",
    "    print(\"4. Profile your code to identify bottlenecks\")\n",
    "    print(\"5. Reduce Python overhead by batching operations\")\n",
    "\n",
    "# Show optimization strategies based on available hardware\n",
    "optimize_training(\"gpu\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82002921",
   "metadata": {},
   "source": [
    "## 8. Generating Text with Our Model\n",
    "\n",
    "Now that we've trained our model, let's use it to generate some text. We'll start with a seed string and let the model predict subsequent characters one by one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21a0ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, seed_string, max_length=50, temperature=0.8):\n",
    "    \"\"\"\n",
    "    Generate text starting with a seed string.\n",
    "    Temperature controls randomness: higher means more random outputs.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        # Initialize hidden state\n",
    "        hidden = model.init_hidden()\n",
    "        \n",
    "        # Prepare input tensor from seed string\n",
    "        input_tensor = string_to_tensor(seed_string).to(device)\n",
    "        \n",
    "        # Initialize output with the seed string\n",
    "        output_string = seed_string\n",
    "        \n",
    "        # Generate characters one by one\n",
    "        for i in range(max_length):\n",
    "            # Forward pass\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            \n",
    "            # Apply temperature to output probabilities\n",
    "            output_dist = output.div(temperature).exp()\n",
    "            \n",
    "            # Sample from the distribution\n",
    "            top_char_index = torch.multinomial(output_dist, 1)[0]\n",
    "            \n",
    "            # Get the corresponding character\n",
    "            predicted_char = idx_to_char[top_char_index.item()]\n",
    "            \n",
    "            # Add the predicted character to the output string\n",
    "            output_string += predicted_char\n",
    "            \n",
    "            # Update input tensor for next prediction\n",
    "            next_char_tensor = torch.zeros(1, 1, n_characters, device=device)\n",
    "            next_char_tensor[0, 0, top_char_index] = 1\n",
    "            input_tensor = torch.cat([input_tensor, next_char_tensor], 0)\n",
    "        \n",
    "        return output_string\n",
    "\n",
    "# Generate text with different seed strings\n",
    "seed_strings = ['fin', 'inv', 'tra', 'mar', 'ban']\n",
    "temperatures = [0.5, 0.8, 1.0]\n",
    "\n",
    "print(\"Generated text samples:\")\n",
    "for seed in seed_strings:\n",
    "    print(f\"\\nSeed: '{seed}'\")\n",
    "    for temp in temperatures:\n",
    "        generated = generate_text(model, seed, max_length=20, temperature=temp)\n",
    "        print(f\"  Temperature {temp}: '{generated}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72844745",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "\n",
    "Let's evaluate our model's performance. We'll calculate perplexity, which is a common metric for language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4058cdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_pairs, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a set of evaluation pairs.\n",
    "    Returns the average loss and perplexity.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients\n",
    "        for input_seq, target_char in eval_pairs:\n",
    "            # Convert input and target to tensors\n",
    "            input_tensor = string_to_tensor(input_seq).to(device)\n",
    "            target_index = torch.tensor([char_to_idx.get(target_char, char_to_idx[' '])], device=device)\n",
    "            \n",
    "            # Initialize hidden state\n",
    "            hidden = model.init_hidden()\n",
    "            \n",
    "            # Forward pass\n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(output, target_index)\n",
    "            total_loss += loss.item()\n",
    "    \n",
    "    # Calculate average loss and perplexity\n",
    "    avg_loss = total_loss / len(eval_pairs)\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return avg_loss, perplexity\n",
    "\n",
    "# Create evaluation set (using part of the training set for simplicity)\n",
    "eval_pairs = training_pairs[-1000:]  # Use last 1000 examples\n",
    "\n",
    "# Evaluate the model\n",
    "criterion = nn.NLLLoss()\n",
    "avg_loss, perplexity = evaluate_model(model, eval_pairs, criterion)\n",
    "\n",
    "print(f\"Evaluation results:\")\n",
    "print(f\"Average loss: {avg_loss:.4f}\")\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d91541",
   "metadata": {},
   "source": [
    "## 10. Scaling Up: From Small Models to LLMs\n",
    "\n",
    "We've just trained a very simple character-level language model. Let's discuss how this relates to large language models (LLMs) used in finance and what it would take to scale up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec94f67",
   "metadata": {},
   "source": [
    "### Comparison: Our Simple Model vs. LLMs\n",
    "\n",
    "| Feature | Our Simple Model | Large Language Models (LLMs) |\n",
    "|---------|------------------|------------------------------|\n",
    "| Parameters | ~100K | Billions to trillions |\n",
    "| Architecture | Simple GRU | Transformer-based (attention mechanisms) |\n",
    "| Training Data | ~100K examples | Trillions of tokens |\n",
    "| Training Time | Minutes on a laptop | Weeks/months on thousands of GPUs |\n",
    "| Context Length | Few characters | Thousands of tokens |\n",
    "| Capabilities | Character prediction | Complex reasoning, generation, understanding |\n",
    "| Applications | Word completion | Financial analysis, report generation, code writing |\n",
    "| Hardware | CPU/Single GPU | Distributed systems, specialized hardware |\n",
    "\n",
    "This comparison highlights the massive scale difference between our toy model and production LLMs. However, the fundamental principles remain similar: predicting the next token based on previous context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25590329",
   "metadata": {},
   "source": [
    "### Scaling to Financial Applications\n",
    "\n",
    "To build LLMs suitable for finance, we would need to:\n",
    "\n",
    "1. **Increase model size**: More layers, larger hidden dimensions, more parameters\n",
    "2. **Use transformer architecture**: Replace our GRU with attention-based transformers\n",
    "3. **Train on financial data**: SEC filings, financial news, analyst reports, market data\n",
    "4. **Implement specialized training techniques**:\n",
    "   - Reinforcement Learning from Human Feedback (RLHF)\n",
    "   - Instruction fine-tuning\n",
    "   - Domain adaptation\n",
    "5. **Optimize for specific financial tasks**:\n",
    "   - Financial sentiment analysis\n",
    "   - Market prediction\n",
    "   - Risk assessment\n",
    "   - Regulatory compliance\n",
    "\n",
    "The skills you've learned in this notebook provide the foundation for understanding these more complex models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16154ee",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we've learned:\n",
    "\n",
    "- How to build and train a simple character-level language model\n",
    "- The process of preparing data for language model training\n",
    "- Techniques for optimizing training on different hardware\n",
    "- Methods for generating text with our trained model\n",
    "- The relationship between simple models and large language models\n",
    "\n",
    "This foundational knowledge helps understand how larger models like those used in finance are trained and operated. While our model is very simple, the core principles scale up to the most sophisticated language models used in the financial industry today.\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- Experiment with different model architectures (LSTM, Transformer)\n",
    "- Try training on financial text data instead of simple words\n",
    "- Implement more advanced optimization techniques\n",
    "- Explore transfer learning by fine-tuning pre-trained models\n",
    "- Apply these concepts to specific financial use cases"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
