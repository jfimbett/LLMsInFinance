{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2b4f7bc",
   "metadata": {},
   "source": [
    "# LLMs Under the Hood: Understanding Hyperparameters and Embeddings\n",
    "\n",
    "Welcome to the second practical session of Day 2! In this notebook, we'll explore the inner workings of Large Language Models, focusing on hyperparameters like temperature and embeddings. By the end of this session, you'll have a deeper understanding of how these components affect model behavior in financial applications.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Understand key LLM hyperparameters and their effects\n",
    "- Explore embedding spaces and their role in semantic understanding\n",
    "- Experiment with temperature, top-k, and top-p sampling\n",
    "- Visualize embeddings for financial terminology\n",
    "- Learn to optimize parameters for different financial tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12349a99",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "First, let's import the necessary libraries. We'll be using the Hugging Face Transformers library to interact with pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85969a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import time\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Load environment variables for API keys (if needed)\n",
    "load_dotenv()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check which models are available locally and their sizes\n",
    "print(\"Checking for locally available models...\")\n",
    "transformers_cache = os.path.join(os.path.expanduser(\"~\"), \".cache\", \"huggingface\", \"transformers\")\n",
    "if os.path.exists(transformers_cache):\n",
    "    print(f\"Models in cache:\")\n",
    "    for model_dir in os.listdir(transformers_cache):\n",
    "        model_path = os.path.join(transformers_cache, model_dir)\n",
    "        if os.path.isdir(model_path):\n",
    "            size_bytes = sum(os.path.getsize(os.path.join(dirpath, filename)) \n",
    "                        for dirpath, _, filenames in os.walk(model_path) \n",
    "                        for filename in filenames)\n",
    "            size_mb = size_bytes / (1024 * 1024)\n",
    "            print(f\"  - {model_dir}: {size_mb:.2f} MB\")\n",
    "else:\n",
    "    print(\"No models found in local cache.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a6577e",
   "metadata": {},
   "source": [
    "## 2. Loading a Small LLM\n",
    "\n",
    "For this practical session, we'll use a smaller language model that can run on most hardware. GPT-2 is a good option as it's relatively small but still capable of generating coherent text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86637b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # Can be changed to \"gpt2-medium\" or \"gpt2-large\" if you have more GPU memory\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "\n",
    "# Set padding token to be the same as the EOS token\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Print model information\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Vocabulary size: {len(tokenizer)}\")\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "print(f\"Layers: {len(model.transformer.h)}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")\n",
    "print(f\"Max position embeddings: {model.config.n_positions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3587c3de",
   "metadata": {},
   "source": [
    "## 3. Understanding Generation Hyperparameters\n",
    "\n",
    "LLMs use several hyperparameters to control text generation. Let's explore the most important ones:\n",
    "\n",
    "- **Temperature**: Controls randomness in generation\n",
    "- **Top-k**: Limits the selection to the k most probable next tokens\n",
    "- **Top-p (nucleus sampling)**: Dynamically limits the selection to the smallest set of tokens whose cumulative probability exceeds p\n",
    "- **Repetition penalty**: Discourages repetition of the same tokens\n",
    "- **Max length**: Maximum number of tokens to generate\n",
    "\n",
    "Let's experiment with these parameters and see how they affect generation in financial contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8124b4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, temperature=1.0, top_k=50, top_p=1.0, \n",
    "                 repetition_penalty=1.0, max_length=100, num_return_sequences=1):\n",
    "    \"\"\"\n",
    "    Generate text using a language model with specified parameters.\n",
    "    \n",
    "    Parameters:\n",
    "    - prompt: The starting text prompt\n",
    "    - temperature: Controls randomness (higher = more random)\n",
    "    - top_k: Number of highest probability tokens to consider\n",
    "    - top_p: Cumulative probability threshold for nucleus sampling\n",
    "    - repetition_penalty: Penalty for repeating tokens\n",
    "    - max_length: Maximum number of tokens to generate\n",
    "    - num_return_sequences: Number of different sequences to generate\n",
    "    \n",
    "    Returns:\n",
    "    - Generated text sequences\n",
    "    \"\"\"\n",
    "    # Encode the prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Record start time\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_k=top_k,\n",
    "        top_p=top_p,\n",
    "        repetition_penalty=repetition_penalty,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=num_return_sequences,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Record end time\n",
    "    end_time = time.time()\n",
    "    generation_time = end_time - start_time\n",
    "    \n",
    "    # Decode the outputs\n",
    "    generated_texts = []\n",
    "    for output in outputs:\n",
    "        generated_text = tokenizer.decode(output, skip_special_tokens=True)\n",
    "        generated_texts.append(generated_text)\n",
    "    \n",
    "    return generated_texts, generation_time\n",
    "\n",
    "# Define financial prompts for testing\n",
    "financial_prompts = [\n",
    "    \"The stock market today experienced\",\n",
    "    \"Analysts predict that interest rates will\",\n",
    "    \"The company's quarterly earnings report showed\",\n",
    "    \"Investors should consider the following factors before\",\n",
    "    \"The Federal Reserve announced that\"\n",
    "]\n",
    "\n",
    "# Let's test with different temperatures\n",
    "temperatures = [0.2, 0.7, 1.5]\n",
    "test_prompt = financial_prompts[0]\n",
    "\n",
    "print(f\"Testing temperature effects with prompt: '{test_prompt}'\")\n",
    "for temp in temperatures:\n",
    "    print(f\"\\nTemperature = {temp}:\")\n",
    "    texts, time_taken = generate_text(test_prompt, temperature=temp, max_length=50)\n",
    "    for i, text in enumerate(texts):\n",
    "        print(f\"  Generated text {i+1}: {text}\")\n",
    "    print(f\"  Generation time: {time_taken:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85fd0bb",
   "metadata": {},
   "source": [
    "## 4. Hyperparameter Comparison\n",
    "\n",
    "Let's create a systematic comparison of how different hyperparameter values affect text generation for financial content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4200b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_hyperparameters(prompt, param_name, param_values, fixed_params=None):\n",
    "    \"\"\"\n",
    "    Compare the effect of varying a single hyperparameter.\n",
    "    \"\"\"\n",
    "    if fixed_params is None:\n",
    "        fixed_params = {}\n",
    "    \n",
    "    results = []\n",
    "    for value in param_values:\n",
    "        params = {**fixed_params, param_name: value}\n",
    "        texts, gen_time = generate_text(prompt, **params, max_length=50)\n",
    "        results.append({\n",
    "            'value': value,\n",
    "            'text': texts[0],\n",
    "            'time': gen_time\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Fixed parameters for comparison\n",
    "fixed_params = {\n",
    "    'temperature': 0.7,\n",
    "    'top_k': 50,\n",
    "    'top_p': 0.9,\n",
    "    'repetition_penalty': 1.1\n",
    "}\n",
    "\n",
    "# Compare top-k values\n",
    "top_k_values = [5, 20, 50]\n",
    "top_k_results = compare_hyperparameters(\n",
    "    \"The investment strategy should focus on\",\n",
    "    'top_k',\n",
    "    top_k_values,\n",
    "    {k: v for k, v in fixed_params.items() if k != 'top_k'}\n",
    ")\n",
    "\n",
    "# Compare top-p values\n",
    "top_p_values = [0.5, 0.8, 0.95]\n",
    "top_p_results = compare_hyperparameters(\n",
    "    \"The investment strategy should focus on\",\n",
    "    'top_p',\n",
    "    top_p_values,\n",
    "    {k: v for k, v in fixed_params.items() if k != 'top_p'}\n",
    ")\n",
    "\n",
    "# Compare repetition penalties\n",
    "rep_penalty_values = [1.0, 1.2, 1.5]\n",
    "rep_penalty_results = compare_hyperparameters(\n",
    "    \"The future outlook for the company is promising because\",\n",
    "    'repetition_penalty',\n",
    "    rep_penalty_values,\n",
    "    {k: v for k, v in fixed_params.items() if k != 'repetition_penalty'}\n",
    ")\n",
    "\n",
    "# Display the results in a table format\n",
    "def display_results(results, param_name):\n",
    "    df = pd.DataFrame(results)\n",
    "    df.columns = [param_name, 'Generated Text', 'Time (s)']\n",
    "    return df\n",
    "\n",
    "print(\"Top-k comparison:\")\n",
    "display(display_results(top_k_results, 'Top-k'))\n",
    "\n",
    "print(\"\\nTop-p comparison:\")\n",
    "display(display_results(top_p_results, 'Top-p'))\n",
    "\n",
    "print(\"\\nRepetition penalty comparison:\")\n",
    "display(display_results(rep_penalty_results, 'Repetition Penalty'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4223581b",
   "metadata": {},
   "source": [
    "## 5. Recommended Hyperparameters for Financial Tasks\n",
    "\n",
    "Different financial tasks require different hyperparameter settings. Let's define some recommended configurations for common financial use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530517c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameter profiles for different financial tasks\n",
    "task_profiles = {\n",
    "    'market_analysis': {\n",
    "        'temperature': 0.7,\n",
    "        'top_k': 40,\n",
    "        'top_p': 0.9,\n",
    "        'repetition_penalty': 1.1,\n",
    "        'description': 'Balanced creativity and coherence for analyzing market conditions'\n",
    "    },\n",
    "    'risk_assessment': {\n",
    "        'temperature': 0.4,\n",
    "        'top_k': 20,\n",
    "        'top_p': 0.85,\n",
    "        'repetition_penalty': 1.2,\n",
    "        'description': 'Lower randomness for more conservative and focused risk analysis'\n",
    "    },\n",
    "    'creative_strategy': {\n",
    "        'temperature': 0.9,\n",
    "        'top_k': 50,\n",
    "        'top_p': 0.95,\n",
    "        'repetition_penalty': 1.05,\n",
    "        'description': 'Higher creativity for innovative investment strategies'\n",
    "    },\n",
    "    'regulatory_compliance': {\n",
    "        'temperature': 0.3,\n",
    "        'top_k': 10,\n",
    "        'top_p': 0.8,\n",
    "        'repetition_penalty': 1.3,\n",
    "        'description': 'Minimal randomness for precise regulatory language'\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create a table of recommended profiles\n",
    "profile_data = []\n",
    "for task, profile in task_profiles.items():\n",
    "    profile_data.append({\n",
    "        'Task': task,\n",
    "        'Temperature': profile['temperature'],\n",
    "        'Top-k': profile['top_k'],\n",
    "        'Top-p': profile['top_p'],\n",
    "        'Repetition Penalty': profile['repetition_penalty'],\n",
    "        'Description': profile['description']\n",
    "    })\n",
    "\n",
    "profile_df = pd.DataFrame(profile_data)\n",
    "display(profile_df)\n",
    "\n",
    "# Test a few of these profiles on financial prompts\n",
    "test_prompt = \"The company's quarterly financial results indicate\"\n",
    "\n",
    "print(\"Testing task-specific hyperparameter profiles:\")\n",
    "for task, profile in list(task_profiles.items())[:2]:  # Test first two profiles\n",
    "    print(f\"\\n{task.replace('_', ' ').title()} Profile:\")\n",
    "    params = {k: v for k, v in profile.items() if k != 'description'}\n",
    "    texts, _ = generate_text(test_prompt, **params, max_length=75)\n",
    "    print(f\"  {texts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590573e5",
   "metadata": {},
   "source": [
    "## 6. Understanding Word Embeddings\n",
    "\n",
    "Embeddings are dense vector representations of words or tokens that capture semantic meaning. They are a fundamental component of all modern LLMs. Let's explore how embeddings work and visualize them for financial terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05211c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model suitable for embeddings\n",
    "embedding_model_name = \"distilbert-base-uncased\"  # Smaller model for embeddings\n",
    "embedding_tokenizer = AutoTokenizer.from_pretrained(embedding_model_name)\n",
    "embedding_model = AutoModel.from_pretrained(embedding_model_name).to(device)\n",
    "\n",
    "def get_embeddings(texts):\n",
    "    \"\"\"Get embeddings for a list of texts.\"\"\"\n",
    "    # Tokenize inputs\n",
    "    inputs = embedding_tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True, max_length=128).to(device)\n",
    "    \n",
    "    # Get model outputs\n",
    "    with torch.no_grad():\n",
    "        outputs = embedding_model(**inputs)\n",
    "    \n",
    "    # Use CLS token embeddings (first token of each sequence)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Define financial terms to visualize\n",
    "financial_terms = [\n",
    "    # Market terms\n",
    "    \"bull market\", \"bear market\", \"volatility\", \"liquidity\", \"market cap\",\n",
    "    # Asset classes\n",
    "    \"stocks\", \"bonds\", \"commodities\", \"cryptocurrency\", \"real estate\",\n",
    "    # Financial metrics\n",
    "    \"earnings\", \"revenue\", \"dividend\", \"P/E ratio\", \"cash flow\",\n",
    "    # Banking terms\n",
    "    \"interest rate\", \"loan\", \"deposit\", \"mortgage\", \"credit\",\n",
    "    # Investment terms\n",
    "    \"portfolio\", \"diversification\", \"hedge\", \"risk\", \"return\"\n",
    "]\n",
    "\n",
    "# Get embeddings for financial terms\n",
    "financial_embeddings = get_embeddings(financial_terms)\n",
    "\n",
    "# Use t-SNE to reduce dimensions for visualization\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "reduced_embeddings = tsne.fit_transform(financial_embeddings)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "embedding_df = pd.DataFrame({\n",
    "    'term': financial_terms,\n",
    "    'x': reduced_embeddings[:, 0],\n",
    "    'y': reduced_embeddings[:, 1]\n",
    "})\n",
    "\n",
    "# Define categories for coloring\n",
    "categories = {\n",
    "    'Market': [\"bull market\", \"bear market\", \"volatility\", \"liquidity\", \"market cap\"],\n",
    "    'Assets': [\"stocks\", \"bonds\", \"commodities\", \"cryptocurrency\", \"real estate\"],\n",
    "    'Metrics': [\"earnings\", \"revenue\", \"dividend\", \"P/E ratio\", \"cash flow\"],\n",
    "    'Banking': [\"interest rate\", \"loan\", \"deposit\", \"mortgage\", \"credit\"],\n",
    "    'Investment': [\"portfolio\", \"diversification\", \"hedge\", \"risk\", \"return\"]\n",
    "}\n",
    "\n",
    "# Add category column\n",
    "embedding_df['category'] = 'Other'\n",
    "for category, terms in categories.items():\n",
    "    embedding_df.loc[embedding_df['term'].isin(terms), 'category'] = category\n",
    "\n",
    "# Plot the embeddings\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(data=embedding_df, x='x', y='y', hue='category', s=100)\n",
    "\n",
    "# Add labels to each point\n",
    "for idx, row in embedding_df.iterrows():\n",
    "    plt.text(row['x']+0.02, row['y']+0.02, row['term'], fontsize=9)\n",
    "\n",
    "plt.title('2D t-SNE Visualization of Financial Term Embeddings')\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede4f457",
   "metadata": {},
   "source": [
    "## 7. Embeddings Similarity for Financial Terms\n",
    "\n",
    "Let's explore how embeddings capture semantic relationships between financial terms. We'll calculate similarities between terms to see which ones the model considers most related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0668fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(embeddings):\n",
    "    \"\"\"Calculate cosine similarity between all pairs of embeddings.\"\"\"\n",
    "    # Normalize embeddings\n",
    "    normalized = embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)\n",
    "    # Calculate similarity matrix\n",
    "    similarity_matrix = np.dot(normalized, normalized.T)\n",
    "    return similarity_matrix\n",
    "\n",
    "# Calculate similarity matrix for financial terms\n",
    "similarity_matrix = calculate_similarity(financial_embeddings)\n",
    "\n",
    "# Create a DataFrame for the similarity matrix\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=financial_terms, columns=financial_terms)\n",
    "\n",
    "# Plot the similarity matrix as a heatmap\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(similarity_df, annot=False, cmap='viridis', vmin=0, vmax=1)\n",
    "plt.title('Cosine Similarity Between Financial Terms')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Find most similar pairs\n",
    "similarities = []\n",
    "for i in range(len(financial_terms)):\n",
    "    for j in range(i+1, len(financial_terms)):\n",
    "        similarities.append({\n",
    "            'term1': financial_terms[i],\n",
    "            'term2': financial_terms[j],\n",
    "            'similarity': similarity_matrix[i, j]\n",
    "        })\n",
    "\n",
    "# Sort by similarity (descending)\n",
    "similarities.sort(key=lambda x: x['similarity'], reverse=True)\n",
    "\n",
    "# Display top 10 most similar pairs\n",
    "top_similarities = pd.DataFrame(similarities[:10])\n",
    "print(\"Top 10 most similar financial term pairs:\")\n",
    "display(top_similarities)\n",
    "\n",
    "# Find most dissimilar pairs\n",
    "similarities.sort(key=lambda x: x['similarity'])\n",
    "bottom_similarities = pd.DataFrame(similarities[:10])\n",
    "print(\"\\nTop 10 most dissimilar financial term pairs:\")\n",
    "display(bottom_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f67a41",
   "metadata": {},
   "source": [
    "## 8. Prompting Strategies Based on Embeddings\n",
    "\n",
    "We can use our understanding of embeddings to create more effective prompts for financial tasks. Let's explore a few strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcb346b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_related_terms(term, embeddings, terms, n=5):\n",
    "    \"\"\"Find the most related terms to a given term based on embedding similarity.\"\"\"\n",
    "    # Get index of the term\n",
    "    term_idx = terms.index(term)\n",
    "    \n",
    "    # Get similarities to the term\n",
    "    similarities = [(terms[i], embeddings[term_idx, i]) for i in range(len(terms)) if i != term_idx]\n",
    "    \n",
    "    # Sort by similarity (descending)\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return top n most similar terms\n",
    "    return similarities[:n]\n",
    "\n",
    "# Find terms related to \"risk\"\n",
    "risk_related = find_related_terms(\"risk\", similarity_matrix, financial_terms)\n",
    "print(\"Terms most related to 'risk':\")\n",
    "for term, similarity in risk_related:\n",
    "    print(f\"  {term}: {similarity:.4f}\")\n",
    "\n",
    "# Create a prompt enrichment function using related terms\n",
    "def enrich_prompt(base_prompt, target_concept, related_terms, n_terms=3):\n",
    "    \"\"\"Enrich a prompt by incorporating related terms to guide the model.\"\"\"\n",
    "    terms_to_use = [term for term, _ in related_terms[:n_terms]]\n",
    "    terms_str = \", \".join(terms_to_use)\n",
    "    \n",
    "    enriched_prompt = f\"{base_prompt} Consider aspects like {target_concept}, {terms_str}.\"\n",
    "    return enriched_prompt\n",
    "\n",
    "# Create a standard prompt\n",
    "standard_prompt = \"Analyze the company's financial health.\"\n",
    "\n",
    "# Create an enriched prompt focusing on risk\n",
    "risk_enriched_prompt = enrich_prompt(standard_prompt, \"risk\", risk_related)\n",
    "\n",
    "# Compare the outputs\n",
    "print(\"\\nStandard prompt:\")\n",
    "print(f\"  '{standard_prompt}'\")\n",
    "standard_output, _ = generate_text(standard_prompt, temperature=0.7, max_length=100)\n",
    "print(f\"  Output: '{standard_output[0]}'\")\n",
    "\n",
    "print(\"\\nEnriched prompt (risk-focused):\")\n",
    "print(f\"  '{risk_enriched_prompt}'\")\n",
    "enriched_output, _ = generate_text(risk_enriched_prompt, temperature=0.7, max_length=100)\n",
    "print(f\"  Output: '{enriched_output[0]}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b37946",
   "metadata": {},
   "source": [
    "## 9. Token-level Probabilities and Confidence\n",
    "\n",
    "Let's examine the token-level probabilities that the model assigns during generation. This gives insight into how confident the model is about its predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5131f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_probabilities(prompt, temperature=1.0, max_length=50):\n",
    "    \"\"\"Generate text and return the probabilities for each generated token.\"\"\"\n",
    "    # Tokenize the prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Store generated tokens and their probabilities\n",
    "    generated_tokens = []\n",
    "    token_probs = []\n",
    "    \n",
    "    # Start with the input sequence\n",
    "    current_input = input_ids\n",
    "    \n",
    "    # Generate one token at a time\n",
    "    for _ in range(max_length - len(input_ids[0])):\n",
    "        with torch.no_grad():\n",
    "            # Get model outputs\n",
    "            outputs = model(current_input)\n",
    "            \n",
    "            # Get logits (unnormalized log probabilities) for the last position\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Convert to probabilities with softmax\n",
    "            probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "            \n",
    "            # Sample from the probability distribution\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            \n",
    "            # Get the probability of the selected token\n",
    "            token_prob = probs[0, next_token.item()].item()\n",
    "            \n",
    "            # Add to results\n",
    "            generated_tokens.append(next_token.item())\n",
    "            token_probs.append(token_prob)\n",
    "            \n",
    "            # Update the input sequence\n",
    "            current_input = torch.cat([current_input, next_token], dim=1)\n",
    "            \n",
    "            # Check if we've generated an end-of-sequence token\n",
    "            if next_token.item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    # Convert tokens back to text\n",
    "    generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "    generated_tokens_text = [tokenizer.decode([token]) for token in generated_tokens]\n",
    "    \n",
    "    return generated_text, generated_tokens_text, token_probs\n",
    "\n",
    "# Test with a financial prompt\n",
    "financial_prompt = \"The Federal Reserve's decision to raise interest rates will affect\"\n",
    "generated_text, tokens, probs = generate_with_probabilities(financial_prompt, temperature=0.8, max_length=30)\n",
    "\n",
    "# Display the results\n",
    "print(f\"Prompt: '{financial_prompt}'\")\n",
    "print(f\"Generated text: '{generated_text}'\")\n",
    "\n",
    "# Create a DataFrame to visualize token probabilities\n",
    "token_df = pd.DataFrame({\n",
    "    'Token': tokens,\n",
    "    'Probability': probs\n",
    "})\n",
    "\n",
    "# Sort by probability to see most/least confident predictions\n",
    "token_df_sorted = token_df.sort_values('Probability', ascending=False)\n",
    "\n",
    "print(\"\\nTokens sorted by model confidence (probability):\")\n",
    "display(token_df_sorted)\n",
    "\n",
    "# Plot the token probabilities\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(range(len(probs)), probs)\n",
    "plt.xticks(range(len(probs)), tokens, rotation=90)\n",
    "plt.xlabel('Generated Token')\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Model Confidence for Each Generated Token')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot a heatmap of the text with confidence levels\n",
    "def plot_text_confidence(text, tokens, probs):\n",
    "    \"\"\"Plot the generated text with color-coded confidence levels.\"\"\"\n",
    "    # Create HTML with background color based on confidence\n",
    "    html_parts = []\n",
    "    for token, prob in zip(tokens, probs):\n",
    "        # Map probability to a color (red to green)\n",
    "        r = int(255 * (1 - prob))\n",
    "        g = int(255 * prob)\n",
    "        b = 0\n",
    "        color = f\"#{r:02x}{g:02x}{b:02x}\"\n",
    "        \n",
    "        # Add token with background color\n",
    "        html_parts.append(f'<span style=\"background-color: {color}; padding: 2px; margin: 1px; color: white;\">{token}</span>')\n",
    "    \n",
    "    # Combine all parts\n",
    "    html = ''.join(html_parts)\n",
    "    \n",
    "    # Display the HTML\n",
    "    display(HTML(f\"<p>Generated text with confidence highlighting (green = high confidence, red = low confidence):</p><p>{html}</p>\"))\n",
    "\n",
    "# Display the text with confidence highlighting\n",
    "plot_text_confidence(generated_text, tokens, probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a822837",
   "metadata": {},
   "source": [
    "## 10. Hyperparameter Recommendations for Financial Applications\n",
    "\n",
    "Based on our experiments, let's summarize the hyperparameter recommendations for different financial applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9eca22d",
   "metadata": {},
   "source": [
    "### Recommended Hyperparameter Settings\n",
    "\n",
    "| Task | Description | Temperature | Top-k | Top-p | Repetition Penalty |\n",
    "|------|-------------|------------|-------|-------|-------------------|\n",
    "| **Financial Reports** | Formal, factual summaries | 0.3-0.4 | 10-20 | 0.8 | 1.2-1.3 |\n",
    "| **Market Analysis** | Balanced analysis of trends | 0.6-0.7 | 30-50 | 0.9 | 1.1 |\n",
    "| **Investment Strategies** | Creative but grounded ideas | 0.7-0.9 | 40-50 | 0.92 | 1.05-1.1 |\n",
    "| **Risk Assessment** | Conservative, careful evaluation | 0.3-0.5 | 20-30 | 0.85 | 1.2 |\n",
    "| **Regulatory Compliance** | Precise, formal language | 0.2-0.3 | 5-15 | 0.7-0.8 | 1.3 |\n",
    "| **Client Communications** | Clear, accessible language | 0.5-0.6 | 30-40 | 0.9 | 1.15 |\n",
    "\n",
    "### General Guidelines\n",
    "\n",
    "1. **Lower temperature** (0.2-0.5): Use for tasks requiring accuracy, consistency, and reliability\n",
    "2. **Medium temperature** (0.5-0.8): Use for balanced analysis and general content\n",
    "3. **Higher temperature** (0.8-1.0): Use for creative ideation and exploring possibilities\n",
    "4. **Top-k and Top-p**: Start with top-p = 0.9 and adjust based on content needs\n",
    "5. **Repetition penalty**: Increase for longer texts to prevent circular reasoning\n",
    "\n",
    "### Task-specific Considerations\n",
    "\n",
    "- **For numerical analysis**: Use lower temperature and top-p to ensure accuracy\n",
    "- **For market forecasts**: Balance between creativity and coherence\n",
    "- **For compliance documents**: Prioritize precision over creativity\n",
    "- **For investment brainstorming**: Allow higher creativity with safeguards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251c638a",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this practical session, we've explored the inner workings of LLMs, focusing on:\n",
    "\n",
    "1. **Hyperparameters** like temperature, top-k, and top-p, and how they affect text generation\n",
    "2. **Embeddings** and how they capture semantic relationships between financial terms\n",
    "3. **Confidence and probability** at the token level, showing how models make predictions\n",
    "4. **Prompt engineering strategies** based on understanding model mechanics\n",
    "5. **Task-specific optimizations** for different financial applications\n",
    "\n",
    "These insights will help you develop more effective financial applications using LLMs by:\n",
    "- Tuning parameters appropriately for different tasks\n",
    "- Creating prompts that leverage the model's semantic understanding\n",
    "- Interpreting model outputs with an understanding of confidence levels\n",
    "- Balancing creativity and accuracy based on your specific needs\n",
    "\n",
    "In the next practical session, we'll explore memory buffers, caching, and automated function calling to build more sophisticated LLM applications for finance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
