{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edba9851",
   "metadata": {},
   "source": [
    "# Memory Buffer, Cache, and Automatic Calls in LLMs\n",
    "\n",
    "Welcome to the third practical session of Day 2! In this notebook, we'll explore advanced techniques for enhancing LLMs with memory systems, caching mechanisms, and automated function calling capabilities. These techniques are crucial for building sophisticated financial applications that maintain context, optimize performance, and interact with external systems.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "- Implement memory buffers to maintain conversation context\n",
    "- Set up caching systems to improve performance and reduce API costs\n",
    "- Create automated function calling for LLMs to interact with financial data sources\n",
    "- Build a simple financial assistant that demonstrates these capabilities\n",
    "- Compare implementation approaches using local models versus API-based LLMs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1490a697",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5c7b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import re\n",
    "import requests\n",
    "import sqlite3\n",
    "import hashlib\n",
    "from IPython.display import display, Markdown, HTML\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Load environment variables for API keys\n",
    "load_dotenv()\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set up paths\n",
    "CACHE_DIR = './cache'\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check if we have API keys for OpenAI or DeepSeek\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "deepseek_api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "api_available = openai_api_key is not None or deepseek_api_key is not None\n",
    "if api_available:\n",
    "    print(\"API key(s) found. API-based LLM examples will be available.\")\n",
    "    \n",
    "    # Import API libraries if keys are available\n",
    "    if openai_api_key:\n",
    "        import openai\n",
    "        openai.api_key = openai_api_key\n",
    "        print(\"OpenAI API is configured.\")\n",
    "    \n",
    "    if deepseek_api_key:\n",
    "        print(\"DeepSeek API is configured.\")\n",
    "else:\n",
    "    print(\"No API keys found. We'll use local models only.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57ab1ff",
   "metadata": {},
   "source": [
    "## 2. Memory Systems for LLMs\n",
    "\n",
    "LLMs have a limited context window, meaning they can only \"see\" a certain number of tokens at once. To build applications that maintain conversation history and context, we need to implement memory systems. Let's explore different approaches to memory management:\n",
    "\n",
    "1. **Basic Message History**: Storing all previous interactions\n",
    "2. **Summary Memory**: Periodically summarizing conversation history\n",
    "3. **Vector Memory**: Storing and retrieving relevant information based on embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ef9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationMemory:\n",
    "    \"\"\"Base class for conversation memory systems.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens=1024):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.messages = []\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"Add a message to the conversation history.\"\"\"\n",
    "        self.messages.append({\"role\": role, \"content\": content})\n",
    "    \n",
    "    def get_conversation_history(self):\n",
    "        \"\"\"Get the current conversation history.\"\"\"\n",
    "        return self.messages\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear the conversation history.\"\"\"\n",
    "        self.messages = []\n",
    "        self.metadata = {}\n",
    "    \n",
    "    def get_token_count(self, tokenizer):\n",
    "        \"\"\"Estimate the number of tokens in the conversation history.\"\"\"\n",
    "        full_text = \" \".join([msg[\"content\"] for msg in self.messages])\n",
    "        tokens = tokenizer.encode(full_text)\n",
    "        return len(tokens)\n",
    "\n",
    "\n",
    "class BasicMemory(ConversationMemory):\n",
    "    \"\"\"Simple message history with token limit enforcement.\"\"\"\n",
    "    \n",
    "    def add_message(self, role, content, tokenizer):\n",
    "        \"\"\"Add a message and trim history if needed to stay within token limit.\"\"\"\n",
    "        super().add_message(role, content)\n",
    "        \n",
    "        # Check token count and trim if necessary\n",
    "        while self.get_token_count(tokenizer) > self.max_tokens and len(self.messages) > 2:\n",
    "            # Remove the oldest message, but keep at least the system prompt\n",
    "            # and the most recent user message\n",
    "            self.messages.pop(1)\n",
    "\n",
    "\n",
    "class SummaryMemory(ConversationMemory):\n",
    "    \"\"\"Memory system that periodically summarizes conversation history.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens=1024, summarize_every=10, model=None, tokenizer=None):\n",
    "        super().__init__(max_tokens)\n",
    "        self.summarize_every = summarize_every\n",
    "        self.summary = \"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"Add a message and summarize if needed.\"\"\"\n",
    "        super().add_message(role, content)\n",
    "        \n",
    "        # Check if it's time to summarize\n",
    "        if len(self.messages) % self.summarize_every == 0 and self.model is not None:\n",
    "            self._summarize_history()\n",
    "    \n",
    "    def _summarize_history(self):\n",
    "        \"\"\"Summarize recent conversation history.\"\"\"\n",
    "        if len(self.messages) < 2:\n",
    "            return\n",
    "        \n",
    "        # Get text to summarize (skip system message if present)\n",
    "        start_idx = 1 if self.messages[0][\"role\"] == \"system\" else 0\n",
    "        to_summarize = self.messages[start_idx:]\n",
    "        \n",
    "        # Prepare prompt for summarization\n",
    "        conversation_text = \"\\n\".join([\n",
    "            f\"{msg['role'].capitalize()}: {msg['content']}\" \n",
    "            for msg in to_summarize\n",
    "        ])\n",
    "        \n",
    "        prompt = f\"\"\"Summarize the following conversation concisely, \n",
    "        focusing on key financial information and decisions:\n",
    "        \n",
    "        {conversation_text}\n",
    "        \n",
    "        Summary:\"\"\"\n",
    "        \n",
    "        # Generate summary using the model\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        output = self.model.generate(\n",
    "            inputs[\"input_ids\"],\n",
    "            max_length=150,\n",
    "            temperature=0.3,\n",
    "            num_return_sequences=1\n",
    "        )\n",
    "        summary = self.tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Extract just the summary part\n",
    "        if \"Summary:\" in summary:\n",
    "            summary = summary.split(\"Summary:\")[1].strip()\n",
    "        \n",
    "        # Update the summary\n",
    "        self.summary = summary\n",
    "        \n",
    "        # Replace older messages with the summary\n",
    "        system_message = None\n",
    "        if self.messages[0][\"role\"] == \"system\":\n",
    "            system_message = self.messages[0]\n",
    "        \n",
    "        # Keep only recent messages and the summary\n",
    "        recent_messages = self.messages[-4:]\n",
    "        self.messages = []\n",
    "        \n",
    "        if system_message:\n",
    "            self.messages.append(system_message)\n",
    "        \n",
    "        self.messages.append({\"role\": \"system\", \"content\": f\"Previous conversation summary: {summary}\"})\n",
    "        self.messages.extend(recent_messages)\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Get the current conversation summary.\"\"\"\n",
    "        return self.summary\n",
    "\n",
    "\n",
    "class VectorMemory(ConversationMemory):\n",
    "    \"\"\"Memory system that uses embeddings to store and retrieve relevant information.\"\"\"\n",
    "    \n",
    "    def __init__(self, max_tokens=1024, embedding_model=None):\n",
    "        super().__init__(max_tokens)\n",
    "        self.embedding_model = embedding_model\n",
    "        self.embeddings = []\n",
    "        self.message_embeddings = []\n",
    "    \n",
    "    def add_message(self, role, content):\n",
    "        \"\"\"Add a message and its embedding to memory.\"\"\"\n",
    "        super().add_message(role, content)\n",
    "        \n",
    "        if self.embedding_model is not None:\n",
    "            # Get embedding for the message\n",
    "            embedding = self._get_embedding(content)\n",
    "            self.message_embeddings.append(embedding)\n",
    "    \n",
    "    def _get_embedding(self, text):\n",
    "        \"\"\"Get embedding for a text using the embedding model.\"\"\"\n",
    "        # This is a simplified version; in practice, you'd use\n",
    "        # a proper embedding model like OpenAI's embedding API\n",
    "        # or a local model via transformers\n",
    "        \n",
    "        # For demonstration, we'll create a simple random embedding\n",
    "        # In a real system, replace this with actual embeddings\n",
    "        return np.random.randn(768)  # Typical embedding dimension\n",
    "    \n",
    "    def search_similar(self, query, top_k=3):\n",
    "        \"\"\"Find messages most similar to the query based on embeddings.\"\"\"\n",
    "        if not self.message_embeddings:\n",
    "            return []\n",
    "        \n",
    "        # Get query embedding\n",
    "        query_embedding = self._get_embedding(query)\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        similarities = []\n",
    "        for i, emb in enumerate(self.message_embeddings):\n",
    "            # Cosine similarity\n",
    "            similarity = np.dot(query_embedding, emb) / (\n",
    "                np.linalg.norm(query_embedding) * np.linalg.norm(emb)\n",
    "            )\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # Sort by similarity (descending)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Return top-k most similar messages\n",
    "        top_messages = []\n",
    "        for i, _ in similarities[:top_k]:\n",
    "            top_messages.append(self.messages[i])\n",
    "        \n",
    "        return top_messages\n",
    "\n",
    "# Load a small model for demonstration\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize memory systems\n",
    "basic_memory = BasicMemory(max_tokens=512)\n",
    "summary_memory = SummaryMemory(max_tokens=512, summarize_every=5, model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Demo conversation\n",
    "print(\"Demonstrating memory systems with a simulated financial conversation...\")\n",
    "\n",
    "conversation = [\n",
    "    (\"system\", \"You are a financial advisor helping with investment decisions.\"),\n",
    "    (\"user\", \"What's the difference between stocks and bonds?\"),\n",
    "    (\"assistant\", \"Stocks represent ownership in a company, while bonds are debt instruments where you lend money to an entity. Stocks generally offer higher potential returns but with more risk, while bonds typically provide more stable but lower returns.\"),\n",
    "    (\"user\", \"How should I allocate my portfolio if I'm 35 years old?\"),\n",
    "    (\"assistant\", \"At 35, you have a long investment horizon before retirement. A common approach is to allocate 70-80% to stocks and 20-30% to bonds. This provides growth potential while maintaining some stability. Consider your risk tolerance and financial goals when making your decision.\"),\n",
    "    (\"user\", \"What about international investments?\"),\n",
    "    (\"assistant\", \"International investments can provide diversification benefits. Consider allocating 20-30% of your stock portion to international markets. This helps reduce country-specific risk and gives you exposure to global growth opportunities.\"),\n",
    "    (\"user\", \"Should I invest in emerging markets?\"),\n",
    "    (\"assistant\", \"Emerging markets can offer higher growth potential but come with additional risks like political instability and currency fluctuations. For a 35-year-old investor, allocating 5-10% of your portfolio to emerging markets can be reasonable, but ensure it aligns with your risk tolerance.\")\n",
    "]\n",
    "\n",
    "# Add messages to both memory systems\n",
    "for role, content in conversation:\n",
    "    basic_memory.add_message(role, content, tokenizer)\n",
    "    summary_memory.add_message(role, content)\n",
    "\n",
    "# Display the memory contents\n",
    "print(\"\\nBasic Memory:\")\n",
    "for i, msg in enumerate(basic_memory.get_conversation_history()):\n",
    "    print(f\"{i+1}. {msg['role'].capitalize()}: {msg['content'][:50]}...\")\n",
    "\n",
    "print(\"\\nSummary Memory:\")\n",
    "print(f\"Summary: {summary_memory.get_summary()}\")\n",
    "for i, msg in enumerate(summary_memory.get_conversation_history()):\n",
    "    print(f\"{i+1}. {msg['role'].capitalize()}: {msg['content'][:50]}...\")\n",
    "\n",
    "# Token counts\n",
    "print(f\"\\nBasic Memory Token Count: {basic_memory.get_token_count(tokenizer)}\")\n",
    "print(f\"Summary Memory Token Count: {summary_memory.get_token_count(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e16eb6",
   "metadata": {},
   "source": [
    "## 3. Caching Systems for LLMs\n",
    "\n",
    "Caching is essential for LLM applications to improve performance, reduce costs, and ensure consistency in responses. Let's implement a simple caching system for LLM queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5723d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMCache:\n",
    "    \"\"\"A simple caching system for LLM queries.\"\"\"\n",
    "    \n",
    "    def __init__(self, cache_dir=CACHE_DIR, ttl=86400, db_name=\"llm_cache.db\"):\n",
    "        \"\"\"\n",
    "        Initialize the cache.\n",
    "        \n",
    "        Args:\n",
    "            cache_dir: Directory to store the cache\n",
    "            ttl: Time to live in seconds (default: 24 hours)\n",
    "            db_name: Name of the SQLite database file\n",
    "        \"\"\"\n",
    "        self.cache_dir = cache_dir\n",
    "        self.ttl = ttl\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Initialize SQLite database\n",
    "        self.db_path = os.path.join(cache_dir, db_name)\n",
    "        self.conn = sqlite3.connect(self.db_path)\n",
    "        self.cursor = self.conn.cursor()\n",
    "        \n",
    "        # Create cache table if it doesn't exist\n",
    "        self.cursor.execute('''\n",
    "        CREATE TABLE IF NOT EXISTS llm_cache (\n",
    "            key TEXT PRIMARY KEY,\n",
    "            value TEXT,\n",
    "            timestamp INTEGER\n",
    "        )\n",
    "        ''')\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def _get_cache_key(self, prompt, model, temperature, max_tokens):\n",
    "        \"\"\"Generate a unique cache key based on the query parameters.\"\"\"\n",
    "        # Combine all parameters into a string\n",
    "        params_str = f\"{prompt}|{model}|{temperature}|{max_tokens}\"\n",
    "        \n",
    "        # Create a hash of the parameters\n",
    "        cache_key = hashlib.md5(params_str.encode()).hexdigest()\n",
    "        return cache_key\n",
    "    \n",
    "    def get(self, prompt, model, temperature=0.7, max_tokens=100):\n",
    "        \"\"\"\n",
    "        Get a cached response if available and not expired.\n",
    "        \n",
    "        Returns:\n",
    "            Cached response or None if not found or expired\n",
    "        \"\"\"\n",
    "        cache_key = self._get_cache_key(prompt, model, temperature, max_tokens)\n",
    "        \n",
    "        # Query the database\n",
    "        self.cursor.execute(\n",
    "            \"SELECT value, timestamp FROM llm_cache WHERE key = ?\", \n",
    "            (cache_key,)\n",
    "        )\n",
    "        result = self.cursor.fetchone()\n",
    "        \n",
    "        if result:\n",
    "            value, timestamp = result\n",
    "            # Check if cache is expired\n",
    "            if time.time() - timestamp <= self.ttl:\n",
    "                print(f\"Cache hit for prompt: {prompt[:30]}...\")\n",
    "                return json.loads(value)\n",
    "            else:\n",
    "                # Remove expired entry\n",
    "                self.cursor.execute(\"DELETE FROM llm_cache WHERE key = ?\", (cache_key,))\n",
    "                self.conn.commit()\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def set(self, prompt, model, temperature, max_tokens, response):\n",
    "        \"\"\"Store a response in the cache.\"\"\"\n",
    "        cache_key = self._get_cache_key(prompt, model, temperature, max_tokens)\n",
    "        \n",
    "        # Store in database\n",
    "        self.cursor.execute(\n",
    "            \"INSERT OR REPLACE INTO llm_cache (key, value, timestamp) VALUES (?, ?, ?)\",\n",
    "            (cache_key, json.dumps(response), int(time.time()))\n",
    "        )\n",
    "        self.conn.commit()\n",
    "        print(f\"Cached response for prompt: {prompt[:30]}...\")\n",
    "    \n",
    "    def clear(self):\n",
    "        \"\"\"Clear all cache entries.\"\"\"\n",
    "        self.cursor.execute(\"DELETE FROM llm_cache\")\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def clear_expired(self):\n",
    "        \"\"\"Clear expired cache entries.\"\"\"\n",
    "        self.cursor.execute(\n",
    "            \"DELETE FROM llm_cache WHERE timestamp < ?\", \n",
    "            (int(time.time() - self.ttl),)\n",
    "        )\n",
    "        self.conn.commit()\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the database connection.\"\"\"\n",
    "        self.conn.close()\n",
    "\n",
    "# Initialize the cache\n",
    "llm_cache = LLMCache()\n",
    "\n",
    "# Function to generate text with caching\n",
    "def generate_text_with_cache(prompt, model_name=\"gpt2\", temperature=0.7, max_tokens=100):\n",
    "    \"\"\"Generate text using a model with caching.\"\"\"\n",
    "    # Check cache first\n",
    "    cached_response = llm_cache.get(prompt, model_name, temperature, max_tokens)\n",
    "    if cached_response:\n",
    "        return cached_response, True  # Return cached response\n",
    "    \n",
    "    # If not in cache, generate new response\n",
    "    if model_name == \"gpt2\":\n",
    "        # Use local GPT-2 model\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(\n",
    "                inputs[\"input_ids\"],\n",
    "                max_length=len(inputs[\"input_ids\"][0]) + max_tokens,\n",
    "                temperature=temperature,\n",
    "                do_sample=True,\n",
    "                num_return_sequences=1\n",
    "            )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Store in cache\n",
    "        llm_cache.set(prompt, model_name, temperature, max_tokens, response)\n",
    "        \n",
    "        return response, False  # Return new response\n",
    "    \n",
    "    # For API-based models, we would implement API calls here\n",
    "    # but we'll use the local model for this demonstration\n",
    "    else:\n",
    "        # Simulate API call\n",
    "        print(f\"Would call API for model: {model_name}\")\n",
    "        return \"API response would be here\", False\n",
    "\n",
    "# Test the cache with a few queries\n",
    "test_prompts = [\n",
    "    \"What are the key factors to consider when evaluating a stock?\",\n",
    "    \"How does inflation affect bond prices?\",\n",
    "    \"What is the efficient market hypothesis?\",\n",
    "    \"What are the key factors to consider when evaluating a stock?\",  # Repeated to test cache\n",
    "]\n",
    "\n",
    "for prompt in test_prompts:\n",
    "    start_time = time.time()\n",
    "    response, cache_hit = generate_text_with_cache(prompt)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Cache hit: {cache_hit}\")\n",
    "    print(f\"Time taken: {end_time - start_time:.4f} seconds\")\n",
    "    print(f\"Response: {response[:100]}...\\n\")\n",
    "\n",
    "# Clear expired cache entries\n",
    "llm_cache.clear_expired()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ee169b",
   "metadata": {},
   "source": [
    "## 4. Automatic Function Calling\n",
    "\n",
    "One of the most powerful features of modern LLMs is their ability to call external functions or tools. This enables them to retrieve real-time data, perform calculations, and interact with external systems. Let's implement a simple function calling system for our financial assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb65fa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FunctionRegistry:\n",
    "    \"\"\"Registry for functions that can be called by the LLM.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.functions = {}\n",
    "        self.function_descriptions = {}\n",
    "    \n",
    "    def register(self, func, description):\n",
    "        \"\"\"Register a function with its description.\"\"\"\n",
    "        self.functions[func.__name__] = func\n",
    "        self.function_descriptions[func.__name__] = description\n",
    "    \n",
    "    def get_function(self, func_name):\n",
    "        \"\"\"Get a function by name.\"\"\"\n",
    "        return self.functions.get(func_name)\n",
    "    \n",
    "    def get_description(self, func_name):\n",
    "        \"\"\"Get a function's description.\"\"\"\n",
    "        return self.function_descriptions.get(func_name)\n",
    "    \n",
    "    def list_functions(self):\n",
    "        \"\"\"List all registered functions.\"\"\"\n",
    "        return list(self.functions.keys())\n",
    "    \n",
    "    def get_all_descriptions(self):\n",
    "        \"\"\"Get descriptions of all registered functions.\"\"\"\n",
    "        return self.function_descriptions.copy()\n",
    "\n",
    "\n",
    "# Create some example financial functions\n",
    "\n",
    "def get_stock_price(symbol):\n",
    "    \"\"\"\n",
    "    Get the current price of a stock.\n",
    "    \n",
    "    Args:\n",
    "        symbol: The stock symbol (e.g., AAPL, MSFT)\n",
    "        \n",
    "    Returns:\n",
    "        Current price of the stock\n",
    "    \"\"\"\n",
    "    # In a real application, this would call an API\n",
    "    # For demonstration, we'll return simulated prices\n",
    "    stock_prices = {\n",
    "        \"AAPL\": 182.52,\n",
    "        \"MSFT\": 425.63,\n",
    "        \"GOOGL\": 175.98,\n",
    "        \"AMZN\": 178.75,\n",
    "        \"META\": 487.55,\n",
    "        \"TSLA\": 175.22,\n",
    "        \"NVDA\": 950.02,\n",
    "    }\n",
    "    \n",
    "    return stock_prices.get(symbol.upper(), None)\n",
    "\n",
    "\n",
    "def calculate_compound_interest(principal, rate, time, compounding_periods=1):\n",
    "    \"\"\"\n",
    "    Calculate compound interest.\n",
    "    \n",
    "    Args:\n",
    "        principal: Initial investment amount\n",
    "        rate: Annual interest rate (as a decimal, e.g., 0.05 for 5%)\n",
    "        time: Time period in years\n",
    "        compounding_periods: Number of compounding periods per year (1 for annual)\n",
    "        \n",
    "    Returns:\n",
    "        Final amount after compound interest\n",
    "    \"\"\"\n",
    "    # A = P(1 + r/n)^(nt)\n",
    "    n = compounding_periods\n",
    "    final_amount = principal * (1 + rate / n) ** (n * time)\n",
    "    return final_amount\n",
    "\n",
    "\n",
    "def calculate_portfolio_value(holdings):\n",
    "    \"\"\"\n",
    "    Calculate the total value of a portfolio.\n",
    "    \n",
    "    Args:\n",
    "        holdings: Dictionary mapping stock symbols to number of shares\n",
    "        \n",
    "    Returns:\n",
    "        Total value of the portfolio\n",
    "    \"\"\"\n",
    "    total_value = 0\n",
    "    for symbol, shares in holdings.items():\n",
    "        price = get_stock_price(symbol)\n",
    "        if price is not None:\n",
    "            total_value += price * shares\n",
    "    \n",
    "    return total_value\n",
    "\n",
    "\n",
    "def get_historical_returns(index_name, years):\n",
    "    \"\"\"\n",
    "    Get historical annual returns for a market index.\n",
    "    \n",
    "    Args:\n",
    "        index_name: Name of the index (e.g., S&P500, NASDAQ)\n",
    "        years: Number of years of data to return (max 10)\n",
    "        \n",
    "    Returns:\n",
    "        List of annual returns for the specified index\n",
    "    \"\"\"\n",
    "    # Simulated historical returns\n",
    "    historical_returns = {\n",
    "        \"S&P500\": [9.3, 28.7, 18.4, 31.5, -4.4, 21.8, -6.2, 32.4, 13.6, 2.1],\n",
    "        \"NASDAQ\": [12.5, 35.2, 22.2, 44.9, -2.8, 28.2, -3.9, 43.6, 23.1, 8.4],\n",
    "        \"DOW\": [7.2, 25.1, 16.5, 26.5, -3.9, 19.4, -5.6, 25.3, 9.7, 0.8]\n",
    "    }\n",
    "    \n",
    "    # Get data for the requested index\n",
    "    returns = historical_returns.get(index_name.upper(), [])\n",
    "    \n",
    "    # Limit to the requested number of years\n",
    "    years = min(years, len(returns))\n",
    "    \n",
    "    return returns[:years]\n",
    "\n",
    "\n",
    "# Register functions\n",
    "function_registry = FunctionRegistry()\n",
    "function_registry.register(get_stock_price, \"Get the current price of a stock by symbol\")\n",
    "function_registry.register(calculate_compound_interest, \"Calculate compound interest on an investment\")\n",
    "function_registry.register(calculate_portfolio_value, \"Calculate the total value of a portfolio of stocks\")\n",
    "function_registry.register(get_historical_returns, \"Get historical annual returns for a market index\")\n",
    "\n",
    "# List registered functions\n",
    "print(\"Registered functions:\")\n",
    "for func_name, description in function_registry.get_all_descriptions().items():\n",
    "    print(f\"  - {func_name}: {description}\")\n",
    "\n",
    "# Function to parse and execute function calls from LLM output\n",
    "def parse_and_execute_function_call(text, function_registry):\n",
    "    \"\"\"\n",
    "    Parse function calls from text and execute them.\n",
    "    \n",
    "    Args:\n",
    "        text: Text containing function calls\n",
    "        function_registry: Registry of available functions\n",
    "        \n",
    "    Returns:\n",
    "        Text with function calls replaced by their results\n",
    "    \"\"\"\n",
    "    # Define pattern for function calls\n",
    "    # Format: {{function_name(arg1, arg2, ...)}}\n",
    "    pattern = r'\\{\\{(\\w+)\\((.*?)\\)\\}\\}'\n",
    "    \n",
    "    # Find all function calls\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    # Process each function call\n",
    "    for func_name, args_str in matches:\n",
    "        # Get the function\n",
    "        func = function_registry.get_function(func_name)\n",
    "        if func is None:\n",
    "            continue\n",
    "        \n",
    "        # Parse arguments\n",
    "        args = []\n",
    "        kwargs = {}\n",
    "        \n",
    "        if args_str.strip():\n",
    "            # Split by comma, but respect nested structures\n",
    "            depth = 0\n",
    "            current_arg = \"\"\n",
    "            for char in args_str:\n",
    "                if char == '(' or char == '{' or char == '[':\n",
    "                    depth += 1\n",
    "                elif char == ')' or char == '}' or char == ']':\n",
    "                    depth -= 1\n",
    "                \n",
    "                if char == ',' and depth == 0:\n",
    "                    # End of argument\n",
    "                    args.append(current_arg.strip())\n",
    "                    current_arg = \"\"\n",
    "                else:\n",
    "                    current_arg += char\n",
    "            \n",
    "            if current_arg.strip():\n",
    "                args.append(current_arg.strip())\n",
    "            \n",
    "            # Process each argument\n",
    "            processed_args = []\n",
    "            for arg in args:\n",
    "                # Check if it's a keyword argument\n",
    "                if '=' in arg and not arg.startswith((\"'\", '\"', '{')):\n",
    "                    key, value = arg.split('=', 1)\n",
    "                    key = key.strip()\n",
    "                    value = value.strip()\n",
    "                    \n",
    "                    # Convert value to appropriate type\n",
    "                    try:\n",
    "                        # Try as number\n",
    "                        if '.' in value:\n",
    "                            value = float(value)\n",
    "                        else:\n",
    "                            value = int(value)\n",
    "                    except ValueError:\n",
    "                        # Try as dict, list, or string\n",
    "                        if value.startswith('{') and value.endswith('}'):\n",
    "                            try:\n",
    "                                value = eval(value)  # Caution: eval can be dangerous\n",
    "                            except:\n",
    "                                pass\n",
    "                        elif value.startswith('[') and value.endswith(']'):\n",
    "                            try:\n",
    "                                value = eval(value)  # Caution: eval can be dangerous\n",
    "                            except:\n",
    "                                pass\n",
    "                        elif (value.startswith('\"') and value.endswith('\"')) or \\\n",
    "                             (value.startswith(\"'\") and value.endswith(\"'\")):\n",
    "                            value = value[1:-1]  # Remove quotes\n",
    "                    \n",
    "                    kwargs[key] = value\n",
    "                else:\n",
    "                    # Positional argument\n",
    "                    # Convert to appropriate type\n",
    "                    arg = arg.strip()\n",
    "                    try:\n",
    "                        # Try as number\n",
    "                        if '.' in arg:\n",
    "                            arg = float(arg)\n",
    "                        else:\n",
    "                            try:\n",
    "                                arg = int(arg)\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "                    except ValueError:\n",
    "                        # Try as dict, list, or string\n",
    "                        if arg.startswith('{') and arg.endswith('}'):\n",
    "                            try:\n",
    "                                arg = eval(arg)  # Caution: eval can be dangerous\n",
    "                            except:\n",
    "                                pass\n",
    "                        elif arg.startswith('[') and arg.endswith(']'):\n",
    "                            try:\n",
    "                                arg = eval(arg)  # Caution: eval can be dangerous\n",
    "                            except:\n",
    "                                pass\n",
    "                        elif (arg.startswith('\"') and arg.endswith('\"')) or \\\n",
    "                             (arg.startswith(\"'\") and arg.endswith(\"'\")):\n",
    "                            arg = arg[1:-1]  # Remove quotes\n",
    "                    \n",
    "                    processed_args.append(arg)\n",
    "        \n",
    "        # Execute the function\n",
    "        try:\n",
    "            result = func(*processed_args, **kwargs)\n",
    "            \n",
    "            # Replace the function call with the result\n",
    "            call_text = f\"{{{{{func_name}({args_str})}}}}}\"\n",
    "            text = text.replace(call_text, str(result))\n",
    "        except Exception as e:\n",
    "            # Replace with error message\n",
    "            error_text = f\"Error in {func_name}: {str(e)}\"\n",
    "            call_text = f\"{{{{{func_name}({args_str})}}}}}\"\n",
    "            text = text.replace(call_text, error_text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Test function calling\n",
    "test_texts = [\n",
    "    \"The current price of Apple stock is {{get_stock_price('AAPL')}}.\",\n",
    "    \"If you invest $10,000 at 5% interest for 10 years with annual compounding, you'll have ${{calculate_compound_interest(10000, 0.05, 10)}}.\",\n",
    "    \"A portfolio with 10 shares of Apple, 5 shares of Microsoft, and 8 shares of Google is worth ${{calculate_portfolio_value({'AAPL': 10, 'MSFT': 5, 'GOOGL': 8})}}.\",\n",
    "    \"The S&P 500 has had the following annual returns over the past 5 years: {{get_historical_returns('S&P500', 5)}}%.\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = parse_and_execute_function_call(text, function_registry)\n",
    "    print(f\"Original: {text}\")\n",
    "    print(f\"Result: {result}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c87aa4",
   "metadata": {},
   "source": [
    "## 5. Building a Financial Assistant\n",
    "\n",
    "Now, let's combine our memory, caching, and function calling systems to build a simple financial assistant that can maintain context, respond efficiently, and access external data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e9b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialAssistant:\n",
    "    \"\"\"A financial assistant with memory, caching, and function calling capabilities.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"gpt2\", use_api=False):\n",
    "        \"\"\"\n",
    "        Initialize the financial assistant.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model to use\n",
    "            use_api: Whether to use an API-based model\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.use_api = use_api\n",
    "        \n",
    "        # Initialize components\n",
    "        self.memory = BasicMemory(max_tokens=1024)\n",
    "        self.cache = LLMCache()\n",
    "        self.function_registry = function_registry  # Use the one we created earlier\n",
    "        \n",
    "        # Set up tokenizer and model\n",
    "        if not use_api:\n",
    "            self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "            self.model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            # For API-based models, we'd initialize API clients here\n",
    "            if openai_api_key:\n",
    "                self.tokenizer = None\n",
    "                self.model = None\n",
    "                print(\"Using OpenAI API\")\n",
    "            else:\n",
    "                print(\"API key not available, falling back to local model\")\n",
    "                self.use_api = False\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "                self.model = GPT2LMHeadModel.from_pretrained(model_name).to(device)\n",
    "                self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Add system prompt\n",
    "        self.system_prompt = \"\"\"\n",
    "        You are a helpful financial assistant. You can provide information and advice on:\n",
    "        - Investment strategies\n",
    "        - Stock analysis\n",
    "        - Financial planning\n",
    "        - Market trends\n",
    "        \n",
    "        You can call functions to get real-time data or perform calculations.\n",
    "        To call a function, use the format {{function_name(arg1, arg2, ...)}}.\n",
    "        \n",
    "        Available functions:\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add function descriptions to system prompt\n",
    "        for func_name, description in self.function_registry.get_all_descriptions().items():\n",
    "            self.system_prompt += f\"- {func_name}: {description}\\n\"\n",
    "        \n",
    "        self.memory.add_message(\"system\", self.system_prompt, self.tokenizer)\n",
    "    \n",
    "    def generate_response(self, prompt, temperature=0.7, max_tokens=150):\n",
    "        \"\"\"\n",
    "        Generate a response to the user's prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt: User's input\n",
    "            temperature: Temperature for text generation\n",
    "            max_tokens: Maximum tokens to generate\n",
    "            \n",
    "        Returns:\n",
    "            Assistant's response\n",
    "        \"\"\"\n",
    "        # Add user message to memory\n",
    "        self.memory.add_message(\"user\", prompt, self.tokenizer)\n",
    "        \n",
    "        # Prepare conversation history for the model\n",
    "        conversation_history = self.memory.get_conversation_history()\n",
    "        \n",
    "        # Format conversation for the model\n",
    "        if self.use_api and openai_api_key:\n",
    "            # Format for OpenAI API\n",
    "            formatted_messages = []\n",
    "            for msg in conversation_history:\n",
    "                formatted_messages.append({\"role\": msg[\"role\"], \"content\": msg[\"content\"]})\n",
    "            \n",
    "            # Call the API\n",
    "            try:\n",
    "                response = openai.ChatCompletion.create(\n",
    "                    model=\"gpt-3.5-turbo\",\n",
    "                    messages=formatted_messages,\n",
    "                    temperature=temperature,\n",
    "                    max_tokens=max_tokens\n",
    "                )\n",
    "                model_response = response.choices[0].message.content\n",
    "            except Exception as e:\n",
    "                model_response = f\"Error: {str(e)}\"\n",
    "        else:\n",
    "            # Format for local model\n",
    "            formatted_prompt = \"\\n\".join([\n",
    "                f\"{msg['role'].capitalize()}: {msg['content']}\" \n",
    "                for msg in conversation_history\n",
    "            ])\n",
    "            \n",
    "            # Check cache\n",
    "            cached_response = self.cache.get(formatted_prompt, self.model_name, temperature, max_tokens)\n",
    "            if cached_response:\n",
    "                model_response = cached_response\n",
    "            else:\n",
    "                # Generate response with local model\n",
    "                inputs = self.tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model.generate(\n",
    "                        inputs[\"input_ids\"],\n",
    "                        max_length=len(inputs[\"input_ids\"][0]) + max_tokens,\n",
    "                        temperature=temperature,\n",
    "                        do_sample=True,\n",
    "                        num_return_sequences=1\n",
    "                    )\n",
    "                \n",
    "                full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                # Extract just the assistant's response\n",
    "                if \"Assistant:\" in full_response:\n",
    "                    model_response = full_response.split(\"Assistant:\")[-1].strip()\n",
    "                else:\n",
    "                    model_response = full_response\n",
    "                \n",
    "                # Cache the response\n",
    "                self.cache.set(formatted_prompt, self.model_name, temperature, max_tokens, model_response)\n",
    "        \n",
    "        # Process function calls\n",
    "        processed_response = parse_and_execute_function_call(model_response, self.function_registry)\n",
    "        \n",
    "        # Add assistant's response to memory\n",
    "        self.memory.add_message(\"assistant\", processed_response, self.tokenizer)\n",
    "        \n",
    "        return processed_response\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"Reset the conversation.\"\"\"\n",
    "        self.memory.clear()\n",
    "        self.memory.add_message(\"system\", self.system_prompt, self.tokenizer)\n",
    "\n",
    "\n",
    "# Create a financial assistant\n",
    "financial_assistant = FinancialAssistant(use_api=False)  # Set to True to use API if available\n",
    "\n",
    "# Test the assistant with some financial queries\n",
    "test_queries = [\n",
    "    \"What's the current price of Apple stock?\",\n",
    "    \"If I invest $5000 at 7% annual interest for 20 years, how much will I have?\",\n",
    "    \"What's the value of a portfolio with 15 shares of NVDA, 10 shares of MSFT, and 20 shares of AAPL?\",\n",
    "    \"What have been the returns of the S&P 500 over the past 3 years?\",\n",
    "    \"Based on our previous conversation, which stock has the highest price?\"\n",
    "]\n",
    "\n",
    "# Interactive conversation\n",
    "for query in test_queries:\n",
    "    print(f\"\\nUser: {query}\")\n",
    "    response = financial_assistant.generate_response(query)\n",
    "    print(f\"Assistant: {response}\")\n",
    "\n",
    "# Display conversation history\n",
    "print(\"\\nConversation History:\")\n",
    "for i, msg in enumerate(financial_assistant.memory.get_conversation_history()):\n",
    "    if msg[\"role\"] != \"system\":  # Skip system message for clarity\n",
    "        print(f\"{i}. {msg['role'].capitalize()}: {msg['content']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca5c29",
   "metadata": {},
   "source": [
    "## 6. Comparing Local Models vs. API-based LLMs\n",
    "\n",
    "Let's compare the advantages and limitations of using local models versus API-based LLMs for financial applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc656753",
   "metadata": {},
   "source": [
    "### Local Models vs. API-based LLMs Comparison\n",
    "\n",
    "| Feature | Local Models | API-based LLMs |\n",
    "|---------|-------------|----------------|\n",
    "| **Cost** | One-time cost for hardware | Pay-per-token pricing |\n",
    "| **Performance** | Limited by local hardware | High performance on provider's infrastructure |\n",
    "| **Latency** | Lower for small models | Depends on network and API provider |\n",
    "| **Privacy** | Data stays on local machine | Data sent to third-party servers |\n",
    "| **Customization** | Full control over fine-tuning | Limited to provider's options |\n",
    "| **Scaling** | Limited by local resources | Easily scales with demand |\n",
    "| **Regulatory Compliance** | Easier to comply with financial regulations | Requires careful data handling |\n",
    "| **Consistency** | Consistent responses (same version) | May change with provider updates |\n",
    "| **Knowledge Cutoff** | Fixed at training time | More recent for some providers |\n",
    "| **Function Calling** | Requires custom implementation | Often built into the API |\n",
    "\n",
    "### When to Use Each Approach\n",
    "\n",
    "**Local Models are better when:**\n",
    "- Privacy and data security are paramount\n",
    "- Consistent, deterministic responses are required\n",
    "- Working with sensitive financial information\n",
    "- Operating in highly regulated environments\n",
    "- Deployment in air-gapped systems is needed\n",
    "- Cost predictability is important\n",
    "\n",
    "**API-based LLMs are better when:**\n",
    "- Advanced capabilities beyond local hardware are needed\n",
    "- Development speed is prioritized over customization\n",
    "- Scaling to many users or high volume is required\n",
    "- Up-to-date knowledge is important\n",
    "- Complex reasoning for financial analysis is needed\n",
    "- Integration with provider's ecosystem is valuable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09943501",
   "metadata": {},
   "source": [
    "## 7. Practical Exercise: Creating a Portfolio Analyzer\n",
    "\n",
    "Let's put everything together by creating a portfolio analyzer that can:\n",
    "1. Track portfolio holdings in memory\n",
    "2. Retrieve current prices using function calls\n",
    "3. Calculate portfolio statistics\n",
    "4. Provide investment recommendations\n",
    "\n",
    "This exercise will demonstrate how memory, caching, and function calling work together in a practical financial application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6279d9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add portfolio-specific functions to our registry\n",
    "\n",
    "def calculate_portfolio_statistics(holdings):\n",
    "    \"\"\"\n",
    "    Calculate statistics for a portfolio.\n",
    "    \n",
    "    Args:\n",
    "        holdings: Dictionary mapping stock symbols to number of shares\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with portfolio statistics\n",
    "    \"\"\"\n",
    "    statistics = {}\n",
    "    total_value = 0\n",
    "    total_investment = 0\n",
    "    positions = []\n",
    "    \n",
    "    # Sample cost basis data (in a real app, this would be stored or provided)\n",
    "    cost_basis = {\n",
    "        \"AAPL\": 150.25,\n",
    "        \"MSFT\": 380.50,\n",
    "        \"GOOGL\": 140.75,\n",
    "        \"AMZN\": 140.30,\n",
    "        \"META\": 320.40,\n",
    "        \"TSLA\": 190.60,\n",
    "        \"NVDA\": 700.25,\n",
    "    }\n",
    "    \n",
    "    for symbol, shares in holdings.items():\n",
    "        price = get_stock_price(symbol)\n",
    "        if price is not None:\n",
    "            # Calculate position value\n",
    "            position_value = price * shares\n",
    "            total_value += position_value\n",
    "            \n",
    "            # Calculate cost basis and gain/loss\n",
    "            if symbol in cost_basis:\n",
    "                position_cost = cost_basis[symbol] * shares\n",
    "                total_investment += position_cost\n",
    "                gain_loss = position_value - position_cost\n",
    "                gain_loss_pct = (gain_loss / position_cost) * 100\n",
    "            else:\n",
    "                position_cost = \"Unknown\"\n",
    "                gain_loss = \"Unknown\"\n",
    "                gain_loss_pct = \"Unknown\"\n",
    "            \n",
    "            # Add position details\n",
    "            positions.append({\n",
    "                \"symbol\": symbol,\n",
    "                \"shares\": shares,\n",
    "                \"price\": price,\n",
    "                \"value\": position_value,\n",
    "                \"cost_basis\": cost_basis.get(symbol, \"Unknown\"),\n",
    "                \"gain_loss\": gain_loss,\n",
    "                \"gain_loss_pct\": gain_loss_pct\n",
    "            })\n",
    "    \n",
    "    # Calculate overall portfolio statistics\n",
    "    if total_investment > 0:\n",
    "        total_gain_loss = total_value - total_investment\n",
    "        total_gain_loss_pct = (total_gain_loss / total_investment) * 100\n",
    "    else:\n",
    "        total_gain_loss = \"Unknown\"\n",
    "        total_gain_loss_pct = \"Unknown\"\n",
    "    \n",
    "    # Store statistics\n",
    "    statistics[\"total_value\"] = total_value\n",
    "    statistics[\"total_investment\"] = total_investment\n",
    "    statistics[\"total_gain_loss\"] = total_gain_loss\n",
    "    statistics[\"total_gain_loss_pct\"] = total_gain_loss_pct\n",
    "    statistics[\"positions\"] = positions\n",
    "    \n",
    "    return statistics\n",
    "\n",
    "\n",
    "def get_portfolio_allocation(holdings):\n",
    "    \"\"\"\n",
    "    Get the allocation breakdown of a portfolio.\n",
    "    \n",
    "    Args:\n",
    "        holdings: Dictionary mapping stock symbols to number of shares\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with allocation percentages by stock\n",
    "    \"\"\"\n",
    "    allocation = {}\n",
    "    total_value = 0\n",
    "    \n",
    "    # Calculate total portfolio value\n",
    "    for symbol, shares in holdings.items():\n",
    "        price = get_stock_price(symbol)\n",
    "        if price is not None:\n",
    "            position_value = price * shares\n",
    "            total_value += position_value\n",
    "            allocation[symbol] = position_value\n",
    "    \n",
    "    # Calculate percentages\n",
    "    if total_value > 0:\n",
    "        for symbol in allocation:\n",
    "            allocation[symbol] = (allocation[symbol] / total_value) * 100\n",
    "    \n",
    "    return allocation\n",
    "\n",
    "\n",
    "def recommend_portfolio_adjustments(holdings, risk_profile=\"moderate\"):\n",
    "    \"\"\"\n",
    "    Recommend portfolio adjustments based on holdings and risk profile.\n",
    "    \n",
    "    Args:\n",
    "        holdings: Dictionary mapping stock symbols to number of shares\n",
    "        risk_profile: Risk profile (conservative, moderate, aggressive)\n",
    "        \n",
    "    Returns:\n",
    "        List of recommended adjustments\n",
    "    \"\"\"\n",
    "    # Get current allocation\n",
    "    allocation = get_portfolio_allocation(holdings)\n",
    "    \n",
    "    # Define target allocations based on risk profile\n",
    "    target_allocations = {\n",
    "        \"conservative\": {\n",
    "            \"AAPL\": 10,\n",
    "            \"MSFT\": 10,\n",
    "            \"GOOGL\": 5,\n",
    "            \"AMZN\": 5,\n",
    "            \"META\": 5,\n",
    "            \"TSLA\": 5,\n",
    "            \"NVDA\": 10,\n",
    "            \"other\": 50  # Bonds, etc.\n",
    "        },\n",
    "        \"moderate\": {\n",
    "            \"AAPL\": 15,\n",
    "            \"MSFT\": 15,\n",
    "            \"GOOGL\": 10,\n",
    "            \"AMZN\": 10,\n",
    "            \"META\": 10,\n",
    "            \"TSLA\": 10,\n",
    "            \"NVDA\": 15,\n",
    "            \"other\": 15  # Bonds, etc.\n",
    "        },\n",
    "        \"aggressive\": {\n",
    "            \"AAPL\": 20,\n",
    "            \"MSFT\": 20,\n",
    "            \"GOOGL\": 15,\n",
    "            \"AMZN\": 15,\n",
    "            \"META\": 10,\n",
    "            \"TSLA\": 10,\n",
    "            \"NVDA\": 10,\n",
    "            \"other\": 0  # Bonds, etc.\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Get target allocation for selected risk profile\n",
    "    targets = target_allocations.get(risk_profile, target_allocations[\"moderate\"])\n",
    "    \n",
    "    # Compare current allocation with targets\n",
    "    recommendations = []\n",
    "    for symbol, target_pct in targets.items():\n",
    "        if symbol == \"other\":\n",
    "            continue\n",
    "            \n",
    "        current_pct = allocation.get(symbol, 0)\n",
    "        difference = target_pct - current_pct\n",
    "        \n",
    "        if difference > 5:\n",
    "            recommendations.append(f\"Increase {symbol} allocation by approximately {difference:.1f}%\")\n",
    "        elif difference < -5:\n",
    "            recommendations.append(f\"Decrease {symbol} allocation by approximately {abs(difference):.1f}%\")\n",
    "    \n",
    "    # Check for diversification\n",
    "    if len(allocation) < 3:\n",
    "        recommendations.append(\"Consider adding more stocks to diversify your portfolio\")\n",
    "    \n",
    "    # Check for missing key stocks\n",
    "    for symbol in [\"AAPL\", \"MSFT\"]:\n",
    "        if symbol not in allocation and targets.get(symbol, 0) > 5:\n",
    "            recommendations.append(f\"Consider adding {symbol} to your portfolio\")\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Register new functions\n",
    "function_registry.register(calculate_portfolio_statistics, \"Calculate statistics for a portfolio of stocks\")\n",
    "function_registry.register(get_portfolio_allocation, \"Get the allocation percentages of a portfolio\")\n",
    "function_registry.register(recommend_portfolio_adjustments, \"Recommend adjustments to a portfolio based on risk profile\")\n",
    "\n",
    "# Create a portfolio analyzer assistant\n",
    "portfolio_analyzer = FinancialAssistant(use_api=False)  # Set to True to use API if available\n",
    "\n",
    "# Sample portfolio for testing\n",
    "sample_portfolio = {\n",
    "    \"AAPL\": 15,\n",
    "    \"MSFT\": 10,\n",
    "    \"GOOGL\": 5,\n",
    "    \"NVDA\": 8\n",
    "}\n",
    "\n",
    "# Test queries for portfolio analysis\n",
    "portfolio_queries = [\n",
    "    f\"Analyze this portfolio: {sample_portfolio}\",\n",
    "    \"What's the allocation of this portfolio?\",\n",
    "    \"Can you recommend any adjustments to this portfolio for a moderate risk profile?\",\n",
    "    \"What would be the total value if I added 10 more shares of NVDA?\",\n",
    "    \"Which stock in my portfolio has performed the best?\"\n",
    "]\n",
    "\n",
    "# Run interactive portfolio analysis\n",
    "print(\"\\n--- Portfolio Analyzer Demo ---\\n\")\n",
    "for query in portfolio_queries:\n",
    "    print(f\"User: {query}\")\n",
    "    response = portfolio_analyzer.generate_response(query)\n",
    "    print(f\"Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b146f19",
   "metadata": {},
   "source": [
    "## 8. Conclusion and Best Practices\n",
    "\n",
    "Let's summarize what we've learned about implementing memory, caching, and function calling in LLM applications for finance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25750291",
   "metadata": {},
   "source": [
    "### Key Takeaways\n",
    "\n",
    "1. **Memory Systems**\n",
    "   - Enable contextual conversations about financial topics\n",
    "   - Can be implemented with different strategies (basic, summary, vector)\n",
    "   - Critical for maintaining user context in financial advisory scenarios\n",
    "   - Help reduce token usage and costs while preserving relevant information\n",
    "\n",
    "2. **Caching Systems**\n",
    "   - Improve performance and reduce costs for repeated queries\n",
    "   - Ensure consistency in financial advice and analysis\n",
    "   - Can be implemented with different storage backends (file, database)\n",
    "   - Important to consider cache invalidation strategies for financial data\n",
    "\n",
    "3. **Function Calling**\n",
    "   - Connects LLMs to real-time financial data and calculations\n",
    "   - Enables more accurate and up-to-date financial analysis\n",
    "   - Can be implemented with different levels of sophistication\n",
    "   - Essential for building practical financial applications\n",
    "\n",
    "4. **Local vs. API Models**\n",
    "   - Choose based on your specific requirements for privacy, performance, and cost\n",
    "   - Consider regulatory compliance needs in financial applications\n",
    "   - Hybrid approaches may offer the best balance for financial use cases\n",
    "\n",
    "### Best Practices for Financial LLM Applications\n",
    "\n",
    "1. **Memory Management**\n",
    "   - Use summarization for long conversations about financial topics\n",
    "   - Store critical financial information (portfolio holdings, goals) separately\n",
    "   - Implement proper security measures for sensitive financial data\n",
    "   - Consider user-specific memory for personalized financial advice\n",
    "\n",
    "2. **Caching Strategy**\n",
    "   - Cache factual financial information with appropriate TTL\n",
    "   - Don't cache personalized financial advice without proper controls\n",
    "   - Implement versioning for cached responses\n",
    "   - Monitor cache hit rates to optimize performance\n",
    "\n",
    "3. **Function Implementation**\n",
    "   - Validate all inputs to financial functions\n",
    "   - Implement error handling for all external data sources\n",
    "   - Document the limitations of financial calculations\n",
    "   - Consider rate limits for external financial APIs\n",
    "\n",
    "4. **Security Considerations**\n",
    "   - Never expose API keys in client-side code\n",
    "   - Implement proper authentication for financial functions\n",
    "   - Log access to sensitive financial data\n",
    "   - Consider regulatory requirements (GDPR, CCPA, financial regulations)\n",
    "\n",
    "5. **Performance Optimization**\n",
    "   - Use the smallest model that meets your accuracy needs\n",
    "   - Implement batching for multiple financial calculations\n",
    "   - Consider asynchronous processing for long-running analyses\n",
    "   - Monitor and optimize token usage for cost control\n",
    "\n",
    "### Further Learning Resources\n",
    "\n",
    "- [HuggingFace Transformers Documentation](https://huggingface.co/docs/transformers/index)\n",
    "- [OpenAI Function Calling Guide](https://platform.openai.com/docs/guides/function-calling)\n",
    "- [LangChain Memory Documentation](https://js.langchain.com/docs/modules/memory/)\n",
    "- [Financial Data APIs](https://polygon.io/docs)\n",
    "- [Regulatory Considerations for AI in Finance](https://www.finra.org/rules-guidance/key-topics/fintech/report/artificial-intelligence-in-the-securities-industry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabad1e0",
   "metadata": {},
   "source": [
    "## 9. Assignment: Build Your Own Financial Chatbot\n",
    "\n",
    "As a practical assignment, try building your own financial chatbot with the following features:\n",
    "\n",
    "1. **Memory System**\n",
    "   - Implement at least one type of memory system\n",
    "   - Test with a multi-turn conversation about financial planning\n",
    "\n",
    "2. **Caching**\n",
    "   - Set up a caching system for LLM responses\n",
    "   - Measure and report performance improvements\n",
    "\n",
    "3. **Financial Functions**\n",
    "   - Implement at least three financial functions (e.g., investment calculator, stock data retrieval)\n",
    "   - Create a function calling mechanism to use these functions\n",
    "\n",
    "4. **User Interface**\n",
    "   - Build a simple interface for your chatbot (CLI, web, or notebook)\n",
    "   - Support multi-turn conversations about financial topics\n",
    "\n",
    "5. **Documentation**\n",
    "   - Document your architecture and design decisions\n",
    "   - Explain how you addressed memory, caching, and function calling\n",
    "\n",
    "Submit your code, documentation, and a demo of your chatbot in action.\n",
    "\n",
    "Good luck!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
