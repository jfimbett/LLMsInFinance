{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd91f75e",
   "metadata": {},
   "source": [
    "# Reading Large Financial Documents with LLMs\n",
    "\n",
    "In this notebook, we'll explore techniques for efficiently processing large financial documents such as annual reports, SEC filings, and earnings call transcripts using Large Language Models. \n",
    "\n",
    "These documents often contain critical information for valuation, but their length and complexity make them challenging to analyze manually. We'll demonstrate how LLMs can help extract, summarize, and analyze this information effectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb565202",
   "metadata": {},
   "source": [
    "## 1. Introduction: The Challenge of Large Financial Documents\n",
    "\n",
    "Financial documents present several unique challenges:\n",
    "\n",
    "- **Length**: Annual reports and SEC filings often exceed 100+ pages\n",
    "- **Structure**: Mix of narrative text, tables, charts, and footnotes\n",
    "- **Technical language**: Industry-specific terminology and financial jargon\n",
    "- **Information dispersal**: Important details scattered throughout the document\n",
    "- **Contextual understanding**: Requires understanding relationships between different sections\n",
    "\n",
    "Traditional approaches like keyword searching or rule-based extraction often miss important context or nuances. LLMs, with their ability to understand natural language and maintain context, offer a promising solution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35022ce",
   "metadata": {},
   "source": [
    "## 2. Setting Up the Environment\n",
    "\n",
    "First, let's install and import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4984c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q openai pandas numpy tiktoken PyPDF2 requests beautifulsoup4 nltk transformers torch langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f0da24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import requests\n",
    "import PyPDF2\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import tiktoken\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from openai import OpenAI\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# If you're using your own API key, uncomment and replace with your key:\n",
    "# client = OpenAI(api_key=\"your-api-key-here\")\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f60431",
   "metadata": {},
   "source": [
    "## 3. Document Acquisition and Loading\n",
    "\n",
    "Let's start by downloading Tesla's 2022 Annual Report (10-K) as our example document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec1c087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to download and load the 10-K filing\n",
    "def download_sec_filing(ticker, filing_type=\"10-K\", year=2022):\n",
    "    \"\"\"\n",
    "    Download an SEC filing for a given ticker, filing type and year\n",
    "    \"\"\"\n",
    "    print(f\"Downloading {filing_type} for {ticker} ({year})...\")\n",
    "    \n",
    "    # This is a simplified example - in a real application, you'd use the SEC EDGAR API\n",
    "    # For demonstration, we'll use a direct link to Tesla's 2022 10-K\n",
    "    \n",
    "    # In a production environment, you would use:\n",
    "    # - SEC EDGAR API\n",
    "    # - Financial data provider APIs (Bloomberg, Refinitiv, FactSet)\n",
    "    # - Web scraping with proper rate limiting and user agents\n",
    "    \n",
    "    if ticker == \"TSLA\" and filing_type == \"10-K\" and year == 2022:\n",
    "        url = \"https://www.sec.gov/Archives/edgar/data/1318605/000095017023001409/tsla-20221231.htm\"\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            # Save the file locally\n",
    "            with open(\"tesla_10k_2022.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "                f.write(response.text)\n",
    "            \n",
    "            # Parse HTML content\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            # Extract text (this is simplified - proper extraction would require more processing)\n",
    "            text = soup.get_text()\n",
    "            \n",
    "            # Clean up the text\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            \n",
    "            print(f\"Successfully downloaded filing ({len(text)} characters)\")\n",
    "            return text\n",
    "        else:\n",
    "            print(f\"Failed to download filing. Status code: {response.status_code}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"For this demo, only Tesla's 2022 10-K is supported.\")\n",
    "        return None\n",
    "\n",
    "# Download Tesla's 2022 10-K\n",
    "tesla_10k = download_sec_filing(\"TSLA\", \"10-K\", 2022)\n",
    "\n",
    "# Display the first 1000 characters to verify the content\n",
    "if tesla_10k:\n",
    "    print(\"\\nPreview of the document:\")\n",
    "    print(tesla_10k[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e07d94",
   "metadata": {},
   "source": [
    "## 4. Understanding Document Length Constraints\n",
    "\n",
    "LLMs have token limits that restrict how much text they can process at once. Let's examine how to handle large documents that exceed these limits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09814ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count tokens in a text\n",
    "def count_tokens(text, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Count the number of tokens in a text string\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "# Count tokens in the 10-K\n",
    "if tesla_10k:\n",
    "    token_count = count_tokens(tesla_10k)\n",
    "    print(f\"The Tesla 10-K contains approximately {token_count:,} tokens\")\n",
    "    \n",
    "    # Common model context windows\n",
    "    context_windows = {\n",
    "        \"gpt-3.5-turbo\": 16_385,\n",
    "        \"gpt-4\": 8_192,\n",
    "        \"gpt-4-turbo\": 128_000,\n",
    "        \"claude-2\": 100_000,\n",
    "        \"llama-2-70b\": 4_096\n",
    "    }\n",
    "    \n",
    "    # Create a dataframe to visualize token limits\n",
    "    models_df = pd.DataFrame({\n",
    "        'Model': list(context_windows.keys()),\n",
    "        'Context Window (tokens)': list(context_windows.values()),\n",
    "        'Can Process Full Document?': [\n",
    "            'Yes' if window >= token_count else 'No' \n",
    "            for window in context_windows.values()\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\nModel Context Windows vs. Document Size:\")\n",
    "    print(models_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce05138",
   "metadata": {},
   "source": [
    "## 5. Chunking Strategies for Large Documents\n",
    "\n",
    "When documents exceed token limits, we need effective chunking strategies that preserve context and coherence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2262d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define different chunking strategies\n",
    "\n",
    "def chunk_by_characters(text, chunk_size=4000, overlap=200):\n",
    "    \"\"\"Split text into chunks of approximately equal character length with overlap\"\"\"\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    \n",
    "    while start < len(text):\n",
    "        # Find the end of the chunk\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        \n",
    "        # If we're not at the end of the text, try to break at a sentence boundary\n",
    "        if end < len(text):\n",
    "            # Look for the last period followed by a space within the overlap window\n",
    "            last_period = text.rfind('. ', end - overlap, end)\n",
    "            if last_period != -1:\n",
    "                end = last_period + 2  # Include the period and space\n",
    "                \n",
    "        # Extract the chunk and add it to our list\n",
    "        chunk = text[start:end]\n",
    "        chunks.append(chunk)\n",
    "        \n",
    "        # Move the start pointer, ensuring overlap\n",
    "        start = end - overlap if end < len(text) else end\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_by_sections(text, section_patterns=[r'Item \\d+\\.', r'PART [IVX]+\\.']):\n",
    "    \"\"\"Split text by regulatory document sections (Items and Parts)\"\"\"\n",
    "    chunks = []\n",
    "    \n",
    "    # Combine all patterns into a single regex\n",
    "    combined_pattern = '|'.join(f'({pattern})' for pattern in section_patterns)\n",
    "    \n",
    "    # Find all section headers\n",
    "    matches = list(re.finditer(combined_pattern, text))\n",
    "    \n",
    "    # Process each section\n",
    "    for i, match in enumerate(matches):\n",
    "        # Determine section start and end\n",
    "        start = match.start()\n",
    "        end = matches[i+1].start() if i < len(matches) - 1 else len(text)\n",
    "        \n",
    "        # Extract the section\n",
    "        section = text[start:end]\n",
    "        \n",
    "        # Further chunk if section is too large\n",
    "        if count_tokens(section) > 8000:  # Assuming a target of 8000 tokens per chunk\n",
    "            subsections = chunk_by_characters(section, 4000, 200)\n",
    "            chunks.extend(subsections)\n",
    "        else:\n",
    "            chunks.append(section)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def chunk_by_semantic_content(text, max_tokens=8000):\n",
    "    \"\"\"\n",
    "    Split text by trying to preserve semantic units (paragraphs, sections)\n",
    "    This is a simplified version - a production implementation would be more sophisticated\n",
    "    \"\"\"\n",
    "    # Split by paragraphs (double newlines)\n",
    "    paragraphs = re.split(r'\\n\\n+', text)\n",
    "    \n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    current_tokens = 0\n",
    "    \n",
    "    for para in paragraphs:\n",
    "        para_tokens = count_tokens(para)\n",
    "        \n",
    "        # If adding this paragraph would exceed the limit, start a new chunk\n",
    "        if current_tokens + para_tokens > max_tokens and current_chunk:\n",
    "            chunks.append(current_chunk)\n",
    "            current_chunk = para\n",
    "            current_tokens = para_tokens\n",
    "        else:\n",
    "            # Add to the current chunk\n",
    "            if current_chunk:\n",
    "                current_chunk += \"\\n\\n\" + para\n",
    "            else:\n",
    "                current_chunk = para\n",
    "            current_tokens += para_tokens\n",
    "    \n",
    "    # Add the last chunk if it's not empty\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "# Apply different chunking strategies to our document\n",
    "if tesla_10k:\n",
    "    # Character-based chunking\n",
    "    char_chunks = chunk_by_characters(tesla_10k)\n",
    "    \n",
    "    # Section-based chunking\n",
    "    section_chunks = chunk_by_sections(tesla_10k)\n",
    "    \n",
    "    # Semantic chunking\n",
    "    semantic_chunks = chunk_by_semantic_content(tesla_10k)\n",
    "    \n",
    "    # Compare the results\n",
    "    chunking_results = pd.DataFrame({\n",
    "        'Chunking Method': ['Character-based', 'Section-based', 'Semantic-based'],\n",
    "        'Number of Chunks': [len(char_chunks), len(section_chunks), len(semantic_chunks)],\n",
    "        'Avg. Tokens per Chunk': [\n",
    "            int(sum(count_tokens(chunk) for chunk in char_chunks) / len(char_chunks)),\n",
    "            int(sum(count_tokens(chunk) for chunk in section_chunks) / len(section_chunks)),\n",
    "            int(sum(count_tokens(chunk) for chunk in semantic_chunks) / len(semantic_chunks))\n",
    "        ],\n",
    "        'Max Tokens in a Chunk': [\n",
    "            max(count_tokens(chunk) for chunk in char_chunks),\n",
    "            max(count_tokens(chunk) for chunk in section_chunks),\n",
    "            max(count_tokens(chunk) for chunk in semantic_chunks)\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"Comparison of Chunking Strategies:\")\n",
    "    print(chunking_results)\n",
    "    \n",
    "    # Let's choose the section-based approach for further analysis\n",
    "    chunks = section_chunks\n",
    "    print(f\"\\nWe'll proceed with {len(chunks)} section-based chunks for our analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8a07b3",
   "metadata": {},
   "source": [
    "## 6. Retrieval Augmented Generation (RAG) for Document Analysis\n",
    "\n",
    "Now that we've chunked our document, we can use Retrieval Augmented Generation (RAG) to analyze specific aspects of the filing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5def0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's create embeddings for each chunk to enable semantic search\n",
    "# For production use, consider using:\n",
    "# - OpenAI's text-embedding-ada-002\n",
    "# - HuggingFace's all-MiniLM-L6-v2\n",
    "# - Other embedding models specialized for financial text\n",
    "\n",
    "# For this example, we'll use a simple function to simulate embeddings\n",
    "def create_embeddings(chunks):\n",
    "    \"\"\"\n",
    "    Create embeddings for each chunk\n",
    "    In a real application, you would use a proper embedding model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize a pre-trained model for embeddings\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        model = AutoModel.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "        \n",
    "        embeddings = []\n",
    "        \n",
    "        # Process chunks in batches to avoid memory issues\n",
    "        for chunk in chunks:\n",
    "            # Tokenize and get model outputs\n",
    "            inputs = tokenizer(chunk, padding=True, truncation=True, \n",
    "                              return_tensors=\"pt\", max_length=512)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "            \n",
    "            # Use the CLS token embedding as the sentence embedding\n",
    "            embedding = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "            embeddings.append(embedding[0])  # Add the embedding vector\n",
    "            \n",
    "        print(f\"Created embeddings for {len(chunks)} chunks successfully!\")\n",
    "        return np.array(embeddings)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating embeddings: {e}\")\n",
    "        # Fall back to a simple TF-IDF approach\n",
    "        from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "        \n",
    "        vectorizer = TfidfVectorizer()\n",
    "        embeddings = vectorizer.fit_transform(chunks).toarray()\n",
    "        print(f\"Created TF-IDF vectors for {len(chunks)} chunks as fallback.\")\n",
    "        return embeddings\n",
    "\n",
    "# Create embeddings for our chunks\n",
    "if 'chunks' in locals():\n",
    "    chunk_embeddings = create_embeddings(chunks)\n",
    "    print(f\"Embedding shape: {chunk_embeddings.shape}\")\n",
    "    \n",
    "    # Function to find the most relevant chunks for a query\n",
    "    def find_relevant_chunks(query, chunks, embeddings, top_n=3):\n",
    "        \"\"\"Find the most relevant chunks for a given query\"\"\"\n",
    "        # Create query embedding\n",
    "        query_embedding = create_embeddings([query])[0]\n",
    "        \n",
    "        # Calculate similarity scores\n",
    "        similarities = np.dot(embeddings, query_embedding) / (\n",
    "            np.linalg.norm(embeddings, axis=1) * np.linalg.norm(query_embedding)\n",
    "        )\n",
    "        \n",
    "        # Get indices of top_n most similar chunks\n",
    "        top_indices = np.argsort(similarities)[-top_n:][::-1]\n",
    "        \n",
    "        # Return the top chunks and their similarity scores\n",
    "        return [(chunks[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    # Example: Find chunks related to \"risk factors\"\n",
    "    risk_chunks = find_relevant_chunks(\"risk factors\", chunks, chunk_embeddings)\n",
    "    \n",
    "    print(\"\\nTop chunks related to 'risk factors':\")\n",
    "    for i, (chunk, score) in enumerate(risk_chunks):\n",
    "        print(f\"\\nChunk {i+1} (similarity score: {score:.2f}):\")\n",
    "        print(f\"{chunk[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0836af",
   "metadata": {},
   "source": [
    "## 7. Extracting Structured Information from Financial Documents\n",
    "\n",
    "Now let's use an LLM to extract specific structured information from our document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098664bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract structured information using an LLM\n",
    "def extract_information(chunk, extraction_prompt, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Extract structured information from a document chunk using an LLM\"\"\"\n",
    "    \n",
    "    # Create a system prompt for the extraction task\n",
    "    system_prompt = \"\"\"You are a financial analyst specializing in SEC filings analysis.\n",
    "    Extract the requested information from the provided text from an SEC filing.\n",
    "    Provide your response in JSON format according to the schema specified.\n",
    "    If information is not found, indicate with \"Not found\" or appropriate null values.\"\"\"\n",
    "    \n",
    "    # Check if we're exceeding token limits\n",
    "    combined_text = system_prompt + extraction_prompt + chunk\n",
    "    token_count = count_tokens(combined_text)\n",
    "    \n",
    "    # If too large, truncate the chunk\n",
    "    if token_count > 15000:  # Leave room for response\n",
    "        print(f\"Warning: Truncating chunk to fit within token limit (current: {token_count})\")\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        tokens = encoding.encode(chunk)\n",
    "        # Truncate to approximately 10000 tokens\n",
    "        truncated_tokens = tokens[:10000]\n",
    "        chunk = encoding.decode(truncated_tokens)\n",
    "        print(f\"New token count: {count_tokens(system_prompt + extraction_prompt + chunk)}\")\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{extraction_prompt}\\n\\nTEXT FROM SEC FILING:\\n{chunk}\"}\n",
    "            ],\n",
    "            temperature=0.0,  # Low temperature for factual extraction\n",
    "            response_format={\"type\": \"json_object\"}  # Request JSON output\n",
    "        )\n",
    "        \n",
    "        # Extract and parse the JSON response\n",
    "        result = json.loads(response.choices[0].message.content)\n",
    "        return result\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during information extraction: {e}\")\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "# Example: Extract risk factors from relevant chunks\n",
    "if 'risk_chunks' in locals():\n",
    "    # Extraction prompt for risk factors\n",
    "    risk_factors_prompt = \"\"\"\n",
    "    Extract the top risk factors mentioned in the filing text.\n",
    "    For each risk factor:\n",
    "    1. Identify the title or main category\n",
    "    2. Provide a brief summary\n",
    "    3. Note any quantitative impacts mentioned (if any)\n",
    "    4. Assess the severity (High, Medium, Low) based on the language used\n",
    "    \n",
    "    Return the information in the following JSON format:\n",
    "    {\n",
    "        \"risk_factors\": [\n",
    "            {\n",
    "                \"title\": \"Risk factor title/category\",\n",
    "                \"summary\": \"Brief summary of the risk\",\n",
    "                \"quantitative_impact\": \"Any numbers/percentages mentioned\",\n",
    "                \"severity\": \"High/Medium/Low assessment\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    Limit your response to the top 5 most significant risk factors.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract risk factors from the most relevant chunk\n",
    "    most_relevant_chunk = risk_chunks[0][0]  # First chunk from our risk_chunks list\n",
    "    risk_factors = extract_information(most_relevant_chunk, risk_factors_prompt)\n",
    "    \n",
    "    # Display the extracted risk factors\n",
    "    print(\"\\nExtracted Risk Factors:\")\n",
    "    if \"risk_factors\" in risk_factors:\n",
    "        for i, factor in enumerate(risk_factors[\"risk_factors\"]):\n",
    "            print(f\"\\n{i+1}. {factor['title']}\")\n",
    "            print(f\"   Summary: {factor['summary']}\")\n",
    "            print(f\"   Quantitative Impact: {factor['quantitative_impact']}\")\n",
    "            print(f\"   Severity: {factor['severity']}\")\n",
    "    else:\n",
    "        print(\"No risk factors were extracted or there was an error.\")\n",
    "        print(risk_factors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f22cd88",
   "metadata": {},
   "source": [
    "## 8. Document Summarization Techniques\n",
    "\n",
    "Different sections of financial documents require different summarization approaches. Let's demonstrate how to create tailored summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c59a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate different types of summaries\n",
    "def generate_summary(chunk, summary_type, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Generate different types of summaries from document chunks\n",
    "    \n",
    "    summary_type options:\n",
    "    - 'executive': High-level overview for executives\n",
    "    - 'detailed': Comprehensive summary with key details\n",
    "    - 'comparative': Summary comparing to previous periods\n",
    "    - 'implications': Focus on business implications\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define prompts for different summary types\n",
    "    prompts = {\n",
    "        'executive': \"\"\"\n",
    "        Create a concise executive summary of this SEC filing excerpt in 3-5 bullet points.\n",
    "        Focus on the most important information that a C-suite executive would need to know.\n",
    "        Highlight material changes, strategic implications, and key risks or opportunities.\n",
    "        \"\"\",\n",
    "        \n",
    "        'detailed': \"\"\"\n",
    "        Create a detailed yet concise summary of this SEC filing excerpt.\n",
    "        Include:\n",
    "        - Key financial metrics and changes\n",
    "        - Important business developments\n",
    "        - Significant risks and contingencies\n",
    "        - Management's strategic focus areas\n",
    "        - Regulatory issues or concerns\n",
    "        \n",
    "        Your summary should be comprehensive but focused on material information.\n",
    "        \"\"\",\n",
    "        \n",
    "        'comparative': \"\"\"\n",
    "        Summarize this SEC filing excerpt with a focus on year-over-year or quarter-over-quarter changes.\n",
    "        Highlight:\n",
    "        - Growth or decline in key metrics\n",
    "        - Changing trends in the business\n",
    "        - Evolution of risks or opportunities\n",
    "        - Shifts in management focus or strategy\n",
    "        \n",
    "        Specifically note what has changed compared to previous periods.\n",
    "        \"\"\",\n",
    "        \n",
    "        'implications': \"\"\"\n",
    "        Analyze this SEC filing excerpt and summarize the business implications.\n",
    "        Focus on:\n",
    "        - What these developments mean for the company's future\n",
    "        - Potential impacts on competitive positioning\n",
    "        - Implications for stakeholders (investors, customers, employees)\n",
    "        - Forward-looking indicators\n",
    "        \n",
    "        Your summary should focus on \"what it means\" rather than just \"what happened.\"\n",
    "        \"\"\"\n",
    "    }\n",
    "    \n",
    "    # Default to executive summary if an invalid type is provided\n",
    "    if summary_type not in prompts:\n",
    "        print(f\"Warning: Unknown summary type '{summary_type}'. Using 'executive' instead.\")\n",
    "        summary_type = 'executive'\n",
    "    \n",
    "    system_prompt = \"\"\"You are a financial analyst specializing in SEC filings analysis.\n",
    "    Summarize the provided text from an SEC filing according to the specific requirements.\n",
    "    Focus only on factual information present in the text. Do not add speculative content.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"{prompts[summary_type]}\\n\\nTEXT FROM SEC FILING:\\n{chunk}\"}\n",
    "            ],\n",
    "            temperature=0.2,  # Low temperature for factual summary\n",
    "        )\n",
    "        \n",
    "        # Extract the summary\n",
    "        summary = response.choices[0].message.content\n",
    "        return summary\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error during summary generation: {e}\")\n",
    "        return f\"Error generating summary: {str(e)}\"\n",
    "\n",
    "# Example: Generate different types of summaries for the MD&A section\n",
    "if 'chunks' in locals():\n",
    "    # Find the MD&A section (Management Discussion and Analysis)\n",
    "    mda_chunks = find_relevant_chunks(\"Management's Discussion and Analysis\", chunks, chunk_embeddings, top_n=1)\n",
    "    \n",
    "    if mda_chunks:\n",
    "        mda_chunk = mda_chunks[0][0]\n",
    "        \n",
    "        # Generate different types of summaries\n",
    "        summary_types = ['executive', 'detailed', 'implications']\n",
    "        summaries = {}\n",
    "        \n",
    "        for summary_type in summary_types:\n",
    "            print(f\"\\nGenerating {summary_type} summary...\")\n",
    "            summaries[summary_type] = generate_summary(mda_chunk[:5000], summary_type)  # Using first 5000 chars for brevity\n",
    "        \n",
    "        # Display the summaries\n",
    "        for summary_type, summary in summaries.items():\n",
    "            print(f\"\\n--- {summary_type.upper()} SUMMARY ---\")\n",
    "            print(summary)\n",
    "    else:\n",
    "        print(\"Could not find MD&A section in the document.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529493de",
   "metadata": {},
   "source": [
    "## 9. Extracting Financial Metrics and KPIs\n",
    "\n",
    "Financial documents contain numerous metrics and KPIs that are crucial for valuation. Let's extract these in a structured format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd76839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract financial metrics\n",
    "def extract_financial_metrics(chunks, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Extract key financial metrics from document chunks\"\"\"\n",
    "    \n",
    "    # Find chunks likely to contain financial information\n",
    "    financial_chunks = find_relevant_chunks(\"financial statements revenue profit margin\", \n",
    "                                           chunks, chunk_embeddings, top_n=2)\n",
    "    \n",
    "    # Extract metrics from each chunk\n",
    "    metrics_prompt = \"\"\"\n",
    "    Extract key financial metrics and KPIs from the provided SEC filing text.\n",
    "    Include:\n",
    "    1. Revenue figures (total and by segment if available)\n",
    "    2. Profit metrics (gross profit, operating income, net income)\n",
    "    3. Margin percentages (gross margin, operating margin, net margin)\n",
    "    4. Growth rates (YoY or QoQ changes)\n",
    "    5. Any other key performance indicators mentioned\n",
    "    \n",
    "    Return the information in the following JSON format:\n",
    "    {\n",
    "        \"time_period\": \"The time period these metrics refer to\",\n",
    "        \"revenue\": {\n",
    "            \"total\": \"Total revenue figure with units\",\n",
    "            \"segments\": [\n",
    "                {\"name\": \"Segment name\", \"value\": \"Segment revenue with units\"}\n",
    "            ]\n",
    "        },\n",
    "        \"profit\": {\n",
    "            \"gross_profit\": \"Figure with units\",\n",
    "            \"operating_income\": \"Figure with units\",\n",
    "            \"net_income\": \"Figure with units\"\n",
    "        },\n",
    "        \"margins\": {\n",
    "            \"gross_margin\": \"Percentage\",\n",
    "            \"operating_margin\": \"Percentage\",\n",
    "            \"net_margin\": \"Percentage\"\n",
    "        },\n",
    "        \"growth\": {\n",
    "            \"revenue_growth\": \"Percentage\",\n",
    "            \"income_growth\": \"Percentage\"\n",
    "        },\n",
    "        \"other_kpis\": [\n",
    "            {\"name\": \"KPI name\", \"value\": \"KPI value\", \"description\": \"Brief description\"}\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    If any information is not available, use null or appropriate placeholder.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_metrics = []\n",
    "    \n",
    "    for i, (chunk, score) in enumerate(financial_chunks):\n",
    "        print(f\"\\nExtracting metrics from financial chunk {i+1}...\")\n",
    "        chunk_metrics = extract_information(chunk, metrics_prompt)\n",
    "        all_metrics.append(chunk_metrics)\n",
    "    \n",
    "    # Combine and deduplicate metrics\n",
    "    # In a real application, you would implement more sophisticated deduplication and validation\n",
    "    combined_metrics = all_metrics[0] if all_metrics else {}\n",
    "    \n",
    "    return combined_metrics\n",
    "\n",
    "# Extract financial metrics\n",
    "if 'chunks' in locals():\n",
    "    financial_metrics = extract_financial_metrics(chunks)\n",
    "    \n",
    "    # Display the extracted metrics\n",
    "    print(\"\\nExtracted Financial Metrics:\")\n",
    "    print(json.dumps(financial_metrics, indent=2))\n",
    "    \n",
    "    # Visualize some key metrics if available\n",
    "    if financial_metrics and 'revenue' in financial_metrics and 'segments' in financial_metrics['revenue']:\n",
    "        # Extract segment revenues for visualization\n",
    "        segments = financial_metrics['revenue']['segments']\n",
    "        if segments and len(segments) > 1:\n",
    "            segment_names = [seg['name'] for seg in segments if 'name' in seg and 'value' in seg]\n",
    "            \n",
    "            # Extract values and convert to numeric (removing non-numeric characters)\n",
    "            segment_values = []\n",
    "            for seg in segments:\n",
    "                if 'value' in seg:\n",
    "                    # Extract numeric value from string like \"$10.2 billion\"\n",
    "                    value_str = seg['value']\n",
    "                    numeric_value = re.findall(r'[\\d.]+', value_str)\n",
    "                    if numeric_value:\n",
    "                        # Convert to float and scale based on units\n",
    "                        value = float(numeric_value[0])\n",
    "                        if 'billion' in value_str.lower():\n",
    "                            value *= 1000\n",
    "                        segment_values.append(value)\n",
    "                    else:\n",
    "                        segment_values.append(0)\n",
    "            \n",
    "            # Create a simple bar chart\n",
    "            if segment_names and segment_values and len(segment_names) == len(segment_values):\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                plt.bar(segment_names, segment_values)\n",
    "                plt.title('Revenue by Segment (in millions USD)')\n",
    "                plt.ylabel('Revenue (USD Millions)')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.tight_layout()\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc353cdc",
   "metadata": {},
   "source": [
    "## 10. Identifying and Analyzing Forward-Looking Statements\n",
    "\n",
    "Forward-looking statements in financial documents provide crucial insights for forecasting future performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57073d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify and analyze forward-looking statements\n",
    "def extract_forward_looking_statements(chunks, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"Extract and analyze forward-looking statements from document chunks\"\"\"\n",
    "    \n",
    "    # Find chunks likely to contain forward-looking statements\n",
    "    fls_chunks = find_relevant_chunks(\"forward-looking statements future outlook guidance\", \n",
    "                                     chunks, chunk_embeddings, top_n=3)\n",
    "    \n",
    "    # Extract forward-looking statements from each chunk\n",
    "    fls_prompt = \"\"\"\n",
    "    Identify and analyze forward-looking statements in the provided SEC filing text.\n",
    "    \n",
    "    For each significant forward-looking statement:\n",
    "    1. Extract the exact statement or a close paraphrase\n",
    "    2. Categorize it (e.g., Revenue Projection, Product Development, Market Expansion)\n",
    "    3. Note any timeframes mentioned\n",
    "    4. Identify any quantitative targets or metrics\n",
    "    5. Assess the confidence level based on the language used (High, Medium, Low)\n",
    "    \n",
    "    Return the information in the following JSON format:\n",
    "    {\n",
    "        \"forward_looking_statements\": [\n",
    "            {\n",
    "                \"statement\": \"The extracted statement\",\n",
    "                \"category\": \"Category of the statement\",\n",
    "                \"timeframe\": \"Mentioned timeframe if any\",\n",
    "                \"quantitative_targets\": \"Any specific numbers/percentages\",\n",
    "                \"confidence_level\": \"High/Medium/Low\",\n",
    "                \"analysis\": \"Brief analysis of implications\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    Limit your response to the top 5-7 most significant forward-looking statements.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_statements = []\n",
    "    \n",
    "    for i, (chunk, score) in enumerate(fls_chunks):\n",
    "        print(f\"\\nExtracting forward-looking statements from chunk {i+1}...\")\n",
    "        chunk_statements = extract_information(chunk, fls_prompt)\n",
    "        \n",
    "        if 'forward_looking_statements' in chunk_statements:\n",
    "            all_statements.extend(chunk_statements['forward_looking_statements'])\n",
    "    \n",
    "    # Deduplicate statements (simplified approach)\n",
    "    unique_statements = []\n",
    "    statement_texts = set()\n",
    "    \n",
    "    for statement in all_statements:\n",
    "        # Use statement text as a deduplication key\n",
    "        if 'statement' in statement and statement['statement'] not in statement_texts:\n",
    "            statement_texts.add(statement['statement'])\n",
    "            unique_statements.append(statement)\n",
    "    \n",
    "    return {\"forward_looking_statements\": unique_statements}\n",
    "\n",
    "# Extract forward-looking statements\n",
    "if 'chunks' in locals():\n",
    "    forward_looking = extract_forward_looking_statements(chunks)\n",
    "    \n",
    "    # Display the extracted statements\n",
    "    print(\"\\nExtracted Forward-Looking Statements:\")\n",
    "    if 'forward_looking_statements' in forward_looking:\n",
    "        for i, statement in enumerate(forward_looking['forward_looking_statements']):\n",
    "            print(f\"\\n{i+1}. {statement.get('statement', 'N/A')}\")\n",
    "            print(f\"   Category: {statement.get('category', 'N/A')}\")\n",
    "            print(f\"   Timeframe: {statement.get('timeframe', 'N/A')}\")\n",
    "            print(f\"   Targets: {statement.get('quantitative_targets', 'N/A')}\")\n",
    "            print(f\"   Confidence: {statement.get('confidence_level', 'N/A')}\")\n",
    "            print(f\"   Analysis: {statement.get('analysis', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b227b8",
   "metadata": {},
   "source": [
    "## 11. Best Practices for Financial Document Analysis with LLMs\n",
    "\n",
    "Based on our exploration in this notebook, here are key best practices for using LLMs to analyze large financial documents:\n",
    "\n",
    "1. **Effective Chunking**:\n",
    "   - Use semantically meaningful chunks (sections, topics) when possible\n",
    "   - Maintain appropriate overlap between chunks to preserve context\n",
    "   - Consider document structure (items, parts, sections) for SEC filings\n",
    "\n",
    "2. **Retrieval Strategy**:\n",
    "   - Use embedding-based retrieval for finding relevant sections\n",
    "   - Implement hybrid retrieval (combining semantic and keyword search)\n",
    "   - Consider document metadata for improved retrieval\n",
    "\n",
    "3. **Prompt Engineering**:\n",
    "   - Use specific, detailed prompts for financial analysis tasks\n",
    "   - Request structured output formats (JSON) for consistency\n",
    "   - Include financial domain knowledge in system prompts\n",
    "\n",
    "4. **Verification and Validation**:\n",
    "   - Cross-check extracted information across multiple chunks\n",
    "   - Validate numerical data against primary sources\n",
    "   - Use multiple extraction approaches for critical information\n",
    "\n",
    "5. **Context Management**:\n",
    "   - Provide relevant historical context for time-sensitive analysis\n",
    "   - Include document metadata (filing type, date, company) in prompts\n",
    "   - Maintain awareness of reporting period boundaries\n",
    "\n",
    "6. **Handling Numerical Data**:\n",
    "   - Request specific units for financial figures\n",
    "   - Validate consistency of numerical extractions\n",
    "   - Check for order-of-magnitude errors in extracted values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a24aa3",
   "metadata": {},
   "source": [
    "## 12. Conclusion and Next Steps\n",
    "\n",
    "In this notebook, we've explored techniques for efficiently processing large financial documents using LLMs. We've covered:\n",
    "\n",
    "- Document acquisition and preprocessing\n",
    "- Chunking strategies for handling large texts\n",
    "- Retrieval-augmented generation for targeted analysis\n",
    "- Extracting structured information from unstructured text\n",
    "- Summarization techniques for different purposes\n",
    "- Extraction of financial metrics and forward-looking statements\n",
    "\n",
    "These capabilities provide a foundation for more advanced financial analysis tasks, such as:\n",
    "\n",
    "- **Forecasting cash flows**: Using extracted metrics and forward-looking statements\n",
    "- **Comparable company analysis**: Identifying peer companies mentioned in documents\n",
    "- **Risk assessment**: Analyzing risk factors and uncertainties\n",
    "- **Valuation model inputs**: Extracting key parameters for DCF and multiples-based valuation\n",
    "\n",
    "In the next notebook, we'll explore how to use these extracted insights to generate financial forecasts for valuation purposes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
