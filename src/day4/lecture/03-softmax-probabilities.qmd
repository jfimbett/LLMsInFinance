---
title: "Understanding Softmax Probabilities"
format: revealjs
---

# Understanding Softmax Probabilities

## Softmax Function Fundamentals

:::: {.columns}

::: {.column width="60%"}
- **Definition and purpose**:
  - Converts raw model scores (logits) to probabilities
  - Ensures outputs sum to 1.0
  - Maintains relative ordering of inputs
  - Creates a proper probability distribution
  
- **Mathematical formulation**:
  - $P(y_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}$
  - $z_i$ is the logit (raw score) for class $i$
  - $K$ is the total number of classes
  - $P(y_i)$ is the probability of class $i$
  
- **Properties**:
  - Outputs in range [0, 1]
  - Sum of outputs equals 1
  - Preserves rank ordering of inputs
  - Non-linear transformation
  - Differentiable (important for training)
:::

::: {.column width="40%"}
**Softmax Example (in words):**

Suppose a model produces three logits: 2.0, 1.0, and 0.1. To convert these to probabilities:

1. Subtract the maximum logit from each value for numerical stability.
2. Exponentiate each adjusted logit.
3. Divide each exponentiated value by the sum of all exponentiated values.

For logits [2.0, 1.0, 0.1], the resulting probabilities are approximately [0.66, 0.24, 0.10].

- The sum of probabilities is always 1.0.
- The largest logit gets the highest probability.
:::

::::

## Softmax in Neural Networks

:::: {.columns}

::: {.column width="60%"}
- **Role in classification models**:
  - Final layer activation function
  - Converts network outputs to class probabilities
  - Used with cross-entropy loss function
  - Enables probabilistic interpretation
  
- **Common applications**:
  - Multi-class classification
  - Token prediction in language models
  - Entity type classification
  - Topic modeling
  - Sentiment analysis
  
- **Gradient properties**:
  - Simple derivative form
  - Well-behaved gradients
  - Compatible with backpropagation
  - Stable with appropriate scaling
:::

::: {.column width="40%"}
- **Credit risk applications**:
  
  **Softmax in credit classification**:
  - Default probability estimation
  - Credit rating assignment (AAA, AA, A, BBB...)
  - Risk category classification (Low, Medium, High)
  - Loan approval likelihood scoring
  
  **Why softmax works for credit**:
  - Ensures probabilities sum to 1
  - Natural ranking of credit outcomes
  - Smooth gradient flow for training
  - Interpretable probability outputs
:::

::::

## Interpreting Softmax Outputs

:::: {.columns}

::: {.column width="60%"}
- **Probability vs. confidence**:
  - Not true probabilities in Bayesian sense
  - Relative preference between classes
  - Influenced by model architecture
  - Affected by training data distribution
  
- **Typical behavior patterns**:
  - Overconfidence in standard models
  - Concentration on few classes
  - Sensitivity to input perturbations
  - Correlation with prediction accuracy
  
- **Shape characteristics**:
  - Peakedness for confident predictions
  - Uniform distribution for uncertainty
  - Bimodal for ambiguous cases
  - Long tail distributions
:::

::: {.column width="40%"}
```
Example softmax distributions:

High Confidence:
[0.92, 0.05, 0.02, 0.01]
- Strong peak on one class
- Very low probabilities for others
- Often seen with clear examples

Medium Confidence:
[0.65, 0.25, 0.07, 0.03]
- Dominant class but viable alternatives
- Second highest probability non-negligible
- Common in partially ambiguous inputs

Low Confidence/Uncertain:
[0.35, 0.30, 0.20, 0.15]
- Flatter distribution
- Multiple plausible classes
- Indicates model uncertainty
- May suggest out-of-distribution input
```
:::

::::

## Calibration of Softmax Probabilities

:::: {.columns}

::: {.column width="60%"}
- **Calibration challenges**:
  - Raw softmax outputs often miscalibrated
  - Overconfidence in neural networks
  - Distribution shifts from training data
  - Task complexity effects
  
- **Evaluation methods**:
  - Reliability diagrams
  - Expected calibration error (ECE)
  - Brier score
  - Proper scoring rules
  
- **Calibration techniques**:
  - Temperature scaling
  - Platt scaling
  - Isotonic regression
  - Ensemble methods
  - Bayesian approaches
:::

::: {.column width="40%"}
**Temperature Scaling (in words):**

Temperature scaling is a simple way to calibrate softmax probabilities:
- Divide each logit by a temperature $T$ before applying softmax.
- Higher $T$ (e.g., $T=2$ or $T=3$) makes the probability distribution softer (less peaked).
- Lower $T$ (closer to 1) makes the distribution more confident (more peaked).

**Example:**
- Original logits: [8.0, 2.0, 0.0, -2.0]
- Standard softmax (T=1): Most probability on the first class.
- With $T=2$, probabilities are more spread out among classes.
- With $T=3$, the distribution is even softer, further reducing overconfidence.
:::

::::

## From Softmax to Real-World Probabilities

:::: {.columns}

::: {.column width="60%"}
- **Bridging the gap**:
  - Softmax outputs ≠ true probabilities
  - Need for proper calibration
  - Domain adaptation requirements
  - Real-world frequency alignment
  
- **Translation approaches**:
  - Empirical validation with historical data
  - Bayesian updates with domain knowledge
  - Adjustment for class imbalance
  - Alignment with observed frequencies
  
- **Domain-specific considerations**:
  - Base rates in target population
  - Cost matrix for different error types
  - Temporal consistency requirements
  - Regulatory compliance factors
:::

::: {.column width="40%"}
```
Credit Default Example:

LLM predicts probability of default:
- Raw softmax: [0.15, 0.85] for [Default, Non-default]
- Historical default rate: 3%

Calibration process:
1. Calculate calibration factor from historical data
2. Adjust raw probabilities using Bayes' rule
3. Incorporate risk factors specific to customer
4. Apply business-specific threshold criteria
5. Produce final calibrated default probability: 7%

The 7% represents a real-world probability
that aligns with observed frequencies while
incorporating the signal from the model.
```
:::

::::

## Financial Applications of Softmax Probabilities

:::: {.columns}

::: {.column width="60%"}
- **Credit risk modeling**:
  - Default probability estimation
  - Risk tier classification
  - Early warning systems
  - Collection prioritization
  
- **Market prediction**:
  - Directional movement probabilities
  - Regime change detection
  - Volatility state prediction
  - Trading signal generation
  
- **Investment management**:
  - Asset allocation probabilities
  - Sector rotation signals
  - Style factor exposure predictions
  - Manager selection assessments
:::

::: {.column width="40%"}
- **Insurance applications**:
  - Claim frequency prediction
  - Fraud detection probabilities
  - Risk classification
  - Premium optimization
  
- **Banking operations**:
  - Customer churn prediction
  - Cross-sell opportunity scoring
  - Transaction fraud probability
  - Branch traffic forecasting
  
- **Corporate finance**:
  - M&A success probability
  - Financing round completion likelihood
  - Covenant breach prediction
  - Bankruptcy risk assessment
:::

::::

## Case Study: Credit Default Prediction

:::: {.columns}

::: {.column width="60%"}
- **Model architecture**:
  - BERT-based text encoder
  - Financial features integration
  - Classification head with softmax
  - Calibration layer
  
- **Input data**:
  - Loan application text
  - Credit bureau features
  - Transaction history
  - Employment information
  
- **Interpretation framework**:
  - Raw softmax output analysis
  - Probability calibration process
  - Feature attribution methods
  - Confidence interval estimation
  
- **Performance metrics**:
  - AUC-ROC: 0.89
  - Precision/Recall balance
  - Expected calibration error: 0.02
  - Profit per loan increase: 15%
:::

::: {.column width="40%"}
```
Example credit application analysis:

Input: "Looking to consolidate $12k in credit 
card debt. Recently changed jobs from retail 
to tech support with 20% salary increase. 
Some late payments last year during unemployment
but current on all obligations for past 6 months."

Model process:
1. Text encoding with BERT → semantic representation
2. Combine with credit score (680) and DTI (35%)
3. Classification head produces logits: [1.2, 2.8]
4. Raw softmax: [0.18, 0.82] for [Default, Non-default]
5. Calibration adjustment based on loan type
6. Final default probability: 22%
7. Decision: Approve with risk-based pricing
```
:::

::::

## Implementation Challenges and Best Practices

:::: {.columns}

::: {.column width="60%"}
- **Numerical stability**:
  - Logit scaling for precision
  - Log-sum-exp trick implementation
  - Handling extreme values
  - Precision in probability calculations
  
- **Calibration workflow**:
  - Regular recalibration schedule
  - Population drift monitoring
  - Segmented calibration approach
  - Validation with recent data
  
- **Interpretability techniques**:
  - Probability explanation methods
  - Confidence interval communication
  - Risk factor attribution
  - Scenario-based explanation
:::

::: {.column width="40%"}
- **Integration considerations**:
  - Decision threshold optimization
  - Business rule alignment
  - Human oversight mechanisms
  - Feedback loops for improvement
  
- **Regulatory requirements**:
  - Model documentation standards
  - Fairness and bias assessment
  - Adverse action compliance
  - Probability disclosure policies
  
- **Evaluation framework**:
  - Calibration metrics tracking
  - Segment-specific performance
  - Temporal stability measurement
  - Business impact assessment
:::

::::

## Beyond Classification: Advanced Probability Modeling

:::: {.columns}

::: {.column width="60%"}
- **Hierarchical softmax**:
  - Tree-structured probability distribution
  - Efficient for large output spaces
  - Natural for taxonomic classifications
  - Computational advantages
  
- **Mixture density networks**:
  - Predicting distribution parameters
  - Handling multimodal outcomes
  - Uncertainty quantification
  - Full probability distributions
  
- **Energy-based models**:
  - Alternative to softmax formulation
  - Flexible dependency modeling
  - Joint probability distributions
  - Complex constraint satisfaction
:::

::: {.column width="40%"}
**Mixture Density Network Example (in words):**

A mixture density network can predict, for example, that a stock return is a mixture of three possible outcomes:
- One centered at -2% (with 25% weight),
- One at +1% (with 50% weight),
- One at +3% (with 25% weight).

Each outcome has its own mean and standard deviation. The overall probability of a specific return is the weighted sum of the probabilities from each component.

- The model can estimate the probability of a return above 2% (e.g., 32%),
- The most likely return (the mode),
- And a confidence interval (e.g., 90% of returns between -3.5% and +4.5%).
:::

::::

## Future Directions

- **Uncertainty-aware softmax**: Better handling of epistemic uncertainty
- **Adaptive calibration**: Context-dependent probability adjustments
- **Explainable probabilities**: Transparent softmax interpretation
- **Multi-resolution probabilities**: Different granularity for different needs
- **Domain-specific calibration**: Financial risk-aligned probability modeling
- **Causal probability models**: Intervention-aware probability estimates
- **Temporal probability modeling**: Time-varying confidence estimation

## Softmax in Credit Risk: Why Probabilities Matter

:::: {.columns}

::: {.column width="60%"}
- **Credit rating assignments using softmax**:
  - Input: Company financial data + text analysis
  - Output: Probability distribution over rating classes [AAA, AA, A, BBB, BB, B, CCC]
  - Example: [0.05, 0.15, 0.35, 0.30, 0.10, 0.04, 0.01]
  - Most likely rating: A (35% probability)
  - But significant uncertainty exists (BBB at 30%)
  
- **Why this beats traditional approaches**:
  - Traditional: Hard classification "This is an A-rated company"
  - LLM softmax: "35% chance of A, 30% chance of BBB, with reasoning"
  - Enables better risk management and portfolio construction
  - Quantifies uncertainty explicitly
:::

::: {.column width="40%"}
- **Practical credit risk applications**:
  
  **Default probability estimation**:
  - Softmax over [No Default, Default] classes
  - Temperature adjustment for time horizon
  - Calibration against historical default rates
  
  **Loan approval decisions**:
  - Softmax over [Approve, Reject, Review] classes
  - Incorporates uncertainty into decision making
  - Enables risk-adjusted pricing
  
  **Portfolio risk assessment**:
  - Aggregate softmax distributions across loans
  - Monte Carlo simulation using probabilities
  - Value-at-Risk calculations with uncertainty
:::

::::

## Temperature Scaling for Credit Models

:::: {.columns}

::: {.column width="60%"}
- **Why temperature matters in credit risk**:
  - **T < 1 (overconfident)**: "99% sure this company won't default"
  - **T = 1 (standard)**: "85% sure this company won't default"  
  - **T > 1 (conservative)**: "70% sure this company won't default"
  
- **Finding optimal temperature**:
  - Calibrate against historical default rates
  - Conservative approach often preferred in credit
  - Regulatory requirements may favor underconfidence
  - Better to be cautious than overconfident with credit risk
:::

::: {.column width="40%"}
- **Example: Corporate default prediction**:
  
  **Before temperature scaling (T=1)**:
  - Model outputs 2% default probability
  - Historical data shows 8% actual defaults for similar companies
  - Model is severely overconfident
  
  **After temperature scaling (T=2.5)**:
  - Same inputs now output 7% default probability
  - Much better aligned with historical experience
  - More appropriate for risk management decisions
  
  **Impact on business**:
  - Better loan pricing reflects true risk
  - Appropriate reserves and capital allocation
  - Reduced unexpected losses
:::

::::
