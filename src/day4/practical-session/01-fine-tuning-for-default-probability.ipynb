{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72c446a5",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT for Default Probability Assessment\n",
    "\n",
    "In this notebook, we will fine-tune a BERT model to assess default probabilities. We'll explore how to:\n",
    "- Process and prepare financial data for fine-tuning\n",
    "- Adapt a BERT model for binary classification with calibrated probabilities\n",
    "- Evaluate the model's performance\n",
    "- Interpret the softmax probabilities as real-world default probabilities\n",
    "\n",
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3d7e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets sklearn pandas numpy matplotlib torch evaluate\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc, brier_score_loss\n",
    "from sklearn.calibration import calibration_curve\n",
    "import evaluate\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70fa207",
   "metadata": {},
   "source": [
    "## 2. Load and Explore Dataset\n",
    "\n",
    "We'll use a loan dataset with default information. This dataset contains loan applications with textual descriptions and default outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca44a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset (adjust the path as needed)\n",
    "# For this example, we'll create a simplified synthetic dataset\n",
    "# In practice, you would load your actual loan data\n",
    "\n",
    "def create_synthetic_loan_data(n_samples=1000):\n",
    "    \"\"\"Create a synthetic loan dataset with default information\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Create loan purposes and descriptions\n",
    "    purposes = ['Home improvement', 'Debt consolidation', 'Business', 'Medical expenses', 'Education', 'Vacation', 'Car purchase']\n",
    "    income_levels = ['Low', 'Medium', 'High', 'Very high']\n",
    "    employment_status = ['Employed', 'Self-employed', 'Unemployed', 'Retired', 'Student']\n",
    "    credit_history = ['Excellent', 'Good', 'Fair', 'Poor']\n",
    "    \n",
    "    data = []\n",
    "    for _ in range(n_samples):\n",
    "        purpose = np.random.choice(purposes)\n",
    "        income = np.random.choice(income_levels)\n",
    "        employment = np.random.choice(employment_status)\n",
    "        credit = np.random.choice(credit_history)\n",
    "        loan_amount = np.random.randint(1000, 50000)\n",
    "        debt_to_income = np.random.uniform(0.1, 0.8)\n",
    "        \n",
    "        # Generate description with variations\n",
    "        description = f\"I am {employment.lower()} with a {income.lower()} income of {np.random.randint(20000, 150000)} per year. \"\n",
    "        description += f\"I am applying for a {purpose.lower()} loan of ${loan_amount}. \"\n",
    "        description += f\"My credit history is {credit.lower()} and my debt-to-income ratio is {debt_to_income:.2f}. \"\n",
    "        \n",
    "        # Add some random details\n",
    "        if purpose == 'Home improvement':\n",
    "            description += f\"I plan to renovate my {np.random.choice(['kitchen', 'bathroom', 'basement', 'roof', 'entire house'])}.\" \n",
    "        elif purpose == 'Debt consolidation':\n",
    "            description += f\"I have {np.random.randint(2, 8)} credit cards with high interest rates that I want to consolidate.\"\n",
    "        elif purpose == 'Business':\n",
    "            description += f\"I run a small {np.random.choice(['retail', 'online', 'service', 'consulting', 'food'])} business.\"\n",
    "        \n",
    "        # Determine default probability based on features\n",
    "        default_prob = 0.05  # Base rate\n",
    "        \n",
    "        # Adjust based on income\n",
    "        if income == 'Low':\n",
    "            default_prob += 0.15\n",
    "        elif income == 'Medium':\n",
    "            default_prob += 0.05\n",
    "        elif income == 'High':\n",
    "            default_prob -= 0.03\n",
    "        elif income == 'Very high':\n",
    "            default_prob -= 0.04\n",
    "        \n",
    "        # Adjust based on employment\n",
    "        if employment == 'Unemployed':\n",
    "            default_prob += 0.2\n",
    "        elif employment == 'Self-employed':\n",
    "            default_prob += 0.05\n",
    "        elif employment == 'Student':\n",
    "            default_prob += 0.1\n",
    "        \n",
    "        # Adjust based on credit history\n",
    "        if credit == 'Poor':\n",
    "            default_prob += 0.25\n",
    "        elif credit == 'Fair':\n",
    "            default_prob += 0.1\n",
    "        elif credit == 'Excellent':\n",
    "            default_prob -= 0.04\n",
    "        \n",
    "        # Adjust based on debt-to-income\n",
    "        default_prob += debt_to_income * 0.2\n",
    "        \n",
    "        # Cap probability between 0.01 and 0.9\n",
    "        default_prob = max(min(default_prob, 0.9), 0.01)\n",
    "        \n",
    "        # Generate default status based on probability\n",
    "        default = 1 if np.random.random() < default_prob else 0\n",
    "        \n",
    "        data.append({\n",
    "            'description': description,\n",
    "            'purpose': purpose,\n",
    "            'income_level': income,\n",
    "            'employment_status': employment,\n",
    "            'credit_history': credit,\n",
    "            'loan_amount': loan_amount,\n",
    "            'debt_to_income': debt_to_income,\n",
    "            'default_prob': default_prob,\n",
    "            'default': default\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Create synthetic dataset\n",
    "loan_data = create_synthetic_loan_data(2000)\n",
    "\n",
    "# Display sample data\n",
    "loan_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d5811e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore the dataset\n",
    "print(f\"Dataset shape: {loan_data.shape}\")\n",
    "print(f\"Default rate: {loan_data['default'].mean():.2f}\")\n",
    "\n",
    "# Explore default rates by category\n",
    "print(\"\\nDefault rate by purpose:\")\n",
    "print(loan_data.groupby('purpose')['default'].mean().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nDefault rate by income level:\")\n",
    "print(loan_data.groupby('income_level')['default'].mean().sort_values(ascending=False))\n",
    "\n",
    "print(\"\\nDefault rate by credit history:\")\n",
    "print(loan_data.groupby('credit_history')['default'].mean().sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d72cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of default probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(data=loan_data, x='default_prob', hue='default', bins=20, kde=True)\n",
    "plt.title('Distribution of Default Probabilities')\n",
    "plt.xlabel('Default Probability')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee0b0c5",
   "metadata": {},
   "source": [
    "## 3. Prepare Data for BERT\n",
    "\n",
    "We'll now prepare our data for fine-tuning a BERT model. We'll focus on using the textual description as the primary input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48dbcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "train_data, test_data = train_test_split(loan_data, test_size=0.2, random_state=seed, stratify=loan_data['default'])\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=seed, stratify=train_data['default'])\n",
    "\n",
    "print(f\"Train data size: {len(train_data)}\")\n",
    "print(f\"Validation data size: {len(val_data)}\")\n",
    "print(f\"Test data size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e8662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer\n",
    "model_name = \"distilbert-base-uncased\"  # We'll use DistilBERT for efficiency\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create a PyTorch dataset\n",
    "class LoanDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.descriptions = dataframe['description'].tolist()\n",
    "        self.targets = dataframe['default'].tolist()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.descriptions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        description = self.descriptions[idx]\n",
    "        target = self.targets[idx]\n",
    "        \n",
    "        encoding = self.tokenizer(\n",
    "            description,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Remove batch dimension\n",
    "        encoding = {k: v.squeeze() for k, v in encoding.items()}\n",
    "        encoding['labels'] = torch.tensor(target, dtype=torch.long)\n",
    "        \n",
    "        return encoding\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = LoanDataset(train_data, tokenizer)\n",
    "val_dataset = LoanDataset(val_data, tokenizer)\n",
    "test_dataset = LoanDataset(test_data, tokenizer)\n",
    "\n",
    "# Check a sample\n",
    "sample = train_dataset[0]\n",
    "print(\"Sample input keys:\", sample.keys())\n",
    "print(\"Input IDs shape:\", sample['input_ids'].shape)\n",
    "print(\"Attention mask shape:\", sample['attention_mask'].shape)\n",
    "print(\"Label:\", sample['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6b63b6",
   "metadata": {},
   "source": [
    "## 4. Define Evaluation Metrics\n",
    "\n",
    "We'll define functions to compute evaluation metrics specific to our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3a984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute metrics for evaluation\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    # Get probabilities with softmax\n",
    "    probs = torch.nn.functional.softmax(torch.tensor(predictions), dim=1)\n",
    "    preds = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    # Get default probability (probability of class 1)\n",
    "    default_probs = probs[:, 1].numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = (preds == labels).mean()\n",
    "    \n",
    "    # ROC AUC score\n",
    "    roc_auc = roc_auc_score(labels, default_probs)\n",
    "    \n",
    "    # Precision-Recall AUC\n",
    "    precision, recall, _ = precision_recall_curve(labels, default_probs)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    \n",
    "    # Brier score (for probability calibration)\n",
    "    brier = brier_score_loss(labels, default_probs)\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'brier_score': brier\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0c3a6e",
   "metadata": {},
   "source": [
    "## 5. Initialize and Fine-Tune BERT Model\n",
    "\n",
    "Now we'll fine-tune a pre-trained BERT model for our default prediction task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff045d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name, \n",
    "    num_labels=2,  # Binary classification: default or not\n",
    "    problem_type=\"single_label_classification\"\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "batch_size = 16\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"roc_auc\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"none\",  # Disable W&B and TensorBoard reporting\n",
    "    save_total_limit=2,  # Only keep the 2 best checkpoints\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97c7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ff8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on validation set\n",
    "val_results = trainer.evaluate()\n",
    "print(\"Validation Results:\", val_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3a3875",
   "metadata": {},
   "source": [
    "## 6. Evaluate Model Performance\n",
    "\n",
    "Now let's evaluate our model on the test set and analyze its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576b7368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "test_results = trainer.evaluate(test_dataset)\n",
    "print(\"Test Results:\", test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fafef1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions on test set\n",
    "test_predictions = trainer.predict(test_dataset)\n",
    "logits = test_predictions.predictions\n",
    "probabilities = torch.nn.functional.softmax(torch.tensor(logits), dim=1).numpy()\n",
    "default_probabilities = probabilities[:, 1]  # Probability of class 1 (default)\n",
    "predicted_labels = np.argmax(logits, axis=1)\n",
    "true_labels = test_data['default'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea5462c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, _ = roc_curve(true_labels, default_probabilities)\n",
    "roc_auc = roc_auc_score(true_labels, default_probabilities)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c959ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(true_labels, default_probabilities)\n",
    "pr_auc = auc(recall, precision)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, color='darkorange', lw=2, label=f'PR curve (area = {pr_auc:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8753de58",
   "metadata": {},
   "source": [
    "## 7. Calibration of Softmax Probabilities\n",
    "\n",
    "Let's evaluate how well calibrated our softmax probabilities are and apply calibration if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cc3567",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check calibration\n",
    "prob_true, prob_pred = calibration_curve(true_labels, default_probabilities, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')\n",
    "plt.plot(prob_pred, prob_true, marker='o', label='Model')\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('True probability (fraction of positives)')\n",
    "plt.title('Calibration Curve (Reliability Diagram)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e233eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply temperature scaling for calibration\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "# We'll try two calibration methods: Isotonic Regression and Platt Scaling\n",
    "ir = IsotonicRegression(out_of_bounds='clip')\n",
    "ir.fit(default_probabilities, true_labels)\n",
    "\n",
    "# Calibrate probabilities\n",
    "calibrated_probs = ir.predict(default_probabilities)\n",
    "\n",
    "# Check calibration after correction\n",
    "prob_true_cal, prob_pred_cal = calibration_curve(true_labels, calibrated_probs, n_bins=10)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')\n",
    "plt.plot(prob_pred, prob_true, marker='o', label='Uncalibrated')\n",
    "plt.plot(prob_pred_cal, prob_true_cal, marker='s', label='Calibrated (Isotonic)')\n",
    "plt.xlabel('Predicted probability')\n",
    "plt.ylabel('True probability (fraction of positives)')\n",
    "plt.title('Calibration Curve Before and After Calibration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563a6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Brier scores before and after calibration\n",
    "brier_before = brier_score_loss(true_labels, default_probabilities)\n",
    "brier_after = brier_score_loss(true_labels, calibrated_probs)\n",
    "\n",
    "print(f\"Brier score before calibration: {brier_before:.4f}\")\n",
    "print(f\"Brier score after calibration: {brier_after:.4f}\")\n",
    "print(f\"Improvement: {(brier_before - brier_after) / brier_before * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce928a4",
   "metadata": {},
   "source": [
    "## 8. Compare Predicted Probabilities to True Default Rates\n",
    "\n",
    "Let's compare our model's predicted probabilities to the true default probabilities in our synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912ea2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to test data\n",
    "test_results_df = test_data.copy()\n",
    "test_results_df['predicted_prob'] = default_probabilities\n",
    "test_results_df['calibrated_prob'] = calibrated_probs\n",
    "test_results_df['predicted_default'] = predicted_labels\n",
    "\n",
    "# Compare predicted vs true default probabilities\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(test_results_df['default_prob'], test_results_df['predicted_prob'], alpha=0.5, label='Uncalibrated')\n",
    "plt.scatter(test_results_df['default_prob'], test_results_df['calibrated_prob'], alpha=0.5, label='Calibrated')\n",
    "plt.plot([0, 1], [0, 1], 'r--', label='Perfect prediction')\n",
    "plt.xlabel('True Default Probability')\n",
    "plt.ylabel('Predicted Default Probability')\n",
    "plt.title('Comparison of True vs. Predicted Default Probabilities')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a102ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate error metrics\n",
    "test_results_df['uncal_error'] = np.abs(test_results_df['default_prob'] - test_results_df['predicted_prob'])\n",
    "test_results_df['cal_error'] = np.abs(test_results_df['default_prob'] - test_results_df['calibrated_prob'])\n",
    "\n",
    "print(f\"Mean absolute error (Uncalibrated): {test_results_df['uncal_error'].mean():.4f}\")\n",
    "print(f\"Mean absolute error (Calibrated): {test_results_df['cal_error'].mean():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac5eb62",
   "metadata": {},
   "source": [
    "## 9. Error Analysis\n",
    "\n",
    "Let's examine cases where our model performed poorly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b40b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find examples with largest errors\n",
    "worst_predictions = test_results_df.sort_values('cal_error', ascending=False).head(5)\n",
    "\n",
    "print(\"Examples with largest errors:\")\n",
    "for i, row in worst_predictions.iterrows():\n",
    "    print(f\"\\nExample {i}:\")\n",
    "    print(f\"Description: {row['description']}\")\n",
    "    print(f\"True default prob: {row['default_prob']:.4f}\")\n",
    "    print(f\"Predicted prob (uncalibrated): {row['predicted_prob']:.4f}\")\n",
    "    print(f\"Predicted prob (calibrated): {row['calibrated_prob']:.4f}\")\n",
    "    print(f\"Actual default: {row['default']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f42b402",
   "metadata": {},
   "source": [
    "## 10. Build a Default Probability Prediction Function\n",
    "\n",
    "Let's create a function that takes a loan description and returns a calibrated default probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4c83e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_default_probability(description, model=model, tokenizer=tokenizer, calibrator=ir):\n",
    "    \"\"\"Predict default probability for a loan description\"\"\"\n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(\n",
    "        description,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "    \n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()\n",
    "    default_prob = probabilities[0, 1]  # Probability of class 1 (default)\n",
    "    \n",
    "    # Apply calibration\n",
    "    calibrated_prob = calibrator.predict([default_prob])[0]\n",
    "    \n",
    "    return {\n",
    "        'raw_default_probability': float(default_prob),\n",
    "        'calibrated_default_probability': float(calibrated_prob),\n",
    "        'predicted_class': 'Default' if calibrated_prob > 0.5 else 'Non-Default'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14151fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the function with some examples\n",
    "test_descriptions = [\n",
    "    \"I am employed with a high income of 120000 per year. I am applying for a home improvement loan of $25000. My credit history is excellent and my debt-to-income ratio is 0.25. I plan to renovate my kitchen.\",\n",
    "    \"I am unemployed with a low income of 25000 per year. I am applying for a debt consolidation loan of $15000. My credit history is poor and my debt-to-income ratio is 0.65. I have 6 credit cards with high interest rates that I want to consolidate.\",\n",
    "    \"I am self-employed with a medium income of 75000 per year. I am applying for a business loan of $35000. My credit history is good and my debt-to-income ratio is 0.40. I run a small retail business.\"\n",
    "]\n",
    "\n",
    "for i, description in enumerate(test_descriptions):\n",
    "    result = predict_default_probability(description)\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Description: {description}\")\n",
    "    print(f\"Raw Default Probability: {result['raw_default_probability']:.4f}\")\n",
    "    print(f\"Calibrated Default Probability: {result['calibrated_default_probability']:.4f}\")\n",
    "    print(f\"Predicted Class: {result['predicted_class']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb2f2a6",
   "metadata": {},
   "source": [
    "## 11. Save the Model and Calibrator\n",
    "\n",
    "Let's save our model and calibrator for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_save_path = \"./default_prediction_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "# Save the calibrator\n",
    "import pickle\n",
    "with open(f\"{model_save_path}/calibrator.pkl\", 'wb') as f:\n",
    "    pickle.dump(ir, f)\n",
    "\n",
    "print(f\"Model and calibrator saved to {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b844d7a",
   "metadata": {},
   "source": [
    "## 12. Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to fine-tune a BERT model to predict loan default probabilities. We've covered:\n",
    "\n",
    "1. Data preparation and exploration\n",
    "2. Fine-tuning a BERT model for default prediction\n",
    "3. Evaluating model performance with appropriate metrics\n",
    "4. Calibrating softmax probabilities to align with real-world default rates\n",
    "5. Building a prediction function for new loan descriptions\n",
    "\n",
    "This approach can be extended to real-world credit risk assessment by:\n",
    "- Using actual loan data with historical default information\n",
    "- Incorporating additional structured data features\n",
    "- Implementing more sophisticated calibration techniques\n",
    "- Conducting fairness audits to ensure unbiased predictions\n",
    "- Regularly retraining the model with new data\n",
    "\n",
    "The model we've built can serve as a starting point for more advanced credit risk assessment systems."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
