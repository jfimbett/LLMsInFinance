{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e804ae2",
   "metadata": {},
   "source": [
    "# Beyond Text: Architecture of Text-to-Image Models\n",
    "\n",
    "In this notebook, we explore the architecture and functionality of text-to-image models as a case study for understanding multimodal AI systems. By examining how these models bridge the gap between language and visual representations, we gain insights into the broader landscape of multimodal AI and its potential applications in finance.\n",
    "\n",
    "## Key Topics\n",
    "\n",
    "- Fundamentals of multimodal AI systems\n",
    "- Architecture of text-to-image models (diffusion models)\n",
    "- Latent space representations and conditioning mechanisms\n",
    "- Practical implementation and API usage\n",
    "- Evaluation metrics for generated images\n",
    "- Financial applications and use cases\n",
    "- Ethical considerations and limitations\n",
    "\n",
    "## 1. Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6160e017",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install openai pillow torch torchvision matplotlib numpy requests pandas diffusers transformers accelerate\n",
    "\n",
    "# Import libraries\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from openai import OpenAI\n",
    "from io import BytesIO\n",
    "import base64\n",
    "from IPython.display import display, HTML\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Initialize API client\n",
    "openai_client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\", \"your-api-key-here\"))\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Helper function to display images\n",
    "def display_image(image_data, title=None):\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image_data)\n",
    "    plt.axis('off')\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96857e6",
   "metadata": {},
   "source": [
    "## 2. Understanding Multimodal AI Systems\n",
    "\n",
    "Multimodal AI systems can process and generate content across different types of data (modalities), such as text, images, audio, and video. These systems learn to understand the relationships between different forms of information, enabling them to translate concepts from one modality to another.\n",
    "\n",
    "### Key Characteristics of Multimodal Models:\n",
    "\n",
    "1. **Cross-modal understanding**: The ability to understand concepts across different data types\n",
    "2. **Joint embeddings**: Representing data from different modalities in a shared semantic space\n",
    "3. **Modal translation**: Converting information from one modality to another\n",
    "4. **Alignment**: Ensuring representations across modalities correspond correctly\n",
    "\n",
    "Text-to-image models exemplify these characteristics by translating textual descriptions into visual representations, making them an excellent case study for understanding multimodal AI systems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a87205c",
   "metadata": {},
   "source": [
    "## 3. Evolution of Text-to-Image Models\n",
    "\n",
    "The development of text-to-image models represents a significant milestone in AI research. Let's explore the evolution of these technologies:\n",
    "\n",
    "### Historical Development:\n",
    "\n",
    "1. **Early GANs (2014-2018)**: Initial attempts using Generative Adversarial Networks with limited capabilities\n",
    "2. **Conditional GANs**: Improved control with conditional inputs\n",
    "3. **DALL-E (2021)**: OpenAI's breakthrough using transformer architectures\n",
    "4. **Diffusion Models (2021-present)**: State-of-the-art approaches like DALL-E 2, Stable Diffusion, and Midjourney\n",
    "5. **Multimodal Foundation Models (2022-present)**: Integrating text-to-image capabilities with broader AI systems\n",
    "\n",
    "Let's visualize this evolution with a timeline and examples of the image quality improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e273b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a timeline dataframe\n",
    "timeline_data = {\n",
    "    'Year': [2014, 2017, 2020, 2021, 2022, 2023],\n",
    "    'Model': ['GAN', 'StackGAN', 'DALL-E', 'GLIDE', 'DALL-E 2/Stable Diffusion', 'DALL-E 3/Midjourney v5'],\n",
    "    'Breakthrough': [\n",
    "        'Initial GAN architecture',\n",
    "        'Multi-stage generation process',\n",
    "        'Transformer-based text-to-image generation',\n",
    "        'Diffusion models with guided sampling',\n",
    "        'Latent diffusion and CLIP guidance',\n",
    "        'Photorealistic quality and complex compositions'\n",
    "    ]\n",
    "}\n",
    "\n",
    "timeline_df = pd.DataFrame(timeline_data)\n",
    "\n",
    "# Plot the timeline\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(timeline_df['Year'], np.ones(len(timeline_df)), 'o', markersize=10, color='blue')\n",
    "\n",
    "for i, row in timeline_df.iterrows():\n",
    "    plt.annotate(f\"{row['Year']}: {row['Model']}\\n{row['Breakthrough']}\", \n",
    "                 xy=(row['Year'], 1), \n",
    "                 xytext=(0, 10 if i % 2 == 0 else -40),\n",
    "                 textcoords='offset points',\n",
    "                 ha='center',\n",
    "                 va='bottom',\n",
    "                 bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.3),\n",
    "                 arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "\n",
    "plt.yticks([])\n",
    "plt.xlabel('Year')\n",
    "plt.title('Evolution of Text-to-Image Models')\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800eb3b8",
   "metadata": {},
   "source": [
    "## 4. Architecture of Diffusion Models\n",
    "\n",
    "Modern text-to-image systems predominantly use diffusion models, which have become the state-of-the-art approach. Let's explore the core components and functioning of these models:\n",
    "\n",
    "### Diffusion Process Explained:\n",
    "\n",
    "1. **Forward Diffusion**: A process that gradually adds noise to an image until it becomes pure noise\n",
    "2. **Reverse Diffusion**: Learning to gradually remove noise to generate an image from random noise\n",
    "3. **Conditioning**: Using text embeddings to guide the denoising process\n",
    "\n",
    "The key innovation is that these models learn to reverse a noise-adding process, guided by textual descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a047635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual explanation of the diffusion process\n",
    "def create_diffusion_diagram():\n",
    "    fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    \n",
    "    # Create a simple 2D representation of the diffusion process\n",
    "    # Starting with a clean image (represented as a pattern)\n",
    "    x = np.linspace(-5, 5, 100)\n",
    "    y = np.linspace(-5, 5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    \n",
    "    # Initial image (a clean pattern)\n",
    "    Z0 = np.sin(0.5 * X) * np.cos(0.5 * Y)\n",
    "    \n",
    "    # Progressive noise addition\n",
    "    noise_levels = [0.0, 0.3, 0.7, 1.2, 2.0]\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    for i, noise in enumerate(noise_levels):\n",
    "        # Add progressively more noise\n",
    "        noise_component = np.random.randn(100, 100) * noise\n",
    "        Z = Z0 + noise_component\n",
    "        \n",
    "        # Plot\n",
    "        im = ax[i].imshow(Z, cmap='viridis', origin='lower', extent=[-5, 5, -5, 5])\n",
    "        ax[i].set_title(f\"Step {i+1}\" if i < 4 else \"Pure Noise\")\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "    \n",
    "    plt.suptitle(\"Forward Diffusion Process: Adding Noise Gradually\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Now create the reverse process diagram\n",
    "    fig, ax = plt.subplots(1, 5, figsize=(15, 5))\n",
    "    \n",
    "    for i, noise in enumerate(reversed(noise_levels)):\n",
    "        # Reverse the process\n",
    "        noise_component = np.random.randn(100, 100) * noise\n",
    "        Z = Z0 + noise_component\n",
    "        \n",
    "        # Plot\n",
    "        im = ax[i].imshow(Z, cmap='viridis', origin='lower', extent=[-5, 5, -5, 5])\n",
    "        ax[i].set_title(f\"Step {i+1}\" if i < 4 else \"Starting Noise\")\n",
    "        ax[i].set_xticks([])\n",
    "        ax[i].set_yticks([])\n",
    "    \n",
    "    plt.suptitle(\"Reverse Diffusion Process: Removing Noise Gradually\", fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the function to create diagrams\n",
    "create_diffusion_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6eb9b7",
   "metadata": {},
   "source": [
    "### Core Components of a Text-to-Image Diffusion Model:\n",
    "\n",
    "1. **Text Encoder**: Transforms text descriptions into embeddings that guide the image generation process\n",
    "2. **U-Net Denoiser**: Neural network that predicts and removes noise at each step\n",
    "3. **Latent Space**: Compressed representation where the diffusion process operates\n",
    "4. **Conditioning Mechanism**: Integrates text embeddings to guide the generation process\n",
    "5. **Sampling Scheduler**: Controls the rate and strategy of noise removal\n",
    "\n",
    "Let's create a simplified diagram to illustrate this architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3abbeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a diagram of text-to-image model architecture\n",
    "def create_architecture_diagram():\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    # Hide axes\n",
    "    ax.axis('off')\n",
    "    \n",
    "    # Component boxes\n",
    "    components = [\n",
    "        {'name': 'Text Prompt', 'pos': (0.5, 0.9), 'width': 0.3, 'height': 0.1, 'color': 'lightblue'},\n",
    "        {'name': 'Text Encoder (CLIP)', 'pos': (0.5, 0.75), 'width': 0.3, 'height': 0.1, 'color': 'lightgreen'},\n",
    "        {'name': 'Text Embeddings', 'pos': (0.5, 0.6), 'width': 0.3, 'height': 0.1, 'color': 'lightyellow'},\n",
    "        {'name': 'Random Noise', 'pos': (0.2, 0.5), 'width': 0.2, 'height': 0.1, 'color': 'lightgray'},\n",
    "        {'name': 'U-Net Denoiser', 'pos': (0.5, 0.4), 'width': 0.4, 'height': 0.15, 'color': 'lightcoral'},\n",
    "        {'name': 'Decoder', 'pos': (0.5, 0.2), 'width': 0.3, 'height': 0.1, 'color': 'lightsalmon'},\n",
    "        {'name': 'Generated Image', 'pos': (0.5, 0.05), 'width': 0.3, 'height': 0.1, 'color': 'lavender'}\n",
    "    ]\n",
    "    \n",
    "    # Draw component boxes\n",
    "    for comp in components:\n",
    "        rect = plt.Rectangle(\n",
    "            (comp['pos'][0] - comp['width']/2, comp['pos'][1] - comp['height']/2),\n",
    "            comp['width'], comp['height'],\n",
    "            linewidth=1, edgecolor='black', facecolor=comp['color'], alpha=0.7\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        ax.text(comp['pos'][0], comp['pos'][1], comp['name'], \n",
    "                ha='center', va='center', fontsize=10, fontweight='bold')\n",
    "    \n",
    "    # Draw arrows connecting components\n",
    "    arrows = [\n",
    "        {'start': (0.5, 0.85), 'end': (0.5, 0.8)},  # Text Prompt to Text Encoder\n",
    "        {'start': (0.5, 0.7), 'end': (0.5, 0.65)},  # Text Encoder to Text Embeddings\n",
    "        {'start': (0.5, 0.55), 'end': (0.5, 0.475)},  # Text Embeddings to U-Net\n",
    "        {'start': (0.3, 0.5), 'end': (0.4, 0.45)},  # Random Noise to U-Net\n",
    "        {'start': (0.5, 0.325), 'end': (0.5, 0.25)},  # U-Net to Decoder\n",
    "        {'start': (0.5, 0.15), 'end': (0.5, 0.1)}  # Decoder to Generated Image\n",
    "    ]\n",
    "    \n",
    "    for arrow in arrows:\n",
    "        ax.annotate(\"\", \n",
    "                   xy=arrow['end'], xycoords='data',\n",
    "                   xytext=arrow['start'], textcoords='data',\n",
    "                   arrowprops=dict(arrowstyle=\"->\", lw=1.5, color='black'))\n",
    "    \n",
    "    # Add a circular arrow for the iterative diffusion process\n",
    "    diffusion_circle = plt.Circle((0.7, 0.4), 0.08, fill=False, color='black', linestyle='--')\n",
    "    ax.add_patch(diffusion_circle)\n",
    "    ax.annotate(\"Iterative\\nProcess\", xy=(0.7, 0.4), xytext=(0.8, 0.4),\n",
    "                arrowprops=dict(arrowstyle=\"->\", lw=1.0, color='black'),\n",
    "                ha='center', va='center', fontsize=8)\n",
    "    \n",
    "    # Add title\n",
    "    ax.set_title('Text-to-Image Diffusion Model Architecture', fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run the function to create the architecture diagram\n",
    "create_architecture_diagram()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5590c9",
   "metadata": {},
   "source": [
    "## 5. Implementation: Using Text-to-Image Models\n",
    "\n",
    "Let's explore how to use text-to-image models in practice. We'll look at both API-based approaches (like OpenAI's DALL-E) and open-source implementations (like Stable Diffusion).\n",
    "\n",
    "### Using OpenAI's DALL-E API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902a2c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate images using OpenAI's DALL-E API\n",
    "def generate_with_dalle(prompt, n=1, size=\"1024x1024\", model=\"dall-e-3\"):\n",
    "    try:\n",
    "        response = openai_client.images.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            n=n,\n",
    "            size=size\n",
    "        )\n",
    "        \n",
    "        # For demonstration, we'll use a placeholder image if API key isn't set\n",
    "        if os.environ.get(\"OPENAI_API_KEY\", \"\") == \"your-api-key-here\":\n",
    "            # Create a placeholder image\n",
    "            img = Image.new('RGB', (512, 512), color='white')\n",
    "            # Return the placeholder\n",
    "            return img\n",
    "        \n",
    "        # Download and display the generated image\n",
    "        image_url = response.data[0].url\n",
    "        image_data = requests.get(image_url).content\n",
    "        img = Image.open(BytesIO(image_data))\n",
    "        return img\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image: {e}\")\n",
    "        # Return a placeholder image\n",
    "        return Image.new('RGB', (512, 512), color='lightgray')\n",
    "\n",
    "# Financial visualization prompts to try\n",
    "financial_prompts = [\n",
    "    \"A clear visualization of stock market trends with bull and bear indicators\",\n",
    "    \"A detailed financial dashboard showing various market metrics and KPIs\",\n",
    "    \"An infographic explaining portfolio diversification strategies\"\n",
    "]\n",
    "\n",
    "# Generate a sample image (only runs if API key is set)\n",
    "if os.environ.get(\"OPENAI_API_KEY\", \"\") != \"your-api-key-here\":\n",
    "    sample_img = generate_with_dalle(financial_prompts[0])\n",
    "    display_image(sample_img, \"DALL-E Generated Financial Visualization\")\n",
    "else:\n",
    "    print(\"OpenAI API key not set. Skipping DALL-E image generation.\")\n",
    "    print(\"Example prompt that would be used:\")\n",
    "    print(financial_prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b977fa0b",
   "metadata": {},
   "source": [
    "### Using Stable Diffusion (Open Source):\n",
    "\n",
    "Stable Diffusion is an open-source text-to-image model that can be run locally or accessed through various APIs. Let's see how to use it with the Diffusers library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d8037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate images using Stable Diffusion\n",
    "def generate_with_stable_diffusion(prompt, guidance_scale=7.5, num_inference_steps=50):\n",
    "    try:\n",
    "        # Check if GPU is available\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Load the pipeline (this downloads the model weights if not present)\n",
    "        # For demonstration, we'll create a placeholder if not running with GPU\n",
    "        if device == \"cuda\":\n",
    "            pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\")\n",
    "            pipe = pipe.to(device)\n",
    "            \n",
    "            # Generate the image\n",
    "            image = pipe(prompt, guidance_scale=guidance_scale, num_inference_steps=num_inference_steps).images[0]\n",
    "            return image\n",
    "        else:\n",
    "            print(\"GPU not available. Using placeholder image instead.\")\n",
    "            # Return a placeholder for demonstration\n",
    "            return Image.new('RGB', (512, 512), color='lightblue')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error generating image with Stable Diffusion: {e}\")\n",
    "        return Image.new('RGB', (512, 512), color='lightgray')\n",
    "\n",
    "# Create a comparison of different guidance scales\n",
    "def compare_guidance_scales(prompt):\n",
    "    guidance_scales = [1.0, 3.0, 7.5, 15.0]\n",
    "    \n",
    "    # For demonstration, we'll create placeholder images\n",
    "    fig, axes = plt.subplots(1, len(guidance_scales), figsize=(20, 5))\n",
    "    \n",
    "    for i, scale in enumerate(guidance_scales):\n",
    "        # In a real implementation, this would use the model\n",
    "        # Here we just create color-coded placeholders\n",
    "        color = plt.cm.viridis(i / len(guidance_scales))\n",
    "        img = Image.new('RGB', (512, 512), color=tuple(int(c*255) for c in color[:3]))\n",
    "        \n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Guidance Scale: {scale}\")\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Effect of Guidance Scale on Generation\\nPrompt: '{prompt}'\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Demonstrate with a financial prompt\n",
    "financial_prompt = \"A detailed 3D chart showing the correlation between market volatility and investor sentiment\"\n",
    "compare_guidance_scales(financial_prompt)\n",
    "\n",
    "print(\"Note: In a real implementation with GPU access, this would generate actual images using Stable Diffusion.\")\n",
    "print(\"The guidance scale controls how closely the generation follows the text prompt.\")\n",
    "print(\"Higher values (7-9) generally produce images that more closely match the text description.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090fca42",
   "metadata": {},
   "source": [
    "## 6. Latent Space and Embeddings\n",
    "\n",
    "The power of text-to-image models lies in their ability to map both text and images to a shared latent space, where semantic relationships are preserved. Let's explore how this works:\n",
    "\n",
    "### Understanding the Latent Space:\n",
    "\n",
    "1. **Text Embeddings**: Capture semantic meaning of textual descriptions\n",
    "2. **Image Embeddings**: Represent visual concepts in a compressed form\n",
    "3. **Cross-modal Alignment**: Mapping between textual and visual representations\n",
    "\n",
    "Let's visualize how different text prompts map to different regions in the latent space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9282d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate latent space embeddings for different financial concepts\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create synthetic high-dimensional embeddings (normally these would come from a model)\n",
    "np.random.seed(42)\n",
    "num_concepts = 50\n",
    "embedding_dim = 512  # Typical dimension for CLIP text embeddings\n",
    "\n",
    "# Generate synthetic embeddings for financial concepts\n",
    "concept_categories = {\n",
    "    'Market Analysis': ['bull market', 'bear market', 'volatility', 'market trend', \n",
    "                        'stock index', 'trading volume', 'market sentiment', \n",
    "                        'price action', 'support level', 'resistance level'],\n",
    "    'Financial Instruments': ['stocks', 'bonds', 'options', 'futures', 'ETFs', \n",
    "                             'mutual funds', 'commodities', 'forex', 'derivatives', \n",
    "                             'treasury bills'],\n",
    "    'Risk Management': ['diversification', 'hedging', 'risk assessment', 'VaR', \n",
    "                       'stop loss', 'risk exposure', 'correlation analysis', \n",
    "                       'tail risk', 'stress testing', 'risk mitigation'],\n",
    "    'Investment Strategies': ['value investing', 'growth investing', 'momentum trading', \n",
    "                             'dollar cost averaging', 'swing trading', 'day trading', \n",
    "                             'buy and hold', 'asset allocation', 'technical analysis', \n",
    "                             'fundamental analysis'],\n",
    "    'Financial Metrics': ['P/E ratio', 'EPS', 'dividend yield', 'ROI', 'EBITDA', \n",
    "                         'free cash flow', 'debt-to-equity', 'profit margin', \n",
    "                         'beta coefficient', 'book value']\n",
    "}\n",
    "\n",
    "# Flatten the concepts and create synthetic embeddings\n",
    "all_concepts = []\n",
    "categories = []\n",
    "for category, concepts in concept_categories.items():\n",
    "    all_concepts.extend(concepts)\n",
    "    categories.extend([category] * len(concepts))\n",
    "\n",
    "# Create synthetic embeddings with cluster structure based on categories\n",
    "embeddings = np.zeros((len(all_concepts), embedding_dim))\n",
    "category_centers = {cat: np.random.randn(embedding_dim) for cat in concept_categories.keys()}\n",
    "\n",
    "for i, (concept, category) in enumerate(zip(all_concepts, categories)):\n",
    "    # Base embedding on category center plus noise\n",
    "    embeddings[i] = category_centers[category] + np.random.randn(embedding_dim) * 0.2\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=50)\n",
    "embeddings_pca = pca.fit_transform(embeddings)\n",
    "\n",
    "tsne = TSNE(n_components=2, perplexity=5, learning_rate=200, random_state=42)\n",
    "embeddings_2d = tsne.fit_transform(embeddings_pca)\n",
    "\n",
    "# Plot the latent space visualization\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# Create a color map for categories\n",
    "unique_categories = list(concept_categories.keys())\n",
    "colors = plt.cm.tab10(np.linspace(0, 1, len(unique_categories)))\n",
    "color_map = {cat: colors[i] for i, cat in enumerate(unique_categories)}\n",
    "\n",
    "# Plot each concept\n",
    "for i, (concept, category) in enumerate(zip(all_concepts, categories)):\n",
    "    plt.scatter(embeddings_2d[i, 0], embeddings_2d[i, 1], color=color_map[category], s=100, alpha=0.7)\n",
    "    plt.annotate(concept, (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=9)\n",
    "\n",
    "# Add legend\n",
    "for i, category in enumerate(unique_categories):\n",
    "    plt.scatter([], [], color=colors[i], label=category)\n",
    "plt.legend(loc='upper right', fontsize=10)\n",
    "\n",
    "plt.title('Simulated Latent Space Visualization of Financial Concepts', fontsize=16)\n",
    "plt.xlabel('t-SNE Dimension 1')\n",
    "plt.ylabel('t-SNE Dimension 2')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8ddf94",
   "metadata": {},
   "source": [
    "### Cross-modal Alignment:\n",
    "\n",
    "The key to text-to-image models is ensuring that text and image embeddings are aligned in the latent space. Models like CLIP (Contrastive Language-Image Pretraining) are trained to map corresponding text and images close together in this space.\n",
    "\n",
    "This alignment enables the model to:\n",
    "1. Find the right visual concepts based on textual descriptions\n",
    "2. Generate images that correspond to specific text prompts\n",
    "3. Create coherent visualizations that match the intended meaning\n",
    "\n",
    "This is particularly valuable in financial contexts where precise visualization of complex concepts is essential."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad6678d",
   "metadata": {},
   "source": [
    "## 7. Applications in Finance\n",
    "\n",
    "Text-to-image models have several promising applications in the financial industry:\n",
    "\n",
    "### 1. Data Visualization and Communication\n",
    "\n",
    "Text-to-image models can transform complex financial data into intuitive visualizations based on natural language descriptions, making it easier to:\n",
    "\n",
    "- Generate custom charts and graphs for reports\n",
    "- Create visual explanations of complex financial concepts\n",
    "- Produce consistent visual assets for presentations and documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6808d810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of financial data visualization prompts\n",
    "financial_viz_prompts = [\n",
    "    \"Create a detailed 3D visualization showing the correlation between inflation, interest rates, and stock market performance over time\",\n",
    "    \"Generate a comprehensive dashboard displaying key financial metrics including P/E ratios, market cap, and growth rates for technology sector companies\",\n",
    "    \"Design an intuitive flow chart explaining the process of mortgage-backed securities creation and trading\",\n",
    "    \"Visualize a network diagram showing relationships between major financial institutions and their exposure to systemic risk\"\n",
    "]\n",
    "\n",
    "# Display the potential prompts\n",
    "for i, prompt in enumerate(financial_viz_prompts):\n",
    "    print(f\"{i+1}. {prompt}\")\n",
    "    \n",
    "# Create a simple mockup of what these visualizations might look like\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# For demonstration purposes, we'll create placeholder visualizations\n",
    "viz_types = [\n",
    "    \"3D Correlation Plot\",\n",
    "    \"Financial Dashboard\",\n",
    "    \"Process Flow Chart\",\n",
    "    \"Network Relationship Diagram\"\n",
    "]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    # Create colored placeholder\n",
    "    color = plt.cm.viridis(i / len(axes))\n",
    "    img = np.ones((100, 100, 3))\n",
    "    for j in range(3):\n",
    "        img[:,:,j] = color[j]\n",
    "    \n",
    "    ax.imshow(img)\n",
    "    ax.set_title(viz_types[i])\n",
    "    ax.set_xlabel(\"Example placeholder for generated visualization\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "plt.suptitle(\"Examples of AI-Generated Financial Visualizations\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24942c8e",
   "metadata": {},
   "source": [
    "### 2. Market Sentiment Analysis\n",
    "\n",
    "Text-to-image models can help visualize market sentiment data:\n",
    "\n",
    "- Transforming sentiment analysis results into intuitive visual representations\n",
    "- Creating \"mood boards\" that reflect market conditions\n",
    "- Visualizing sentiment trends over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144b922f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample sentiment visualization\n",
    "def create_sentiment_visualization():\n",
    "    # Sample sentiment data\n",
    "    dates = pd.date_range(start='2023-01-01', periods=90)\n",
    "    sentiment = np.cumsum(np.random.randn(90) * 0.3)  # Random walk for sentiment\n",
    "    \n",
    "    # Normalize to range from -1 to 1\n",
    "    sentiment = sentiment / max(abs(sentiment.min()), abs(sentiment.max()))\n",
    "    \n",
    "    # Create a DataFrame\n",
    "    sentiment_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Sentiment': sentiment\n",
    "    })\n",
    "    \n",
    "    # Plot the sentiment\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Color mapping based on sentiment\n",
    "    colors = ['red' if s < 0 else 'green' for s in sentiment]\n",
    "    \n",
    "    # Create the plot\n",
    "    ax.plot(sentiment_df['Date'], sentiment_df['Sentiment'], color='black', alpha=0.5)\n",
    "    ax.scatter(sentiment_df['Date'], sentiment_df['Sentiment'], c=colors, s=50, alpha=0.7)\n",
    "    \n",
    "    # Fill between the line and zero\n",
    "    ax.fill_between(sentiment_df['Date'], sentiment_df['Sentiment'], 0, \n",
    "                    where=(sentiment_df['Sentiment'] > 0),\n",
    "                    color='green', alpha=0.3)\n",
    "    ax.fill_between(sentiment_df['Date'], sentiment_df['Sentiment'], 0, \n",
    "                    where=(sentiment_df['Sentiment'] < 0),\n",
    "                    color='red', alpha=0.3)\n",
    "    \n",
    "    # Add annotations for extreme points\n",
    "    peak_idx = sentiment_df['Sentiment'].idxmax()\n",
    "    trough_idx = sentiment_df['Sentiment'].idxmin()\n",
    "    \n",
    "    ax.annotate('Peak Optimism', \n",
    "                xy=(sentiment_df.loc[peak_idx, 'Date'], sentiment_df.loc[peak_idx, 'Sentiment']),\n",
    "                xytext=(10, 20), textcoords='offset points',\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3'))\n",
    "    \n",
    "    ax.annotate('Peak Pessimism', \n",
    "                xy=(sentiment_df.loc[trough_idx, 'Date'], sentiment_df.loc[trough_idx, 'Sentiment']),\n",
    "                xytext=(10, -20), textcoords='offset points',\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.3'))\n",
    "    \n",
    "    # Add some visual elements that AI-generated images might include\n",
    "    ax.axhline(y=0, color='grey', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add labels and title\n",
    "    ax.set_title('Market Sentiment Visualization (Q1 2023)', fontsize=14)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Sentiment Score')\n",
    "    \n",
    "    # Add a note about AI-generated imagery\n",
    "    plt.figtext(0.5, 0.01, \n",
    "                \"Note: This is a simulation of what an AI-generated sentiment visualization might look like.\",\n",
    "                ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Generate the visualization\n",
    "create_sentiment_visualization()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d8e6cd",
   "metadata": {},
   "source": [
    "### 3. Financial Education and Communication\n",
    "\n",
    "Text-to-image models can enhance financial education by:\n",
    "\n",
    "- Creating visual explanations of complex financial concepts\n",
    "- Generating consistent illustrations for educational materials\n",
    "- Producing infographics that make financial information more accessible\n",
    "\n",
    "### 4. Scenario Analysis and Stress Testing\n",
    "\n",
    "These models can help visualize different scenarios:\n",
    "\n",
    "- Creating visual representations of stress test outcomes\n",
    "- Illustrating potential market scenarios based on different parameters\n",
    "- Visualizing risk exposure across portfolios\n",
    "\n",
    "### 5. User Experience in Financial Applications\n",
    "\n",
    "Text-to-image models can enhance financial applications:\n",
    "\n",
    "- Generating custom UI elements based on user preferences\n",
    "- Creating personalized visual summaries of financial data\n",
    "- Enhancing chatbots and virtual assistants with visual outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a789bc",
   "metadata": {},
   "source": [
    "## 8. Limitations and Ethical Considerations\n",
    "\n",
    "While text-to-image models offer exciting possibilities, they also come with important limitations and ethical considerations:\n",
    "\n",
    "### Technical Limitations:\n",
    "\n",
    "1. **Accuracy of Financial Details**: These models may generate visually appealing but factually incorrect representations of financial data\n",
    "2. **Consistency**: Generated images may lack consistency across multiple generations\n",
    "3. **Specificity**: Models may struggle with highly technical or specialized financial concepts\n",
    "4. **Control**: Fine-grained control over generated visualizations remains challenging\n",
    "\n",
    "### Ethical Considerations:\n",
    "\n",
    "1. **Misinformation Risk**: Generated visualizations could potentially mislead if they present incorrect financial information\n",
    "2. **Bias**: Models may reproduce or amplify biases present in training data\n",
    "3. **Over-reliance**: Risk of over-trusting AI-generated visualizations without critical assessment\n",
    "4. **Transparency**: Need for clear disclosure when AI-generated visuals are used in financial contexts\n",
    "\n",
    "### Best Practices:\n",
    "\n",
    "1. Always verify the accuracy of generated financial visualizations\n",
    "2. Use text-to-image models as assistive tools rather than authoritative sources\n",
    "3. Implement human review processes for any AI-generated content used in formal financial communications\n",
    "4. Be transparent about the use of AI-generated imagery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd56e4b",
   "metadata": {},
   "source": [
    "## 9. Future Directions\n",
    "\n",
    "The field of multimodal AI is rapidly evolving, with several exciting directions for future development:\n",
    "\n",
    "1. **Improved Accuracy**: Better representation of numerical data and financial specifics\n",
    "2. **Multi-modal Financial Models**: Systems that understand and generate across text, images, and structured financial data\n",
    "3. **Interactive Visualizations**: Moving beyond static images to interactive, explorable visualizations\n",
    "4. **Domain-specific Models**: Text-to-image models specifically trained on financial imagery and concepts\n",
    "5. **Regulatory Frameworks**: Development of guidelines for the use of generative AI in financial contexts\n",
    "\n",
    "As these technologies continue to mature, they will likely become increasingly integrated into financial workflows, enhancing communication, analysis, and decision-making processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad630d01",
   "metadata": {},
   "source": [
    "## 10. Practical Exercise: Designing Financial Visualizations\n",
    "\n",
    "In this exercise, we'll explore how to craft effective prompts for generating financial visualizations. The key is to be specific about both the data relationships and the visual style you want to represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112591a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to help craft effective prompts\n",
    "def create_financial_visualization_prompt(\n",
    "    data_type,\n",
    "    relationships,\n",
    "    visual_style,\n",
    "    audience,\n",
    "    additional_details=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a well-structured prompt for financial visualization generation.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_type: The type of financial data (e.g., \"stock prices\", \"portfolio allocation\")\n",
    "    - relationships: What relationships should be shown (e.g., \"correlation between X and Y\")\n",
    "    - visual_style: Desired visual style (e.g., \"minimalist\", \"detailed 3D\", \"infographic\")\n",
    "    - audience: Who will view this visualization (e.g., \"retail investors\", \"board members\")\n",
    "    - additional_details: Any other specific requirements\n",
    "    \n",
    "    Returns:\n",
    "    - A formatted prompt string\n",
    "    \"\"\"\n",
    "    prompt = f\"Create a {visual_style} visualization of {data_type} showing {relationships}.\"\n",
    "    prompt += f\" The visualization should be appropriate for {audience}.\"\n",
    "    \n",
    "    if additional_details:\n",
    "        prompt += f\" {additional_details}\"\n",
    "        \n",
    "    return prompt\n",
    "\n",
    "# Example prompts for different financial visualization needs\n",
    "example_prompts = [\n",
    "    create_financial_visualization_prompt(\n",
    "        \"sector performance\",\n",
    "        \"relative performance of technology, healthcare, and financial sectors over the last 5 years\",\n",
    "        \"clean, modern chart\",\n",
    "        \"investment committee members\",\n",
    "        \"Use a consistent color scheme and include annotations for major market events.\"\n",
    "    ),\n",
    "    \n",
    "    create_financial_visualization_prompt(\n",
    "        \"portfolio risk exposure\",\n",
    "        \"geographic and sector diversification of a balanced investment portfolio\",\n",
    "        \"intuitive treemap\",\n",
    "        \"retail investors\",\n",
    "        \"Use a color gradient to indicate risk levels from low (blue) to high (red).\"\n",
    "    ),\n",
    "    \n",
    "    create_financial_visualization_prompt(\n",
    "        \"market volatility indicators\",\n",
    "        \"the relationship between VIX index, trading volume, and S&P 500 performance\",\n",
    "        \"multi-dimensional dashboard\",\n",
    "        \"professional traders\",\n",
    "        \"Include historical volatility patterns and highlight regime shifts.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# Display the example prompts\n",
    "print(\"Example Prompts for Financial Visualizations:\\n\")\n",
    "for i, prompt in enumerate(example_prompts):\n",
    "    print(f\"Example {i+1}:\\n{prompt}\\n\")\n",
    "\n",
    "# Let's create a function that evaluates the quality of a prompt\n",
    "def evaluate_prompt_quality(prompt):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of a visualization prompt based on key factors.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "    \n",
    "    # Check for specificity\n",
    "    specificity_keywords = ['specific', 'exactly', 'precise', 'detailed', 'particular']\n",
    "    scores['Specificity'] = min(sum(word in prompt.lower() for word in specificity_keywords) + 1, 5)\n",
    "    \n",
    "    # Check for clarity about data relationships\n",
    "    relationship_keywords = ['correlation', 'comparison', 'relationship', 'trend', 'versus', 'against', 'between']\n",
    "    scores['Data Relationships'] = min(sum(word in prompt.lower() for word in relationship_keywords) + 1, 5)\n",
    "    \n",
    "    # Check for visual style guidance\n",
    "    style_keywords = ['style', 'design', 'color', 'layout', 'format', 'aesthetic', 'visual', '3D', 'modern', 'clean']\n",
    "    scores['Visual Guidance'] = min(sum(word in prompt.lower() for word in style_keywords) + 1, 5)\n",
    "    \n",
    "    # Check for audience consideration\n",
    "    audience_keywords = ['audience', 'viewer', 'reader', 'client', 'investor', 'professional', 'committee', 'board']\n",
    "    scores['Audience Awareness'] = min(sum(word in prompt.lower() for word in audience_keywords) + 1, 5)\n",
    "    \n",
    "    # Overall length (proxy for detail)\n",
    "    scores['Detail Level'] = min(len(prompt.split()) // 10, 5)\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Evaluate our example prompts\n",
    "for i, prompt in enumerate(example_prompts):\n",
    "    print(f\"Evaluation of Example {i+1}:\")\n",
    "    scores = evaluate_prompt_quality(prompt)\n",
    "    \n",
    "    # Create a radar chart to visualize the evaluation\n",
    "    categories = list(scores.keys())\n",
    "    values = list(scores.values())\n",
    "    \n",
    "    # Close the polygon by appending the first value to the end\n",
    "    values.append(values[0])\n",
    "    categories.append(categories[0])\n",
    "    \n",
    "    # Create the plot\n",
    "    fig = plt.figure(figsize=(6, 6))\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "    \n",
    "    # Plot the values\n",
    "    ax.plot(np.linspace(0, 2*np.pi, len(values)), values, 'o-', linewidth=2)\n",
    "    ax.fill(np.linspace(0, 2*np.pi, len(values)), values, alpha=0.25)\n",
    "    \n",
    "    # Set the labels\n",
    "    ax.set_thetagrids(np.degrees(np.linspace(0, 2*np.pi, len(categories)-1, endpoint=False)), categories[:-1])\n",
    "    \n",
    "    # Set the radial limits\n",
    "    ax.set_ylim(0, 5)\n",
    "    ax.set_rticks([1, 2, 3, 4, 5])\n",
    "    \n",
    "    plt.title(f\"Prompt Quality Evaluation\", size=15)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97af9c9",
   "metadata": {},
   "source": [
    "## 11. Conclusion\n",
    "\n",
    "In this notebook, we've explored the architecture, implementation, and applications of text-to-image models, using them as a window into the broader world of multimodal AI systems.\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Architectural Understanding**: Text-to-image models use diffusion processes guided by text embeddings to generate visual content from textual descriptions\n",
    "2. **Latent Space Representation**: These models operate in a shared latent space where text and visual concepts are aligned\n",
    "3. **Financial Applications**: From data visualization to sentiment analysis, these models offer various potential applications in finance\n",
    "4. **Limitations**: While powerful, these models have important technical limitations and ethical considerations\n",
    "5. **Future Directions**: The field is rapidly evolving, with exciting possibilities for financial applications\n",
    "\n",
    "As multimodal AI systems continue to develop, financial professionals who understand these technologies will be well-positioned to leverage their capabilities while navigating their limitations appropriately.\n",
    "\n",
    "The transition from text-only to multimodal AI systems represents a significant evolution in artificial intelligence. By understanding how these systems work and their potential applications in finance, we gain valuable insights into the future direction of AI in the financial industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a14380",
   "metadata": {},
   "source": [
    "## 12. Further Resources\n",
    "\n",
    "To continue learning about text-to-image models and multimodal AI in finance, consider exploring these resources:\n",
    "\n",
    "1. **Research Papers**:\n",
    "   - \"High-Resolution Image Synthesis with Latent Diffusion Models\" (Rombach et al., 2022)\n",
    "   - \"DALL-E 2: Hierarchical Text-Conditional Image Generation with CLIP Latents\" (Ramesh et al., 2022)\n",
    "   - \"Multimodal Deep Learning in Finance: A Systematic Review\" (Various authors)\n",
    "\n",
    "2. **Online Courses and Tutorials**:\n",
    "   - Stanford CS231n: Convolutional Neural Networks for Visual Recognition\n",
    "   - Hugging Face Diffusers Library Documentation\n",
    "   - PyTorch Tutorials for Image Generation\n",
    "\n",
    "3. **Books**:\n",
    "   - \"Deep Learning for Computer Vision\" by Rajalingappaa Shanmugamani\n",
    "   - \"Generative Deep Learning\" by David Foster\n",
    "   - \"Artificial Intelligence in Finance\" by Yves Hilpisch\n",
    "\n",
    "4. **APIs and Tools**:\n",
    "   - OpenAI DALL-E API\n",
    "   - Stability AI's APIs\n",
    "   - Hugging Face Diffusers Library\n",
    "   - Replicate.com for testing various models\n",
    "\n",
    "5. **Communities**:\n",
    "   - Hugging Face Community\n",
    "   - Papers with Code (Computer Vision section)\n",
    "   - AI Finance communities on Reddit and Discord"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
